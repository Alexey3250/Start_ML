{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Lambda\n",
    "import torchvision.transforms as T\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "class CustomOxfordIIITPet(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(root_dir, 'images', '*.jpg')))\n",
    "        self.annotation_paths = sorted(glob.glob(os.path.join(root_dir, 'annotations', 'trimaps', '*.png')))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        annotation_path = self.annotation_paths[idx]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        annotation = Image.open(annotation_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            annotation = self.target_transform(annotation)\n",
    "\n",
    "        return image, annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "# Define the transforms in the global scope\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((256, 256)),\n",
    "        T.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def minus_one(x):\n",
    "    return (x - 1).long()\n",
    "\n",
    "target_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((256, 256)),\n",
    "        T.PILToTensor(),\n",
    "        T.Lambda(minus_one)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform_data(path, batch_size=64, num_workers=2, pin_memory=False):\n",
    "    train_dataset = CustomOxfordIIITPet(path, transform=transform, target_transform=target_transform)\n",
    "    valid_dataset = CustomOxfordIIITPet(path, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "train_loader, valid_loader = transform_data('./PETSdataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model) -> float:\n",
    "    \"\"\"\n",
    "    Эта функция отвечает за обучение модели на данных из набора train_loader. \n",
    "    На каждой итерации происходит расчет ошибки модели и обратное распространение ошибки.\n",
    "    Возвращаются средний loss и точность на данных для обучения.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for x, y in tqdm(train_loader, desc='Train'):\n",
    "        bs = y.size(0)\n",
    "\n",
    "        x, y = x.to(device), y.squeeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output.reshape(bs, 3, -1), y.reshape(bs, -1))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        _, y_pred = output.max(dim=1)\n",
    "        total += y.size(0) * y.size(1) * y.size(2)\n",
    "        correct += (y == y_pred).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return train_loss, accuracy\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, loader) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Эта функция оценивает модель на валидационном наборе данных. \n",
    "    Она рассчитывает потери и точность модели на валидационных данных.\n",
    "    Возвращаются средний loss и точность на валидационных данных.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for x, y in tqdm(loader, desc='Evaluation'):\n",
    "        bs = y.size(0)\n",
    "\n",
    "        x, y = x.to(device), y.squeeze(1).to(device)\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output.reshape(bs, 3, -1), y.reshape(bs, -1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, y_pred = output.max(dim=1)\n",
    "        total += y.size(0) * y.size(1) * y.size(2)\n",
    "        correct += (y == y_pred).sum().item()\n",
    "\n",
    "    total_loss /= len(loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return total_loss, accuracy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "def plot_stats(\n",
    "    train_loss: list[float],\n",
    "    valid_loss: list[float],\n",
    "    train_accuracy: list[float],\n",
    "    valid_accuracy: list[float],\n",
    "    title: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Эта функция отображает графики потерь и точности модели на данных для обучения и валидации. \n",
    "    Отдельно выводятся графики для потерь и точности.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.title(title + ' loss')\n",
    "\n",
    "    plt.plot(train_loss, label='Train loss')\n",
    "    plt.plot(valid_loss, label='Valid loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.title(title + ' accuracy')\n",
    "    \n",
    "    plt.plot(train_accuracy, label='Train accuracy')\n",
    "    plt.plot(valid_accuracy, label='Valid accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "@torch.inference_mode()\n",
    "def visualize(model, batch):\n",
    "    \"\"\"\n",
    "    Эта функция визуализирует результаты модели, предсказывая сегментацию на изображениях из валидационного набора данных. \n",
    "    Она выводит оригинальное изображение, маску сегментации и предсказание модели.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    xs, ys = batch\n",
    "    \n",
    "    to_pil = T.ToPILImage()\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "        prediction = model(x.unsqueeze(0).cuda()).squeeze(0).max(dim=0)[1]\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(24, 8), facecolor='white')\n",
    "\n",
    "        ax[0].imshow(to_pil(x))\n",
    "        ax[1].imshow(to_pil(y.to(torch.uint8)))\n",
    "        ax[2].imshow(to_pil(prediction.to(torch.uint8)))\n",
    "\n",
    "        ax[0].axis('off')\n",
    "        ax[1].axis('off')\n",
    "        ax[2].axis('off')\n",
    "\n",
    "        ax[0].set_title('Original image')\n",
    "        ax[1].set_title('Segmentation mask')\n",
    "        ax[2].set_title('Prediction')\n",
    "\n",
    "        plt.subplots_adjust(wspace=0, hspace=0.1)\n",
    "        plt.show()\n",
    "\n",
    "        if i >= 9:\n",
    "            break\n",
    "            \n",
    "def whole_train_valid_cycle(model, num_epochs, title):\n",
    "    \"\"\"\n",
    "    Эта функция производит полный цикл обучения и валидации модели. \n",
    "    Она проводит заданное количество эпох, на каждой эпохе сначала обучая модель, затем валидируя ее. \n",
    "    В процессе сохраняются истории потерь и точности, которые затем визуализируются. \n",
    "    После каждой эпохи также выводятся визуализации предсказаний модели.\n",
    "    \"\"\"\n",
    "    train_loss_history, valid_loss_history = [], []\n",
    "    train_accuracy_history, valid_accuracy_history = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(model)\n",
    "        valid_loss, valid_accuracy = evaluate(model, valid_loader)\n",
    "\n",
    "        train_loss_history.append(train_loss)\n",
    "        valid_loss_history.append(valid_loss)\n",
    "\n",
    "        train_accuracy_history.append(train_accuracy)\n",
    "        valid_accuracy_history.append(valid_accuracy)\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        plot_stats(\n",
    "            train_loss_history, valid_loss_history,\n",
    "            train_accuracy_history, valid_accuracy_history,\n",
    "            title\n",
    "        )\n",
    "\n",
    "        visualize(model, next(iter(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv_plus_conv(in_channels: int, out_channels: int):\n",
    "    \"\"\"\n",
    "    Makes UNet block\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :return: UNet block\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        ),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        ),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        base_channels = 64\n",
    "\n",
    "        # Down-convolutions\n",
    "        self.down1 = conv_plus_conv(3, base_channels)\n",
    "        self.down2 = conv_plus_conv(base_channels, base_channels * 2)\n",
    "        self.down3 = conv_plus_conv(base_channels * 2, base_channels * 4)\n",
    "        self.down4 = conv_plus_conv(base_channels * 4, base_channels * 8)\n",
    "\n",
    "        # Up-convolutions\n",
    "        self.up1 = conv_plus_conv(base_channels * 12, base_channels * 4)\n",
    "        self.up2 = conv_plus_conv(base_channels * 6, base_channels * 2)\n",
    "        self.up3 = conv_plus_conv(base_channels * 3, base_channels)\n",
    "\n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(in_channels=base_channels, out_channels=3, kernel_size=1)\n",
    "\n",
    "        self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting Path\n",
    "        residual1 = self.down1(x)\n",
    "        x = self.downsample(residual1)\n",
    "\n",
    "        residual2 = self.down2(x)\n",
    "        x = self.downsample(residual2)\n",
    "\n",
    "        residual3 = self.down3(x)\n",
    "        x = self.downsample(residual3)\n",
    "\n",
    "        x = self.down4(x)\n",
    "\n",
    "        # Expansive Path\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = torch.cat((x, residual3), dim=1)\n",
    "        x = self.up1(x)\n",
    "\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = torch.cat((x, residual2), dim=1)\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = torch.cat((x, residual1), dim=1)\n",
    "        x = self.up3(x)\n",
    "\n",
    "        # Final Convolution\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = UNET().to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/116 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "whole_train_valid_cycle(model, 50, 'UNET segmentation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
