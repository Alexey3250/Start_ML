{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Введение. Полносвязные слои. Функции активации (ноутбук)\n",
    "\n",
    "> Начнем осваивать библиотеку `PyTorch`. Познакомимся с нейронными сетями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## План ноутбука\n",
    "\n",
    "1. Установка `PyTorch`\n",
    "1. Введение в `PyTorch`\n",
    "1. Полносвязные слои и функции активации в `PyTorch`\n",
    "1. Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Установка `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы будем использовать библиотеку для глубинного обучения `PyTorch`, ее можно не устанавливать, будем пользоваться сайтом [kaggle.com](kaggle.com) для обучения в облаке (или с учителем?). \n",
    "\n",
    "Чтобы установить `PyTorch` локально себе на компьютер нужно ответить на два вопроса - какая у вас операционная система и есть ли у вас дискретная видеокарта (GPU) и если есть, то какого производителя. В зависимости от ваших ответов мы получаем три варианта по операционной системе - Linux, Mac и Windows; три варианта по дискретной видеокарте - нет видеокарты (доступен только центральный процессор CPU), есть видеокарта от Nvidia или есть видеокарта от AMD (это производитель именно чипа, конечный вендор может быть другой, например, ASUS, MSI, Palit). Работа с PyTorch с видеокартой от AMD это экзотика, которая выходит за рамки нашего курса, поэтому рассмотрим только варианты *нет видеокарты*/*есть видеокарта от Nvidia*.\n",
    "\n",
    "\n",
    "Выберите на [сайте](https://pytorch.org/get-started/locally/) подходящие вам варианты операционной системы/видеокарты и скопируйте команду для установки. Разберем подробно самые популярные варианты установки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Linux ([поддерживаемые дистрибутивы](https://pytorch.org/get-started/locally/#supported-linux-distributions))\n",
    "\n",
    "На линуксе будет работать поддержка `PyTorch` в любой конфигурации, что у вас нет видеокарты, что есть от Nvidia, что от AMD. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Windows\n",
    "\n",
    "На винде будет работать поддержка `PyTorch` только для видеокарт от Nvidia и без видеокарт вообще. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка на Mac\n",
    "\n",
    "На маках есть пока что поддержка `PyTorch` только центрального процессора, чуть позже появится поддержка ускорения на чипах M1, M2, M1 Pro и так далее.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` \n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Введение в `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Тензоры\n",
    "\n",
    "Тензоры — это специализированная структура данных, по сути это массивы и матрицы. Тензоры очень похожи на массивы в numpy, так что, если у вас хорошо с numpy, то разобраться в PyTorch тензорах будет очень просто. В PyTorch мы используем тензоры для кодирования входных и выходных данных модели, а также параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание тензоров\n",
    "\n",
    "Тензор можно создать напрямую из каких-то данных - нам подходят все списки с числами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [1, 2, 3, 4]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[1, 2], [3, 4], [5, 6]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле про \"все\" списки с числами - обман. Если у вашего списка есть какой-то уровень вложенности, то должны совпадать размерности у всех вложенных списков (подробнее про размерности поговорим позже):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m some_other_data \u001b[39m=\u001b[39m [[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m], [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m], [\u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m7\u001b[39m]]\n\u001b[1;32m----> 2\u001b[0m some_other_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(some_other_data)\n\u001b[0;32m      4\u001b[0m some_other_tensor\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 3)"
     ]
    }
   ],
   "source": [
    "some_other_data = [[1, 2], [3, 4], [5, 6, 7]]\n",
    "some_other_tensor = torch.tensor(some_other_data)\n",
    "\n",
    "some_other_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также тензоры можно создавать из numpy массивов и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [2]],\n",
       "\n",
       "       [[3],\n",
       "        [4]],\n",
       "\n",
       "       [[5],\n",
       "        [6]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_numpy_array = np.array(some_data)\n",
    "\n",
    "some_numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor_from_numpy = torch.from_numpy(some_numpy_array)\n",
    "\n",
    "some_tensor_from_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом если мы создаем тензор из numpy массива с помощью `torch.from_numpy`, то они делят между собой память, где лежат их данные и, соответственно, при изменении тензора меняется numpy массив и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(10)\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10)\n",
    "y = x.numpy()\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем создать тензор со случайными или константными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1912, 0.9945, 0.4320],\n",
       "         [0.4410, 0.3055, 0.4617]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[5.3137e-08, 1.2801e-11, 3.2540e+21],\n",
       "         [2.0315e+20, 2.0971e-07, 2.5230e-09]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "\n",
    "random_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "empty_tensor = torch.empty(shape)\n",
    "\n",
    "random_tensor, ones_tensor, zeros_tensor, empty_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про размерности подробнее.\n",
    "\n",
    "У тензора есть какой-то размер, какая форма. Первое с чем нужно определиться, какой **размерности** тензор - количество осей у него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3589, 0.4420, 0.0275, 0.5204, 0.3438, 0.7615, 0.2045, 0.3488, 0.5234,\n",
       "        0.1249])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (10)  # одна ось (вектор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0580, 0.5991, 0.4891],\n",
       "        [0.7393, 0.9290, 0.5490]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)  # две оси (матрица)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4323, 0.2534, 0.0997],\n",
       "         [0.9366, 0.3577, 0.0989]],\n",
       "\n",
       "        [[0.7847, 0.9679, 0.6405],\n",
       "         [0.5522, 0.6875, 0.3443]],\n",
       "\n",
       "        [[0.9040, 0.1846, 0.4654],\n",
       "         [0.9122, 0.9842, 0.8802]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (3, 2, 3)  # три оси (и больше - тензор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензор с размерностью 1 - это просто вектор, список чисел.\n",
    "\n",
    "Тензор с размерностью 2 - это просто матрица, то есть список списков чисел.\n",
    "\n",
    "Тензор с размерностью 3 и больше - это тензор, то есть список списков списков ... чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить доступ к размеру уже созданного тензора - метод `.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4]],\n",
      "\n",
      "        [[5],\n",
      "         [6]]])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "print(some_tensor)\n",
    "print(some_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лекции мы говорили про изображения, давайте сделаем тензор, который будет нам имитировать изображение - сделаем его размер `(c, h, w)`, где `h` и `w` это его высота и ширина, а `c` - число каналов в цветовом пространстве (в черно-белом 1, в RGB 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.5065e-01, 6.3749e-01, 5.6489e-01, 4.1040e-01, 3.8251e-01,\n",
       "          9.0649e-01, 2.7085e-01, 7.2695e-01, 4.1717e-02, 6.9695e-01,\n",
       "          5.6163e-01, 6.3981e-01, 1.9839e-01, 3.8246e-01, 4.2399e-01,\n",
       "          4.8876e-01],\n",
       "         [9.1767e-02, 6.6617e-01, 2.0268e-01, 4.1912e-02, 7.6421e-02,\n",
       "          8.4394e-01, 9.3419e-01, 2.4331e-02, 7.1487e-01, 2.0309e-01,\n",
       "          6.8665e-05, 2.6198e-01, 1.9354e-01, 1.6518e-01, 7.4314e-01,\n",
       "          4.5664e-01],\n",
       "         [2.9842e-01, 9.9818e-01, 5.7447e-01, 9.6147e-01, 1.4426e-01,\n",
       "          3.5030e-01, 2.1674e-02, 4.3049e-01, 4.5182e-01, 2.5391e-01,\n",
       "          9.4792e-01, 2.6867e-01, 4.5384e-02, 2.8194e-02, 7.2448e-01,\n",
       "          5.3086e-01],\n",
       "         [7.0636e-01, 2.4701e-01, 3.0313e-01, 5.3504e-01, 2.1272e-01,\n",
       "          5.7130e-01, 1.7485e-01, 6.9039e-01, 5.9262e-02, 9.1264e-02,\n",
       "          8.0106e-02, 5.5693e-01, 2.3414e-01, 1.4344e-02, 2.2960e-01,\n",
       "          1.0149e-01],\n",
       "         [1.7792e-01, 8.4268e-01, 5.8148e-02, 1.4533e-01, 9.2825e-01,\n",
       "          2.8619e-01, 4.2691e-01, 7.1047e-01, 1.3794e-01, 2.6083e-01,\n",
       "          2.5526e-01, 5.2380e-01, 7.6869e-01, 3.4350e-01, 9.6382e-01,\n",
       "          7.7810e-01],\n",
       "         [5.5472e-01, 5.5175e-01, 1.4569e-01, 4.7368e-01, 9.3184e-01,\n",
       "          3.0508e-01, 7.1726e-01, 7.0891e-01, 1.6688e-01, 8.6723e-01,\n",
       "          5.3737e-01, 1.4574e-01, 9.9238e-01, 7.3652e-01, 1.5875e-01,\n",
       "          1.9319e-01],\n",
       "         [9.2448e-01, 6.5697e-01, 8.0844e-01, 6.1954e-01, 5.0696e-01,\n",
       "          2.0470e-01, 3.7929e-02, 5.2735e-01, 5.1637e-01, 4.9383e-02,\n",
       "          9.5405e-01, 8.4323e-02, 1.8710e-01, 5.5343e-01, 6.7105e-01,\n",
       "          3.7778e-01],\n",
       "         [9.4372e-01, 9.3565e-01, 6.4483e-01, 3.6394e-01, 1.3299e-01,\n",
       "          5.6112e-01, 6.5769e-01, 1.1816e-01, 7.1031e-01, 5.6836e-01,\n",
       "          1.5818e-01, 8.4070e-01, 9.9285e-01, 4.1919e-01, 6.3288e-01,\n",
       "          1.9362e-01],\n",
       "         [9.2557e-02, 1.8198e-01, 2.9419e-01, 6.9201e-01, 5.1691e-01,\n",
       "          4.1238e-01, 6.0782e-01, 3.2694e-01, 6.1300e-01, 8.4580e-01,\n",
       "          5.1044e-01, 6.2121e-01, 8.8140e-01, 1.6982e-01, 4.5985e-01,\n",
       "          3.2103e-01]],\n",
       "\n",
       "        [[9.0622e-01, 5.3517e-01, 8.1856e-01, 2.7901e-04, 1.0146e-01,\n",
       "          1.7393e-01, 4.1270e-01, 6.3110e-01, 3.9537e-01, 9.9243e-01,\n",
       "          2.2058e-01, 8.6434e-01, 1.5639e-01, 9.0913e-01, 2.8177e-01,\n",
       "          7.6795e-02],\n",
       "         [2.0263e-01, 2.0134e-01, 7.7246e-01, 1.2973e-01, 7.6043e-01,\n",
       "          3.9084e-01, 1.9447e-01, 8.2132e-01, 6.5440e-01, 1.7974e-01,\n",
       "          9.1565e-01, 3.2313e-01, 4.0341e-01, 6.0783e-01, 7.3992e-01,\n",
       "          8.6270e-01],\n",
       "         [4.1850e-02, 4.9583e-02, 9.3916e-01, 4.6476e-01, 9.8559e-01,\n",
       "          2.4044e-01, 6.8706e-01, 9.5808e-01, 8.7666e-01, 7.4585e-01,\n",
       "          3.0013e-01, 2.4500e-01, 9.6677e-01, 3.0438e-01, 8.5869e-01,\n",
       "          1.7052e-01],\n",
       "         [6.3365e-01, 4.5551e-01, 8.6606e-01, 6.9879e-01, 9.3344e-01,\n",
       "          7.1346e-01, 1.9419e-01, 7.7005e-02, 3.6156e-01, 3.4093e-02,\n",
       "          2.5942e-01, 8.3292e-01, 8.2815e-01, 5.5801e-01, 8.7903e-01,\n",
       "          6.4517e-01],\n",
       "         [2.5799e-01, 6.7304e-01, 1.2626e-01, 5.4701e-01, 1.2090e-01,\n",
       "          1.0455e-01, 7.0199e-01, 7.9666e-01, 6.7935e-01, 8.4128e-01,\n",
       "          4.7494e-01, 5.4044e-01, 9.7314e-01, 5.7610e-01, 6.2562e-01,\n",
       "          2.7520e-01],\n",
       "         [9.8202e-01, 5.6763e-01, 9.6371e-01, 9.9301e-01, 9.6848e-01,\n",
       "          8.0895e-01, 1.4766e-01, 7.8165e-01, 2.8264e-01, 3.4830e-01,\n",
       "          7.0643e-01, 7.2674e-01, 6.7785e-01, 8.8024e-01, 4.4974e-01,\n",
       "          5.7000e-01],\n",
       "         [6.8667e-01, 5.7195e-01, 3.8857e-01, 4.3868e-01, 1.8910e-01,\n",
       "          1.2526e-01, 8.4804e-01, 1.4638e-01, 3.2244e-01, 1.3999e-01,\n",
       "          6.0293e-01, 9.4467e-01, 9.0883e-01, 9.3342e-01, 7.3794e-01,\n",
       "          5.4614e-01],\n",
       "         [8.9961e-01, 1.3065e-01, 2.0951e-01, 5.3794e-01, 6.5822e-01,\n",
       "          9.9911e-01, 6.4868e-01, 1.6865e-01, 3.7007e-01, 8.4604e-01,\n",
       "          6.0895e-01, 3.7224e-01, 4.3847e-01, 3.8789e-01, 9.8942e-01,\n",
       "          6.0605e-01],\n",
       "         [9.9399e-01, 9.9097e-01, 7.1757e-02, 9.4568e-01, 8.1394e-01,\n",
       "          1.2361e-02, 7.1413e-01, 5.4050e-01, 7.7568e-01, 5.7565e-01,\n",
       "          3.9022e-01, 3.0450e-01, 7.5251e-01, 8.2158e-01, 6.6822e-01,\n",
       "          1.6340e-01]],\n",
       "\n",
       "        [[5.2846e-01, 4.1984e-02, 9.7531e-01, 1.3137e-01, 5.3668e-01,\n",
       "          5.9084e-01, 1.7428e-01, 8.8277e-01, 3.7877e-01, 5.1332e-02,\n",
       "          6.1309e-01, 4.1577e-01, 1.2221e-01, 6.5954e-01, 8.5787e-01,\n",
       "          2.4799e-01],\n",
       "         [1.4320e-01, 7.0367e-01, 1.4530e-01, 9.6001e-01, 8.1726e-01,\n",
       "          7.4634e-01, 9.3219e-01, 4.6899e-01, 3.4846e-01, 2.1480e-01,\n",
       "          3.5788e-02, 6.7478e-01, 5.6295e-01, 7.9326e-01, 6.5973e-01,\n",
       "          6.5440e-01],\n",
       "         [6.6071e-01, 2.2849e-01, 2.4665e-01, 6.2822e-01, 9.4442e-01,\n",
       "          6.5615e-01, 5.8829e-01, 3.6896e-01, 6.1543e-01, 9.0980e-02,\n",
       "          4.3567e-01, 5.2164e-01, 8.7691e-01, 7.9431e-01, 3.1492e-01,\n",
       "          5.9459e-01],\n",
       "         [6.0508e-01, 4.7828e-02, 5.4690e-01, 8.4605e-02, 1.8037e-01,\n",
       "          7.0317e-01, 3.8197e-01, 4.5039e-01, 4.5770e-01, 1.3916e-01,\n",
       "          5.0258e-01, 7.3028e-01, 7.2994e-01, 2.6979e-01, 8.3069e-03,\n",
       "          7.7022e-01],\n",
       "         [7.4505e-01, 7.8208e-01, 4.9930e-01, 9.0664e-01, 3.1490e-01,\n",
       "          6.2079e-01, 3.9508e-01, 6.6265e-01, 4.5645e-01, 9.1666e-01,\n",
       "          9.8966e-01, 4.7433e-01, 6.4182e-01, 2.3986e-01, 2.9657e-01,\n",
       "          7.6806e-01],\n",
       "         [6.8477e-01, 2.2007e-01, 9.2610e-01, 1.9113e-01, 9.8072e-02,\n",
       "          2.2430e-01, 8.1419e-01, 3.8420e-01, 2.3455e-01, 8.4276e-01,\n",
       "          9.9190e-02, 2.9229e-01, 5.9805e-02, 1.9161e-01, 7.7644e-01,\n",
       "          2.2750e-02],\n",
       "         [1.4596e-01, 4.4185e-01, 4.2116e-01, 4.8402e-01, 1.9858e-02,\n",
       "          8.8179e-01, 4.7145e-01, 7.8355e-01, 2.6146e-01, 9.5347e-01,\n",
       "          4.6774e-01, 7.4087e-01, 3.2661e-01, 9.9393e-01, 1.5341e-01,\n",
       "          5.3745e-01],\n",
       "         [1.6880e-02, 1.3820e-02, 3.6630e-01, 2.5217e-01, 1.1757e-01,\n",
       "          1.4088e-01, 7.8149e-01, 6.5412e-01, 9.4594e-01, 6.4948e-02,\n",
       "          6.8776e-01, 9.0334e-01, 5.1127e-01, 7.5747e-01, 7.8878e-01,\n",
       "          7.5335e-01],\n",
       "         [6.1582e-01, 4.2480e-01, 4.9206e-02, 2.3277e-01, 4.3766e-01,\n",
       "          5.8616e-01, 7.0982e-03, 2.7577e-01, 3.3842e-01, 2.7510e-01,\n",
       "          7.9809e-02, 8.3843e-01, 4.6318e-01, 4.3206e-01, 1.9551e-01,\n",
       "          5.4090e-02]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 9\n",
    "w = 16\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9, 16])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем попробовать поменять размер тензора, например, [вытянуть его в вектор](https://pytorch.org/docs/stable/generated/torch.ravel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5065e-01, 6.3749e-01, 5.6489e-01, 4.1040e-01, 3.8251e-01, 9.0649e-01,\n",
       "        2.7085e-01, 7.2695e-01, 4.1717e-02, 6.9695e-01, 5.6163e-01, 6.3981e-01,\n",
       "        1.9839e-01, 3.8246e-01, 4.2399e-01, 4.8876e-01, 9.1767e-02, 6.6617e-01,\n",
       "        2.0268e-01, 4.1912e-02, 7.6421e-02, 8.4394e-01, 9.3419e-01, 2.4331e-02,\n",
       "        7.1487e-01, 2.0309e-01, 6.8665e-05, 2.6198e-01, 1.9354e-01, 1.6518e-01,\n",
       "        7.4314e-01, 4.5664e-01, 2.9842e-01, 9.9818e-01, 5.7447e-01, 9.6147e-01,\n",
       "        1.4426e-01, 3.5030e-01, 2.1674e-02, 4.3049e-01, 4.5182e-01, 2.5391e-01,\n",
       "        9.4792e-01, 2.6867e-01, 4.5384e-02, 2.8194e-02, 7.2448e-01, 5.3086e-01,\n",
       "        7.0636e-01, 2.4701e-01, 3.0313e-01, 5.3504e-01, 2.1272e-01, 5.7130e-01,\n",
       "        1.7485e-01, 6.9039e-01, 5.9262e-02, 9.1264e-02, 8.0106e-02, 5.5693e-01,\n",
       "        2.3414e-01, 1.4344e-02, 2.2960e-01, 1.0149e-01, 1.7792e-01, 8.4268e-01,\n",
       "        5.8148e-02, 1.4533e-01, 9.2825e-01, 2.8619e-01, 4.2691e-01, 7.1047e-01,\n",
       "        1.3794e-01, 2.6083e-01, 2.5526e-01, 5.2380e-01, 7.6869e-01, 3.4350e-01,\n",
       "        9.6382e-01, 7.7810e-01, 5.5472e-01, 5.5175e-01, 1.4569e-01, 4.7368e-01,\n",
       "        9.3184e-01, 3.0508e-01, 7.1726e-01, 7.0891e-01, 1.6688e-01, 8.6723e-01,\n",
       "        5.3737e-01, 1.4574e-01, 9.9238e-01, 7.3652e-01, 1.5875e-01, 1.9319e-01,\n",
       "        9.2448e-01, 6.5697e-01, 8.0844e-01, 6.1954e-01, 5.0696e-01, 2.0470e-01,\n",
       "        3.7929e-02, 5.2735e-01, 5.1637e-01, 4.9383e-02, 9.5405e-01, 8.4323e-02,\n",
       "        1.8710e-01, 5.5343e-01, 6.7105e-01, 3.7778e-01, 9.4372e-01, 9.3565e-01,\n",
       "        6.4483e-01, 3.6394e-01, 1.3299e-01, 5.6112e-01, 6.5769e-01, 1.1816e-01,\n",
       "        7.1031e-01, 5.6836e-01, 1.5818e-01, 8.4070e-01, 9.9285e-01, 4.1919e-01,\n",
       "        6.3288e-01, 1.9362e-01, 9.2557e-02, 1.8198e-01, 2.9419e-01, 6.9201e-01,\n",
       "        5.1691e-01, 4.1238e-01, 6.0782e-01, 3.2694e-01, 6.1300e-01, 8.4580e-01,\n",
       "        5.1044e-01, 6.2121e-01, 8.8140e-01, 1.6982e-01, 4.5985e-01, 3.2103e-01,\n",
       "        9.0622e-01, 5.3517e-01, 8.1856e-01, 2.7901e-04, 1.0146e-01, 1.7393e-01,\n",
       "        4.1270e-01, 6.3110e-01, 3.9537e-01, 9.9243e-01, 2.2058e-01, 8.6434e-01,\n",
       "        1.5639e-01, 9.0913e-01, 2.8177e-01, 7.6795e-02, 2.0263e-01, 2.0134e-01,\n",
       "        7.7246e-01, 1.2973e-01, 7.6043e-01, 3.9084e-01, 1.9447e-01, 8.2132e-01,\n",
       "        6.5440e-01, 1.7974e-01, 9.1565e-01, 3.2313e-01, 4.0341e-01, 6.0783e-01,\n",
       "        7.3992e-01, 8.6270e-01, 4.1850e-02, 4.9583e-02, 9.3916e-01, 4.6476e-01,\n",
       "        9.8559e-01, 2.4044e-01, 6.8706e-01, 9.5808e-01, 8.7666e-01, 7.4585e-01,\n",
       "        3.0013e-01, 2.4500e-01, 9.6677e-01, 3.0438e-01, 8.5869e-01, 1.7052e-01,\n",
       "        6.3365e-01, 4.5551e-01, 8.6606e-01, 6.9879e-01, 9.3344e-01, 7.1346e-01,\n",
       "        1.9419e-01, 7.7005e-02, 3.6156e-01, 3.4093e-02, 2.5942e-01, 8.3292e-01,\n",
       "        8.2815e-01, 5.5801e-01, 8.7903e-01, 6.4517e-01, 2.5799e-01, 6.7304e-01,\n",
       "        1.2626e-01, 5.4701e-01, 1.2090e-01, 1.0455e-01, 7.0199e-01, 7.9666e-01,\n",
       "        6.7935e-01, 8.4128e-01, 4.7494e-01, 5.4044e-01, 9.7314e-01, 5.7610e-01,\n",
       "        6.2562e-01, 2.7520e-01, 9.8202e-01, 5.6763e-01, 9.6371e-01, 9.9301e-01,\n",
       "        9.6848e-01, 8.0895e-01, 1.4766e-01, 7.8165e-01, 2.8264e-01, 3.4830e-01,\n",
       "        7.0643e-01, 7.2674e-01, 6.7785e-01, 8.8024e-01, 4.4974e-01, 5.7000e-01,\n",
       "        6.8667e-01, 5.7195e-01, 3.8857e-01, 4.3868e-01, 1.8910e-01, 1.2526e-01,\n",
       "        8.4804e-01, 1.4638e-01, 3.2244e-01, 1.3999e-01, 6.0293e-01, 9.4467e-01,\n",
       "        9.0883e-01, 9.3342e-01, 7.3794e-01, 5.4614e-01, 8.9961e-01, 1.3065e-01,\n",
       "        2.0951e-01, 5.3794e-01, 6.5822e-01, 9.9911e-01, 6.4868e-01, 1.6865e-01,\n",
       "        3.7007e-01, 8.4604e-01, 6.0895e-01, 3.7224e-01, 4.3847e-01, 3.8789e-01,\n",
       "        9.8942e-01, 6.0605e-01, 9.9399e-01, 9.9097e-01, 7.1757e-02, 9.4568e-01,\n",
       "        8.1394e-01, 1.2361e-02, 7.1413e-01, 5.4050e-01, 7.7568e-01, 5.7565e-01,\n",
       "        3.9022e-01, 3.0450e-01, 7.5251e-01, 8.2158e-01, 6.6822e-01, 1.6340e-01,\n",
       "        5.2846e-01, 4.1984e-02, 9.7531e-01, 1.3137e-01, 5.3668e-01, 5.9084e-01,\n",
       "        1.7428e-01, 8.8277e-01, 3.7877e-01, 5.1332e-02, 6.1309e-01, 4.1577e-01,\n",
       "        1.2221e-01, 6.5954e-01, 8.5787e-01, 2.4799e-01, 1.4320e-01, 7.0367e-01,\n",
       "        1.4530e-01, 9.6001e-01, 8.1726e-01, 7.4634e-01, 9.3219e-01, 4.6899e-01,\n",
       "        3.4846e-01, 2.1480e-01, 3.5788e-02, 6.7478e-01, 5.6295e-01, 7.9326e-01,\n",
       "        6.5973e-01, 6.5440e-01, 6.6071e-01, 2.2849e-01, 2.4665e-01, 6.2822e-01,\n",
       "        9.4442e-01, 6.5615e-01, 5.8829e-01, 3.6896e-01, 6.1543e-01, 9.0980e-02,\n",
       "        4.3567e-01, 5.2164e-01, 8.7691e-01, 7.9431e-01, 3.1492e-01, 5.9459e-01,\n",
       "        6.0508e-01, 4.7828e-02, 5.4690e-01, 8.4605e-02, 1.8037e-01, 7.0317e-01,\n",
       "        3.8197e-01, 4.5039e-01, 4.5770e-01, 1.3916e-01, 5.0258e-01, 7.3028e-01,\n",
       "        7.2994e-01, 2.6979e-01, 8.3069e-03, 7.7022e-01, 7.4505e-01, 7.8208e-01,\n",
       "        4.9930e-01, 9.0664e-01, 3.1490e-01, 6.2079e-01, 3.9508e-01, 6.6265e-01,\n",
       "        4.5645e-01, 9.1666e-01, 9.8966e-01, 4.7433e-01, 6.4182e-01, 2.3986e-01,\n",
       "        2.9657e-01, 7.6806e-01, 6.8477e-01, 2.2007e-01, 9.2610e-01, 1.9113e-01,\n",
       "        9.8072e-02, 2.2430e-01, 8.1419e-01, 3.8420e-01, 2.3455e-01, 8.4276e-01,\n",
       "        9.9190e-02, 2.9229e-01, 5.9805e-02, 1.9161e-01, 7.7644e-01, 2.2750e-02,\n",
       "        1.4596e-01, 4.4185e-01, 4.2116e-01, 4.8402e-01, 1.9858e-02, 8.8179e-01,\n",
       "        4.7145e-01, 7.8355e-01, 2.6146e-01, 9.5347e-01, 4.6774e-01, 7.4087e-01,\n",
       "        3.2661e-01, 9.9393e-01, 1.5341e-01, 5.3745e-01, 1.6880e-02, 1.3820e-02,\n",
       "        3.6630e-01, 2.5217e-01, 1.1757e-01, 1.4088e-01, 7.8149e-01, 6.5412e-01,\n",
       "        9.4594e-01, 6.4948e-02, 6.8776e-01, 9.0334e-01, 5.1127e-01, 7.5747e-01,\n",
       "        7.8878e-01, 7.5335e-01, 6.1582e-01, 4.2480e-01, 4.9206e-02, 2.3277e-01,\n",
       "        4.3766e-01, 5.8616e-01, 7.0982e-03, 2.7577e-01, 3.3842e-01, 2.7510e-01,\n",
       "        7.9809e-02, 8.3843e-01, 4.6318e-01, 4.3206e-01, 1.9551e-01, 5.4090e-02])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([432])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h * w * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество элементов в тензоре с помощью [специальной функции](https://pytorch.org/docs/stable/generated/torch.numel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5402, 0.3633, 0.7560],\n",
       "         [0.5026, 0.3691, 0.4226]],\n",
       "\n",
       "        [[0.2304, 0.1036, 0.1841],\n",
       "         [0.5608, 0.4025, 0.4875]],\n",
       "\n",
       "        [[0.8159, 0.3444, 0.0381],\n",
       "         [0.3014, 0.5945, 0.9387]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 2\n",
    "w = 3\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем поменять размер с помощью функции [reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5402, 0.3633, 0.7560, 0.5026, 0.3691, 0.4226],\n",
       "        [0.2304, 0.1036, 0.1841, 0.5608, 0.4025, 0.4875],\n",
       "        [0.8159, 0.3444, 0.0381, 0.3014, 0.5945, 0.9387]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.reshape(c, h * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем собрать из нескольких тензоров один большой:\n",
    "\n",
    "[torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4081, -0.1584,  0.1262],\n",
       "        [ 1.2818,  0.4380, -0.2734]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4081, -0.1584,  0.1262],\n",
       "        [ 1.2818,  0.4380, -0.2734],\n",
       "        [-1.4081, -0.1584,  0.1262],\n",
       "        [ 1.2818,  0.4380, -0.2734],\n",
       "        [-1.4081, -0.1584,  0.1262],\n",
       "        [ 1.2818,  0.4380, -0.2734]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4081, -0.1584,  0.1262, -1.4081, -0.1584,  0.1262, -1.4081, -0.1584,\n",
       "          0.1262],\n",
       "        [ 1.2818,  0.4380, -0.2734,  1.2818,  0.4380, -0.2734,  1.2818,  0.4380,\n",
       "         -0.2734]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5554e+00,  9.6247e-01, -5.0852e-01],\n",
      "        [-1.1485e+00,  2.9893e-01,  3.1158e-01],\n",
      "        [-2.0600e-01, -2.6704e+00, -4.1880e-04]])\n",
      "tensor([[ 1.0418,  1.2733, -0.1258],\n",
      "        [-0.8262, -0.7864, -0.3093],\n",
      "        [ 0.6579, -3.0240,  1.3560],\n",
      "        [-1.5537, -1.4683, -1.3890],\n",
      "        [-0.4021, -0.1294, -0.4719]])\n",
      "tensor([[-1.6368, -0.0777, -0.6602]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5554e+00,  9.6247e-01, -5.0852e-01],\n",
       "        [-1.1485e+00,  2.9893e-01,  3.1158e-01],\n",
       "        [-2.0600e-01, -2.6704e+00, -4.1880e-04],\n",
       "        [ 1.0418e+00,  1.2733e+00, -1.2578e-01],\n",
       "        [-8.2623e-01, -7.8645e-01, -3.0926e-01],\n",
       "        [ 6.5789e-01, -3.0240e+00,  1.3560e+00],\n",
       "        [-1.5537e+00, -1.4683e+00, -1.3890e+00],\n",
       "        [-4.0214e-01, -1.2941e-01, -4.7194e-01],\n",
       "        [-1.6368e+00, -7.7666e-02, -6.6015e-01]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(5, 3)\n",
    "z = torch.randn(1, 3)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0276, -1.0269,  0.2551],\n",
      "        [ 0.5838, -0.8767,  0.8825]])\n",
      "tensor([[ 0.7434,  0.2519, -0.7274, -0.1589,  1.0437],\n",
      "        [ 1.6179,  0.5177, -0.0426,  0.1303,  0.1158]])\n",
      "tensor([[-0.0690],\n",
      "        [-0.1560]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0276, -1.0269,  0.2551,  0.7434,  0.2519, -0.7274, -0.1589,  1.0437,\n",
       "         -0.0690],\n",
       "        [ 0.5838, -0.8767,  0.8825,  1.6179,  0.5177, -0.0426,  0.1303,  0.1158,\n",
       "         -0.1560]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 5)\n",
    "z = torch.randn(2, 1)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим дополнительную ось:\n",
    "\n",
    "[torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1495, 0.7940, 0.0920],\n",
      "        [0.7541, 0.2311, 0.5964]])\n",
      "\n",
      "tensor([[[0.1495, 0.7940, 0.0920],\n",
      "         [0.7541, 0.2311, 0.5964]]]) torch.Size([1, 2, 3])\n",
      "\n",
      "tensor([[[0.1495, 0.7940, 0.0920]],\n",
      "\n",
      "        [[0.7541, 0.2311, 0.5964]]]) torch.Size([2, 1, 3])\n",
      "\n",
      "tensor([[[0.1495],\n",
      "         [0.7940],\n",
      "         [0.0920]],\n",
      "\n",
      "        [[0.7541],\n",
      "         [0.2311],\n",
      "         [0.5964]]]) torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.unsqueeze(0), x.unsqueeze(0).shape)\n",
    "print()\n",
    "print(x.unsqueeze(1), x.unsqueeze(1).shape)\n",
    "print()\n",
    "print(x.unsqueeze(2), x.unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем лишние оси (где размер единичка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.1542, 0.6394, 0.4837]],\n",
      "\n",
      "         [[0.0710, 0.3198, 0.6776]]]])\n",
      "\n",
      "tensor([[0.1542, 0.6394, 0.4837],\n",
      "        [0.0710, 0.3198, 0.6776]]) torch.Size([2, 3])\n",
      "\n",
      "tensor([[[0.1542, 0.6394, 0.4837]],\n",
      "\n",
      "        [[0.0710, 0.3198, 0.6776]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 2, 1, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.squeeze(), x.squeeze().shape)\n",
    "print()\n",
    "print(x.squeeze(0), x.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про типы данных в тензорах. По умолчанию в тензорах лежат числа в torch.float32 для вещественных и torch.int64 для целочисленных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.1992, 3.6992, 4.8984], dtype=torch.float16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float64)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int32)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размещение тензора на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available())\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mget_device_name())\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:365\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[0;32m    356\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     \u001b[39mreturn\u001b[39;00m get_device_properties(device)\u001b[39m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:395\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_properties\u001b[39m(device: _device_t) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    386\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \n\u001b[0;32m    388\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[39m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m     _lazy_init()  \u001b[39m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    396\u001b[0m     device \u001b[39m=\u001b[39m _get_device_index(device, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    397\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count():\n",
      "File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 19 18:13:00 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.61                 Driver Version: 531.61       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080       WDDM | 00000000:01:00.0  On |                  N/A |\n",
      "| 57%   48C    P0              103W / 370W|   1751MiB / 10240MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3496    C+G   ...e Stream\\74.0.2.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A      6888    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     11192    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12192    C+G   G:\\Games\\Battle.net\\Battle.net.exe        N/A      |\n",
      "|    0   N/A  N/A     14048    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     14492    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     14632    C+G   ...6.0_x64__cv1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     15704    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16844    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18396    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     18688    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     19088    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     19204    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     19340    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     19860    C+G   ...AppData\\Roaming\\Spotify\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     20380    C+G   ...al\\Discord\\app-1.0.9012\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     23296    C+G   ..._8wekyb3d8bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     23724    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A     24116    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     25184    C+G   ...\\Local\\slack\\app-4.31.155\\slack.exe    N/A      |\n",
      "|    0   N/A  N/A     25504    C+G   ...ience\\NVIDIA GeForce Experience.exe    N/A      |\n",
      "|    0   N/A  N/A     25764    C+G   ...am Files\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     27224    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     30540    C+G   ...aming\\Telegram Desktop\\Telegram.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], device=device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 22, 37, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "tensor = tensor.to(device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor.cpu()\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7184, 0.5498, 0.5604],\n",
       "        [0.4750, 1.2123, 0.9842]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "a = a.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7184, 0.5498, 0.5604],\n",
       "        [0.4750, 1.2123, 0.9842]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Операции с тензорами\n",
    "\n",
    "Большая часть операций с тензорами хорошо описана в их [документации](https://pytorch.org/docs/stable/torch.html), разберем основные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0542, 0.3823, 0.5019],\n",
       "         [0.7535, 0.0122, 0.9364]]),\n",
       " tensor([[0.9840, 0.0882, 0.7651],\n",
       "         [0.7870, 0.4985, 0.0727]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0383, 0.4705, 1.2670],\n",
      "        [1.5404, 0.5107, 1.0090]])\n",
      "\n",
      "tensor([[1.0383, 0.4705, 1.2670],\n",
      "        [1.5404, 0.5107, 1.0090]])\n",
      "\n",
      "tensor([[1.0383, 0.4705, 1.2670],\n",
      "        [1.5404, 0.5107, 1.0090]])\n"
     ]
    }
   ],
   "source": [
    "# поэлементные\n",
    "\n",
    "print(a + b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.add(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.add(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n",
      "\n",
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n",
      "\n",
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n"
     ]
    }
   ],
   "source": [
    "print(a - b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.sub(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.sub(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n",
      "\n",
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n",
      "\n",
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n"
     ]
    }
   ],
   "source": [
    "print(a * b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.mul(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.mul(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n",
      "\n",
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n",
      "\n",
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n"
     ]
    }
   ],
   "source": [
    "print(a / b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.div(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.div(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9952, 0.3948, 0.6624],\n",
       "         [0.8935, 0.0395, 0.9213]]),\n",
       " tensor([[0.2513, 0.2235, 0.2446, 0.0461],\n",
       "         [0.7737, 0.9193, 0.6260, 0.1214],\n",
       "         [0.0117, 0.7833, 0.6692, 0.1084]]),\n",
       " tensor([[0.0264, 0.9775, 0.4449, 0.3109, 0.7528],\n",
       "         [0.3279, 0.7904, 0.5625, 0.1013, 0.3399],\n",
       "         [0.4000, 0.6632, 0.5702, 0.9594, 0.9428],\n",
       "         [0.8072, 0.0614, 0.3238, 0.0036, 0.6157],\n",
       "         [0.4995, 0.8749, 0.2871, 0.5104, 0.5305]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 4)\n",
    "c = torch.rand(5, 5)\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5633, 1.1042, 0.9339, 0.1656],\n",
      "        [0.2658, 0.9576, 0.8598, 0.1459]]) torch.Size([2, 4])\n",
      "\n",
      "tensor([[0.5633, 1.1042, 0.9339, 0.1656],\n",
      "        [0.2658, 0.9576, 0.8598, 0.1459]]) torch.Size([2, 4])\n",
      "\n",
      "tensor(1.9212)\n",
      "\n",
      "tensor([[1.0268, 2.6579, 1.5603, 1.3647, 2.1229],\n",
      "        [1.3880, 2.2043, 1.7551, 1.1066, 1.4048],\n",
      "        [1.4919, 1.9409, 1.7685, 2.6100, 2.5671],\n",
      "        [2.2417, 1.0633, 1.3823, 1.0036, 1.8509],\n",
      "        [1.6479, 2.3987, 1.3325, 1.6659, 1.6998]])\n"
     ]
    }
   ],
   "source": [
    "# матричные операции\n",
    "\n",
    "print(a @ b, (a @ b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.matmul(a, b), torch.matmul(a, b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.trace())\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### [Автоматическое дифференцирование](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7076, 0.3914, 0.9870, 0.3090, 0.6262])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3531, 0.4118, 0.7373, 0.5783, 0.4483],\n",
       "        [0.3005, 0.9868, 0.6453, 0.5094, 0.3786],\n",
       "        [0.6972, 0.2405, 0.4653, 0.5081, 0.5772]], requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7320e-12,  3.0774e-41, -1.7366e-12])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_z = torch.empty(3)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5983, 1.6304, 1.5652], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    first_z[i] = torch.sum(w[i] * x)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5983, 1.6304, 1.5652], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w.t())\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1148, 0.7261, 0.6116], requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.rand(3, requires_grad=True)\n",
    "\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3244, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.sum(z * v)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3244199752807617"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((y - 2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1052, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=None\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuke/.local/lib/python3.10/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=tensor([[0.0527, 0.0291, 0.0735, 0.0230, 0.0466],\n",
      "        [0.3334, 0.1844, 0.4650, 0.1456, 0.2950],\n",
      "        [0.2808, 0.1553, 0.3917, 0.1226, 0.2485]])\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=tensor([1.0370, 1.0579, 1.0155])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5492], requires_grad=True), tensor([0.4124], requires_grad=True))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1368], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([2.])\n",
      "\n",
      "b.grad=tensor([-2.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 1\n",
    "print(f'{b.grad=}\\n')  # -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0187], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b) ** 2\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.])\n",
      "\n",
      "b.grad=tensor([0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.2736])\n",
      "\n",
      "b.grad=tensor([-0.2736])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 2 * (a - b)\n",
    "print(f'{b.grad=}\\n')  # -2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2736], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3279, 0.5942, 0.6495, 0.8076, 0.6961],\n",
       "         [0.7243, 0.3865, 0.4297, 0.0069, 0.3211],\n",
       "         [0.9084, 0.0009, 0.5393, 0.4543, 0.1057]], requires_grad=True),\n",
       " tensor([[0.3158, 0.7038, 0.3901, 0.7456, 0.8604],\n",
       "         [0.4813, 0.8476, 0.9554, 0.9591, 0.9958],\n",
       "         [0.5856, 0.4295, 0.9476, 0.6895, 0.3585]], requires_grad=True))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3189, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean(a * b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
      "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
      "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]])\n",
      "\n",
      "b.grad=tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
      "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
      "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # b / (3 * 5)\n",
    "print(f'{b.grad=}\\n')  # a / (3 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
       "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
       "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
       "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
       "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[0.9249, 0.5178, 0.4018, 0.5241, 0.2307],\n",
      "        [0.4165, 0.5878, 0.6177, 0.4664, 0.3297],\n",
      "        [0.2164, 0.3102, 0.1307, 0.8902, 0.9583]], requires_grad=True)\n",
      "\n",
      "a.grad=None\n",
      "\n",
      "a.grad=tensor([[1.8499, 1.0356, 0.8036, 1.0483, 0.4613],\n",
      "        [0.8329, 1.1757, 1.2353, 0.9329, 0.6594],\n",
      "        [0.4329, 0.6204, 0.2613, 1.7805, 1.9166]])\n",
      "\n",
      "a.grad=tensor([[2.8499, 2.0356, 1.8036, 2.0483, 1.4613],\n",
      "        [1.8329, 2.1757, 2.2353, 1.9329, 1.6594],\n",
      "        [1.4329, 1.6204, 1.2613, 2.7805, 2.9166]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "print(f'{a=}\\n')\n",
    "\n",
    "loss1 = torch.sum(a ** 2) # 2a\n",
    "loss2 = torch.sum(a) # 1\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss1.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss2.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*a=tensor([[1.8499, 1.0356, 0.8036, 1.0483, 0.4613],\n",
      "        [0.8329, 1.1757, 1.2353, 0.9329, 0.6594],\n",
      "        [0.4329, 0.6204, 0.2613, 1.7805, 1.9166]], grad_fn=<MulBackward0>)\n",
      "\n",
      "2*a+1=tensor([[2.8499, 2.0356, 1.8036, 2.0483, 1.4613],\n",
      "        [1.8329, 2.1757, 2.2353, 1.9329, 1.6594],\n",
      "        [1.4329, 1.6204, 1.2613, 2.7805, 2.9166]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'{2*a=}\\n')\n",
    "print(f'{2*a+1=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9585, 0.8137, 0.7837, 0.3117, 0.5308],\n",
       "         [0.5991, 0.7310, 0.9645, 0.7374, 0.7492],\n",
       "         [0.5522, 0.8426, 0.0137, 0.8135, 0.0080]], requires_grad=True),\n",
       " tensor([[0.0779, 0.3624, 0.0938, 0.1770, 0.2233],\n",
       "         [0.3437, 0.3984, 0.9332, 0.7502, 0.0515],\n",
       "         [0.3722, 0.6910, 0.0804, 0.5425, 0.1549]]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=False)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1571, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # all ones\n",
    "print(f'{b.grad=}\\n')  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4873, 0.0094, 0.3410, 0.6388, 0.3698],\n",
       "         [0.3318, 0.3989, 0.4540, 0.8396, 0.7872],\n",
       "         [0.2356, 0.5495, 0.3216, 0.0462, 0.1471]], requires_grad=True),\n",
       " tensor([[0.9219, 0.8603, 0.0196, 0.7148, 0.3156],\n",
       "         [0.4160, 0.4039, 0.9161, 0.9858, 0.4589],\n",
       "         [0.9566, 0.1626, 0.5449, 0.9074, 0.7661]], requires_grad=True))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.3924)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4194, 0.7270, 0.6554, 0.8120, 0.2465],\n",
       "         [0.3338, 0.5301, 0.4804, 0.7732, 0.9213],\n",
       "         [0.9066, 0.9495, 0.0488, 0.6136, 0.7182]], requires_grad=True),\n",
       " tensor([[0.2233, 0.5705, 0.0119, 0.8199, 0.7503],\n",
       "         [0.4163, 0.5326, 0.9683, 0.9022, 0.6971],\n",
       "         [0.3597, 0.9322, 0.4627, 0.8851, 0.0571]], requires_grad=True))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5469)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(15.7352)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.7352, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(19.4477)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [113]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.4477)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(0.9730)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0091, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.0910)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0245)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Полносвязные слои и функции активации в `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Полносвязный слой\n",
    "\n",
    ">$y_j = \\sum\\limits_{i=1}^{n}x_iw_{ji} + b_j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=3, bias=True)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1036, -0.0857,  0.1891,  0.1899,  0.0061],\n",
       "        [-0.2206, -0.1663, -0.0230, -0.0566, -0.0549],\n",
       "        [-0.1880, -0.2969, -0.1313,  0.2695,  0.4457]], requires_grad=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1110, -0.0980, -0.0736], requires_grad=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3352,  0.3743,  0.8121], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Сигмоида $f(x) = \\dfrac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7887, -2.3923,  0.1006, -1.6202,  1.0863])\n",
      "tensor([0.1432, 0.0838, 0.5251, 0.1652, 0.7477])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ReLU $f(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6977,  0.5788, -1.0462, -0.7537,  0.8037])\n",
      "tensor([0.0000, 0.5788, 0.0000, 0.0000, 0.8037])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Leaky ReLU $f(x) = \\max(0, x) + \\alpha \\min(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.LeakyReLU(negative_slope=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4079, -0.8732,  0.4297,  0.5221,  0.3086])\n",
      "tensor([ 1.4079e+00, -8.7321e-04,  4.2974e-01,  5.2209e-01,  3.0863e-01])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "b_true = torch.randn(1)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects) + b_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "step_size = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 23.65750\n",
      "MSE на шаге 2 15.53605\n",
      "MSE на шаге 3 12.11575\n",
      "MSE на шаге 4 10.36995\n",
      "MSE на шаге 5 9.30175\n",
      "MSE на шаге 6 8.55659\n",
      "MSE на шаге 7 7.99169\n",
      "MSE на шаге 8 7.53972\n",
      "MSE на шаге 9 7.16377\n",
      "MSE на шаге 10 6.84131\n",
      "MSE на шаге 11 6.55765\n",
      "MSE на шаге 12 6.30292\n",
      "MSE на шаге 13 6.07033\n",
      "MSE на шаге 14 5.85515\n",
      "MSE на шаге 15 5.65409\n",
      "MSE на шаге 16 5.46479\n",
      "MSE на шаге 17 5.28555\n",
      "MSE на шаге 18 5.11514\n",
      "MSE на шаге 19 4.95265\n",
      "MSE на шаге 20 4.79735\n",
      "MSE на шаге 21 4.64871\n",
      "MSE на шаге 31 3.45555\n",
      "MSE на шаге 41 2.65631\n",
      "MSE на шаге 51 2.11964\n",
      "MSE на шаге 61 1.75925\n",
      "MSE на шаге 71 1.51724\n",
      "MSE на шаге 81 1.35471\n",
      "MSE на шаге 91 1.24558\n",
      "MSE на шаге 101 1.17229\n",
      "MSE на шаге 111 1.12307\n",
      "MSE на шаге 121 1.09002\n",
      "MSE на шаге 131 1.06782\n",
      "MSE на шаге 141 1.05292\n",
      "MSE на шаге 151 1.04291\n",
      "MSE на шаге 161 1.03619\n",
      "MSE на шаге 171 1.03168\n",
      "MSE на шаге 181 1.02864\n",
      "MSE на шаге 191 1.02661\n"
     ]
    }
   ],
   "source": [
    "w = torch.rand(n_features, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = torch.matmul(x, w) + b\n",
    "    \n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * step_size\n",
    "        b -= b.grad * step_size\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 67.76115\n",
      "MSE на шаге 2 42.90440\n",
      "MSE на шаге 3 34.78294\n",
      "MSE на шаге 4 31.90576\n",
      "MSE на шаге 5 30.72448\n",
      "MSE на шаге 6 30.12742\n",
      "MSE на шаге 7 29.75548\n",
      "MSE на шаге 8 29.48539\n",
      "MSE на шаге 9 29.27014\n",
      "MSE на шаге 10 29.08890\n",
      "MSE на шаге 11 28.93083\n",
      "MSE на шаге 12 28.78951\n",
      "MSE на шаге 13 28.66082\n",
      "MSE на шаге 14 28.54199\n",
      "MSE на шаге 15 28.43109\n",
      "MSE на шаге 16 28.32677\n",
      "MSE на шаге 17 28.22807\n",
      "MSE на шаге 18 28.13428\n",
      "MSE на шаге 19 28.04487\n",
      "MSE на шаге 20 27.95945\n",
      "MSE на шаге 21 27.87770\n",
      "MSE на шаге 31 27.22173\n",
      "MSE на шаге 41 26.78238\n",
      "MSE на шаге 51 26.48738\n",
      "MSE на шаге 61 26.28927\n",
      "MSE на шаге 71 26.15623\n",
      "MSE на шаге 81 26.06689\n",
      "MSE на шаге 91 26.00690\n",
      "MSE на шаге 101 25.96661\n",
      "MSE на шаге 111 25.93955\n",
      "MSE на шаге 121 25.92138\n",
      "MSE на шаге 131 25.90918\n",
      "MSE на шаге 141 25.90099\n",
      "MSE на шаге 151 25.89549\n",
      "MSE на шаге 161 25.89179\n",
      "MSE на шаге 171 25.88931\n",
      "MSE на шаге 181 25.88764\n",
      "MSE на шаге 191 25.88653\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x)\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "#     layer.weight.grad.zero_()\n",
    "#     layer.bias.grad.zero_()\n",
    "    \n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7298],\n",
       "        [-2.7292],\n",
       "        [-2.7318],\n",
       "        [-2.7311],\n",
       "        [-2.7278],\n",
       "        [-2.7308],\n",
       "        [-2.7292],\n",
       "        [-2.7330],\n",
       "        [-2.7296],\n",
       "        [-2.7168],\n",
       "        [-2.7313],\n",
       "        [-2.7349],\n",
       "        [-2.7305],\n",
       "        [-2.7286],\n",
       "        [-2.7363],\n",
       "        [-2.7269],\n",
       "        [-2.7251],\n",
       "        [-2.7222],\n",
       "        [-2.7308],\n",
       "        [-2.7260],\n",
       "        [-2.7310],\n",
       "        [-2.7279],\n",
       "        [-2.7256],\n",
       "        [-2.7202],\n",
       "        [-2.7229],\n",
       "        [-2.7306],\n",
       "        [-2.7265],\n",
       "        [-2.7337],\n",
       "        [-2.7233],\n",
       "        [-2.7287],\n",
       "        [-2.7279],\n",
       "        [-2.7318],\n",
       "        [-2.7289],\n",
       "        [-2.7257],\n",
       "        [-2.7223],\n",
       "        [-2.7243],\n",
       "        [-2.7390],\n",
       "        [-2.7176],\n",
       "        [-2.7257],\n",
       "        [-2.7269],\n",
       "        [-2.7223],\n",
       "        [-2.7276],\n",
       "        [-2.7244],\n",
       "        [-2.7321],\n",
       "        [-2.7252],\n",
       "        [-2.7309],\n",
       "        [-2.7242],\n",
       "        [-2.7299],\n",
       "        [-2.7348],\n",
       "        [-2.7298],\n",
       "        [-2.7290],\n",
       "        [-2.7337],\n",
       "        [-2.7367],\n",
       "        [-2.7189],\n",
       "        [-2.7300],\n",
       "        [-2.7280],\n",
       "        [-2.7253],\n",
       "        [-2.7337],\n",
       "        [-2.7322],\n",
       "        [-2.7274],\n",
       "        [-2.7266],\n",
       "        [-2.7305],\n",
       "        [-2.7325],\n",
       "        [-2.7221],\n",
       "        [-2.7273],\n",
       "        [-2.7294],\n",
       "        [-2.7239],\n",
       "        [-2.7206],\n",
       "        [-2.7323],\n",
       "        [-2.7229],\n",
       "        [-2.7279],\n",
       "        [-2.7308],\n",
       "        [-2.7320],\n",
       "        [-2.7193],\n",
       "        [-2.7196],\n",
       "        [-2.7275],\n",
       "        [-2.7284],\n",
       "        [-2.7304],\n",
       "        [-2.7291],\n",
       "        [-2.7330],\n",
       "        [-2.7342],\n",
       "        [-2.7195],\n",
       "        [-2.7230],\n",
       "        [-2.7376],\n",
       "        [-2.7266],\n",
       "        [-2.7182],\n",
       "        [-2.7261],\n",
       "        [-2.7240],\n",
       "        [-2.7260],\n",
       "        [-2.7287],\n",
       "        [-2.7365],\n",
       "        [-2.7253],\n",
       "        [-2.7333],\n",
       "        [-2.7305],\n",
       "        [-2.7338],\n",
       "        [-2.7322],\n",
       "        [-2.7306],\n",
       "        [-2.7302],\n",
       "        [-2.7355],\n",
       "        [-2.7345],\n",
       "        [-2.7258],\n",
       "        [-2.7270],\n",
       "        [-2.7259],\n",
       "        [-2.7286],\n",
       "        [-2.7312],\n",
       "        [-2.7181],\n",
       "        [-2.7331],\n",
       "        [-2.7254],\n",
       "        [-2.7366],\n",
       "        [-2.7318],\n",
       "        [-2.7297],\n",
       "        [-2.7317],\n",
       "        [-2.7312],\n",
       "        [-2.7204],\n",
       "        [-2.7212],\n",
       "        [-2.7324],\n",
       "        [-2.7290],\n",
       "        [-2.7303],\n",
       "        [-2.7195],\n",
       "        [-2.7187],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7204],\n",
       "        [-2.7305],\n",
       "        [-2.7373],\n",
       "        [-2.7283],\n",
       "        [-2.7349],\n",
       "        [-2.7276],\n",
       "        [-2.7236],\n",
       "        [-2.7314],\n",
       "        [-2.7231],\n",
       "        [-2.7292],\n",
       "        [-2.7359],\n",
       "        [-2.7278],\n",
       "        [-2.7184],\n",
       "        [-2.7301],\n",
       "        [-2.7245],\n",
       "        [-2.7376],\n",
       "        [-2.7306],\n",
       "        [-2.7318],\n",
       "        [-2.7294],\n",
       "        [-2.7340],\n",
       "        [-2.7255],\n",
       "        [-2.7266],\n",
       "        [-2.7193],\n",
       "        [-2.7233],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7229],\n",
       "        [-2.7287],\n",
       "        [-2.7244],\n",
       "        [-2.7332],\n",
       "        [-2.7236],\n",
       "        [-2.7283],\n",
       "        [-2.7318],\n",
       "        [-2.7252],\n",
       "        [-2.7279],\n",
       "        [-2.7341],\n",
       "        [-2.7271],\n",
       "        [-2.7242],\n",
       "        [-2.7222],\n",
       "        [-2.7299],\n",
       "        [-2.7236],\n",
       "        [-2.7349],\n",
       "        [-2.7271],\n",
       "        [-2.7251],\n",
       "        [-2.7279],\n",
       "        [-2.7245],\n",
       "        [-2.7366],\n",
       "        [-2.7269],\n",
       "        [-2.7339],\n",
       "        [-2.7342],\n",
       "        [-2.7311],\n",
       "        [-2.7286],\n",
       "        [-2.7327],\n",
       "        [-2.7341],\n",
       "        [-2.7282],\n",
       "        [-2.7376],\n",
       "        [-2.7284],\n",
       "        [-2.7178],\n",
       "        [-2.7322],\n",
       "        [-2.7360],\n",
       "        [-2.7312],\n",
       "        [-2.7323],\n",
       "        [-2.7257],\n",
       "        [-2.7263],\n",
       "        [-2.7201],\n",
       "        [-2.7289],\n",
       "        [-2.7302],\n",
       "        [-2.7270],\n",
       "        [-2.7328],\n",
       "        [-2.7259],\n",
       "        [-2.7294],\n",
       "        [-2.7368],\n",
       "        [-2.7221],\n",
       "        [-2.7358],\n",
       "        [-2.7235],\n",
       "        [-2.7357],\n",
       "        [-2.7213],\n",
       "        [-2.7244],\n",
       "        [-2.7333],\n",
       "        [-2.7313],\n",
       "        [-2.7218],\n",
       "        [-2.7315],\n",
       "        [-2.7290],\n",
       "        [-2.7316],\n",
       "        [-2.7246],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7257],\n",
       "        [-2.7328],\n",
       "        [-2.7349],\n",
       "        [-2.7220],\n",
       "        [-2.7311],\n",
       "        [-2.7227],\n",
       "        [-2.7199],\n",
       "        [-2.7304],\n",
       "        [-2.7326],\n",
       "        [-2.7388],\n",
       "        [-2.7322],\n",
       "        [-2.7233],\n",
       "        [-2.7243],\n",
       "        [-2.7230],\n",
       "        [-2.7367],\n",
       "        [-2.7298],\n",
       "        [-2.7324],\n",
       "        [-2.7333],\n",
       "        [-2.7226],\n",
       "        [-2.7281],\n",
       "        [-2.7237],\n",
       "        [-2.7322],\n",
       "        [-2.7293],\n",
       "        [-2.7231],\n",
       "        [-2.7207],\n",
       "        [-2.7364],\n",
       "        [-2.7344],\n",
       "        [-2.7308],\n",
       "        [-2.7364],\n",
       "        [-2.7323],\n",
       "        [-2.7260],\n",
       "        [-2.7262],\n",
       "        [-2.7301],\n",
       "        [-2.7209],\n",
       "        [-2.7212],\n",
       "        [-2.7315],\n",
       "        [-2.7257],\n",
       "        [-2.7381],\n",
       "        [-2.7270],\n",
       "        [-2.7191],\n",
       "        [-2.7275],\n",
       "        [-2.7317],\n",
       "        [-2.7330],\n",
       "        [-2.7282],\n",
       "        [-2.7277],\n",
       "        [-2.7319],\n",
       "        [-2.7232],\n",
       "        [-2.7245],\n",
       "        [-2.7219],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7256],\n",
       "        [-2.7252],\n",
       "        [-2.7296],\n",
       "        [-2.7335],\n",
       "        [-2.7267],\n",
       "        [-2.7314],\n",
       "        [-2.7328],\n",
       "        [-2.7273],\n",
       "        [-2.7308],\n",
       "        [-2.7331],\n",
       "        [-2.7353],\n",
       "        [-2.7304],\n",
       "        [-2.7176],\n",
       "        [-2.7261],\n",
       "        [-2.7284],\n",
       "        [-2.7234],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7371],\n",
       "        [-2.7287],\n",
       "        [-2.7245],\n",
       "        [-2.7244],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7260],\n",
       "        [-2.7295],\n",
       "        [-2.7261],\n",
       "        [-2.7358],\n",
       "        [-2.7261],\n",
       "        [-2.7282],\n",
       "        [-2.7257],\n",
       "        [-2.7248],\n",
       "        [-2.7390],\n",
       "        [-2.7267],\n",
       "        [-2.7387],\n",
       "        [-2.7252],\n",
       "        [-2.7323],\n",
       "        [-2.7375],\n",
       "        [-2.7289],\n",
       "        [-2.7218]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 300])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x) - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x).ravel() - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 82.40031\n",
      "MSE на шаге 2 34.83059\n",
      "MSE на шаге 3 18.67639\n",
      "MSE на шаге 4 12.57786\n",
      "MSE на шаге 5 9.87140\n",
      "MSE на шаге 6 8.42349\n",
      "MSE на шаге 7 7.51441\n",
      "MSE на шаге 8 6.87739\n",
      "MSE на шаге 9 6.39865\n",
      "MSE на шаге 10 6.02125\n",
      "MSE на шаге 11 5.71254\n",
      "MSE на шаге 12 5.45201\n",
      "MSE на шаге 13 5.22616\n",
      "MSE на шаге 14 5.02584\n",
      "MSE на шаге 15 4.84476\n",
      "MSE на шаге 16 4.67857\n",
      "MSE на шаге 17 4.52421\n",
      "MSE на шаге 18 4.37955\n",
      "MSE на шаге 19 4.24304\n",
      "MSE на шаге 20 4.11358\n",
      "MSE на шаге 21 3.99035\n",
      "MSE на шаге 31 3.01138\n",
      "MSE на шаге 41 2.35799\n",
      "MSE на шаге 51 1.91931\n",
      "MSE на шаге 61 1.62472\n",
      "MSE на шаге 71 1.42689\n",
      "MSE на шаге 81 1.29405\n",
      "MSE на шаге 91 1.20484\n",
      "MSE на шаге 101 1.14493\n",
      "MSE на шаге 111 1.10470\n",
      "MSE на шаге 121 1.07768\n",
      "MSE на шаге 131 1.05954\n",
      "MSE на шаге 141 1.04736\n",
      "MSE на шаге 151 1.03917\n",
      "MSE на шаге 161 1.03368\n",
      "MSE на шаге 171 1.02999\n",
      "MSE на шаге 181 1.02751\n",
      "MSE на шаге 191 1.02585\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 2052.12988\n",
      "MSE на шаге 2 606.21967\n",
      "MSE на шаге 3 207.55234\n",
      "MSE на шаге 4 82.47279\n",
      "MSE на шаге 5 40.03379\n",
      "MSE на шаге 6 24.82249\n",
      "MSE на шаге 7 19.02928\n",
      "MSE на шаге 8 16.58483\n",
      "MSE на шаге 9 15.35648\n",
      "MSE на шаге 10 14.58259\n",
      "MSE на шаге 11 13.98817\n",
      "MSE на шаге 12 13.47272\n",
      "MSE на шаге 13 12.99872\n",
      "MSE на шаге 14 12.55160\n",
      "MSE на шаге 15 12.12528\n",
      "MSE на шаге 16 11.71682\n",
      "MSE на шаге 17 11.32460\n",
      "MSE на шаге 18 10.94749\n",
      "MSE на шаге 19 10.58467\n",
      "MSE на шаге 20 10.23542\n",
      "MSE на шаге 51 3.92278\n",
      "MSE на шаге 101 1.43192\n",
      "MSE на шаге 151 1.02777\n",
      "MSE на шаге 201 0.95445\n",
      "MSE на шаге 251 0.93498\n",
      "MSE на шаге 301 0.92541\n",
      "MSE на шаге 351 0.91856\n",
      "MSE на шаге 401 0.91310\n",
      "MSE на шаге 451 0.90864\n",
      "MSE на шаге 501 0.90498\n",
      "MSE на шаге 551 0.90198\n",
      "MSE на шаге 601 0.89951\n",
      "MSE на шаге 651 0.89748\n",
      "MSE на шаге 701 0.89582\n",
      "MSE на шаге 751 0.89445\n",
      "MSE на шаге 801 0.89333\n",
      "MSE на шаге 851 0.89241\n",
      "MSE на шаге 901 0.89165\n",
      "MSE на шаге 951 0.89103\n",
      "MSE на шаге 1001 0.89052\n",
      "MSE на шаге 1051 0.89010\n",
      "MSE на шаге 1101 0.88976\n",
      "MSE на шаге 1151 0.88948\n",
      "MSE на шаге 1201 0.88925\n",
      "MSE на шаге 1251 0.88906\n",
      "MSE на шаге 1301 0.88890\n",
      "MSE на шаге 1351 0.88877\n",
      "MSE на шаге 1401 0.88867\n",
      "MSE на шаге 1451 0.88858\n",
      "MSE на шаге 1501 0.88851\n",
      "MSE на шаге 1551 0.88845\n",
      "MSE на шаге 1601 0.88840\n",
      "MSE на шаге 1651 0.88836\n",
      "MSE на шаге 1701 0.88833\n",
      "MSE на шаге 1751 0.88830\n",
      "MSE на шаге 1801 0.88828\n",
      "MSE на шаге 1851 0.88826\n",
      "MSE на шаге 1901 0.88825\n",
      "MSE на шаге 1951 0.88824\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 1942.53650\n",
      "MSE на шаге 2 1676.33215\n",
      "MSE на шаге 3 1462.73621\n",
      "MSE на шаге 4 1252.29272\n",
      "MSE на шаге 5 1084.68542\n",
      "MSE на шаге 6 969.74554\n",
      "MSE на шаге 7 892.42969\n",
      "MSE на шаге 8 837.99945\n",
      "MSE на шаге 9 791.54950\n",
      "MSE на шаге 10 736.85345\n",
      "MSE на шаге 11 660.66931\n",
      "MSE на шаге 12 552.94324\n",
      "MSE на шаге 13 420.34772\n",
      "MSE на шаге 14 295.80695\n",
      "MSE на шаге 15 213.59764\n",
      "MSE на шаге 16 170.35097\n",
      "MSE на шаге 17 140.10820\n",
      "MSE на шаге 18 116.49634\n",
      "MSE на шаге 19 95.18255\n",
      "MSE на шаге 20 74.59748\n",
      "MSE на шаге 51 9.93147\n",
      "MSE на шаге 101 3.76550\n",
      "MSE на шаге 151 1.81388\n",
      "MSE на шаге 201 1.21714\n",
      "MSE на шаге 251 1.03567\n",
      "MSE на шаге 301 0.97787\n",
      "MSE на шаге 351 0.95785\n",
      "MSE на шаге 401 0.94890\n",
      "MSE на шаге 451 0.94261\n",
      "MSE на шаге 501 0.93729\n",
      "MSE на шаге 551 0.93249\n",
      "MSE на шаге 601 0.92810\n",
      "MSE на шаге 651 0.92405\n",
      "MSE на шаге 701 0.92031\n",
      "MSE на шаге 751 0.91686\n",
      "MSE на шаге 801 0.91367\n",
      "MSE на шаге 851 0.91071\n",
      "MSE на шаге 901 0.90798\n",
      "MSE на шаге 951 0.90545\n",
      "MSE на шаге 1001 0.90310\n",
      "MSE на шаге 1051 0.90093\n",
      "MSE на шаге 1101 0.89892\n",
      "MSE на шаге 1151 0.89705\n",
      "MSE на шаге 1201 0.89531\n",
      "MSE на шаге 1251 0.89371\n",
      "MSE на шаге 1301 0.89224\n",
      "MSE на шаге 1351 0.89088\n",
      "MSE на шаге 1401 0.88962\n",
      "MSE на шаге 1451 0.88845\n",
      "MSE на шаге 1501 0.88737\n",
      "MSE на шаге 1551 0.88636\n",
      "MSE на шаге 1601 0.88543\n",
      "MSE на шаге 1651 0.88456\n",
      "MSE на шаге 1701 0.88375\n",
      "MSE на шаге 1751 0.88300\n",
      "MSE на шаге 1801 0.88231\n",
      "MSE на шаге 1851 0.88166\n",
      "MSE на шаге 1901 0.88105\n",
      "MSE на шаге 1951 0.88049\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=n_features, out_features=3)\n",
    "layer2 = nn.Linear(in_features=3, out_features=1)\n",
    "activation = nn.ReLU()\n",
    "\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer2(activation(layer1(x))).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer1.weight -= layer1.weight.grad * step_size\n",
    "        layer1.bias -= layer1.bias.grad * step_size\n",
    "        layer2.weight -= layer2.weight.grad * step_size\n",
    "        layer2.bias -= layer2.bias.grad * step_size\n",
    "\n",
    "    layer1.zero_grad()\n",
    "    layer2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Seminar 1. Intro to DL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
