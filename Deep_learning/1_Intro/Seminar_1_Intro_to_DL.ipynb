{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Введение. Полносвязные слои. Функции активации (ноутбук)\n",
    "\n",
    "> Начнем осваивать библиотеку `PyTorch`. Познакомимся с нейронными сетями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## План ноутбука\n",
    "\n",
    "1. Установка `PyTorch`\n",
    "1. Введение в `PyTorch`\n",
    "1. Полносвязные слои и функции активации в `PyTorch`\n",
    "1. Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Установка `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы будем использовать библиотеку для глубинного обучения `PyTorch`, ее можно не устанавливать, будем пользоваться сайтом [kaggle.com](kaggle.com) для обучения в облаке (или с учителем?). \n",
    "\n",
    "Чтобы установить `PyTorch` локально себе на компьютер нужно ответить на два вопроса - какая у вас операционная система и есть ли у вас дискретная видеокарта (GPU) и если есть, то какого производителя. В зависимости от ваших ответов мы получаем три варианта по операционной системе - Linux, Mac и Windows; три варианта по дискретной видеокарте - нет видеокарты (доступен только центральный процессор CPU), есть видеокарта от Nvidia или есть видеокарта от AMD (это производитель именно чипа, конечный вендор может быть другой, например, ASUS, MSI, Palit). Работа с PyTorch с видеокартой от AMD это экзотика, которая выходит за рамки нашего курса, поэтому рассмотрим только варианты *нет видеокарты*/*есть видеокарта от Nvidia*.\n",
    "\n",
    "\n",
    "Выберите на [сайте](https://pytorch.org/get-started/locally/) подходящие вам варианты операционной системы/видеокарты и скопируйте команду для установки. Разберем подробно самые популярные варианты установки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Linux ([поддерживаемые дистрибутивы](https://pytorch.org/get-started/locally/#supported-linux-distributions))\n",
    "\n",
    "На линуксе будет работать поддержка `PyTorch` в любой конфигурации, что у вас нет видеокарты, что есть от Nvidia, что от AMD. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Windows\n",
    "\n",
    "На винде будет работать поддержка `PyTorch` только для видеокарт от Nvidia и без видеокарт вообще. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка на Mac\n",
    "\n",
    "На маках есть пока что поддержка `PyTorch` только центрального процессора, чуть позже появится поддержка ускорения на чипах M1, M2, M1 Pro и так далее.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` \n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Введение в `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Тензоры\n",
    "\n",
    "Тензоры — это специализированная структура данных, по сути это массивы и матрицы. Тензоры очень похожи на массивы в numpy, так что, если у вас хорошо с numpy, то разобраться в PyTorch тензорах будет очень просто. В PyTorch мы используем тензоры для кодирования входных и выходных данных модели, а также параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание тензоров\n",
    "\n",
    "Тензор можно создать напрямую из каких-то данных - нам подходят все списки с числами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [1, 2, 3, 4]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[1, 2], [3, 4], [5, 6]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле про \"все\" списки с числами - обман. Если у вашего списка есть какой-то уровень вложенности, то должны совпадать размерности у всех вложенных списков (подробнее про размерности поговорим позже):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m some_other_data \u001b[39m=\u001b[39m [[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m], [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m], [\u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m, \u001b[39m7\u001b[39m]]\n\u001b[1;32m----> 2\u001b[0m some_other_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(some_other_data)\n\u001b[0;32m      4\u001b[0m some_other_tensor\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 2 at dim 1 (got 3)"
     ]
    }
   ],
   "source": [
    "some_other_data = [[1, 2], [3, 4], [5, 6, 7]]\n",
    "some_other_tensor = torch.tensor(some_other_data)\n",
    "\n",
    "some_other_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также тензоры можно создавать из numpy массивов и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [2]],\n",
       "\n",
       "       [[3],\n",
       "        [4]],\n",
       "\n",
       "       [[5],\n",
       "        [6]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_numpy_array = np.array(some_data)\n",
    "\n",
    "some_numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [4]],\n",
       "\n",
       "        [[5],\n",
       "         [6]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor_from_numpy = torch.from_numpy(some_numpy_array)\n",
    "\n",
    "some_tensor_from_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом если мы создаем тензор из numpy массива с помощью `torch.from_numpy`, то они делят между собой память, где лежат их данные и, соответственно, при изменении тензора меняется numpy массив и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(10)\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10)\n",
    "y = x.numpy()\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем создать тензор со случайными или константными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7027, 0.5529, 0.6092],\n",
       "         [0.2448, 0.8254, 0.3980]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 5.3950e-43]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "\n",
    "random_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "empty_tensor = torch.empty(shape)\n",
    "\n",
    "random_tensor, ones_tensor, zeros_tensor, empty_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про размерности подробнее.\n",
    "\n",
    "У тензора есть какой-то размер, какая форма. Первое с чем нужно определиться, какой **размерности** тензор - количество осей у него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8687, 0.7276, 0.5675, 0.5139, 0.4769, 0.2755, 0.8285, 0.7817, 0.9749,\n",
       "        0.6515])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (10)  # одна ось (вектор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4513, 0.7931, 0.7372],\n",
       "        [0.4958, 0.5700, 0.1145]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)  # две оси (матрица)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5925, 0.6564, 0.3615],\n",
       "         [0.5864, 0.0921, 0.1682]],\n",
       "\n",
       "        [[0.0072, 0.0686, 0.9264],\n",
       "         [0.0090, 0.7570, 0.7957]],\n",
       "\n",
       "        [[0.5115, 0.0161, 0.5309],\n",
       "         [0.9625, 0.1506, 0.2029]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (3, 2, 3)  # три оси (и больше - тензор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензор с размерностью 1 - это просто вектор, список чисел.\n",
    "\n",
    "Тензор с размерностью 2 - это просто матрица, то есть список списков чисел.\n",
    "\n",
    "Тензор с размерностью 3 и больше - это тензор, то есть список списков списков ... чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить доступ к размеру уже созданного тензора - метод `.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4]],\n",
      "\n",
      "        [[5],\n",
      "         [6]]])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "print(some_tensor)\n",
    "print(some_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лекции мы говорили про изображения, давайте сделаем тензор, который будет нам имитировать изображение - сделаем его размер `(c, h, w)`, где `h` и `w` это его высота и ширина, а `c` - число каналов в цветовом пространстве (в черно-белом 1, в RGB 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5100, 0.2477, 0.0406, 0.9548, 0.6132, 0.4432, 0.5544, 0.7823,\n",
       "          0.9526, 0.3649, 0.4308, 0.3078, 0.5028, 0.1789, 0.6044, 0.0305],\n",
       "         [0.7153, 0.1671, 0.0436, 0.7312, 0.7102, 0.6099, 0.4297, 0.8649,\n",
       "          0.5430, 0.4407, 0.1450, 0.2762, 0.5721, 0.6189, 0.4304, 0.6830],\n",
       "         [0.5985, 0.3754, 0.2829, 0.3718, 0.0563, 0.5774, 0.7259, 0.1074,\n",
       "          0.2567, 0.4655, 0.5426, 0.4860, 0.6088, 0.5652, 0.0466, 0.1031],\n",
       "         [0.9086, 0.5898, 0.5975, 0.1507, 0.0381, 0.8413, 0.4975, 0.5298,\n",
       "          0.5358, 0.8261, 0.1266, 0.2165, 0.4506, 0.5299, 0.4746, 0.9859],\n",
       "         [0.8806, 0.9596, 0.9185, 0.0546, 0.0504, 0.2414, 0.7865, 0.1819,\n",
       "          0.6420, 0.5789, 0.7096, 0.4473, 0.3449, 0.1673, 0.9018, 0.9330],\n",
       "         [0.7165, 0.3362, 0.7926, 0.0353, 0.5123, 0.2261, 0.4139, 0.1031,\n",
       "          0.0070, 0.2250, 0.0660, 0.6726, 0.4404, 0.4773, 0.2850, 0.4955],\n",
       "         [0.4342, 0.2390, 0.2934, 0.0827, 0.4543, 0.0459, 0.4550, 0.3946,\n",
       "          0.3679, 0.6149, 0.6366, 0.4439, 0.1183, 0.4071, 0.8366, 0.7255],\n",
       "         [0.4250, 0.4521, 0.9120, 0.8928, 0.5991, 0.5098, 0.2141, 0.1905,\n",
       "          0.2099, 0.7261, 0.1607, 0.2712, 0.1660, 0.2644, 0.1629, 0.7032],\n",
       "         [0.8785, 0.0399, 0.3405, 0.2314, 0.5367, 0.9538, 0.1439, 0.7502,\n",
       "          0.0681, 0.2648, 0.8375, 0.5400, 0.7900, 0.8889, 0.9721, 0.9726]],\n",
       "\n",
       "        [[0.7483, 0.5898, 0.7789, 0.1219, 0.1126, 0.2357, 0.2340, 0.9047,\n",
       "          0.1921, 0.2386, 0.6158, 0.1767, 0.6880, 0.2048, 0.3879, 0.1561],\n",
       "         [0.2166, 0.0373, 0.8911, 0.9663, 0.2565, 0.3026, 0.4328, 0.5419,\n",
       "          0.2764, 0.3468, 0.0377, 0.2724, 0.0139, 0.6565, 0.2963, 0.3711],\n",
       "         [0.0579, 0.1890, 0.0994, 0.4920, 0.0283, 0.4694, 0.3632, 0.2843,\n",
       "          0.8035, 0.2117, 0.9128, 0.5349, 0.5473, 0.6513, 0.8846, 0.5762],\n",
       "         [0.1668, 0.3712, 0.3396, 0.9759, 0.1093, 0.9428, 0.9123, 0.9524,\n",
       "          0.9093, 0.8473, 0.1435, 0.6356, 0.4882, 0.5472, 0.0846, 0.3585],\n",
       "         [0.7293, 0.9854, 0.3041, 0.8691, 0.4499, 0.0339, 0.6111, 0.6623,\n",
       "          0.9317, 0.2726, 0.9612, 0.0476, 0.8030, 0.5151, 0.2306, 0.7556],\n",
       "         [0.3420, 0.9077, 0.3250, 0.5603, 0.8836, 0.9816, 0.6437, 0.9288,\n",
       "          0.9523, 0.1491, 0.7632, 0.5532, 0.9450, 0.1149, 0.7404, 0.9535],\n",
       "         [0.5110, 0.4248, 0.5215, 0.3713, 0.0351, 0.0982, 0.5961, 0.3889,\n",
       "          0.0734, 0.5612, 0.2977, 0.6462, 0.2024, 0.0256, 0.5740, 0.4662],\n",
       "         [0.8076, 0.1828, 0.0709, 0.2763, 0.6600, 0.3755, 0.9726, 0.3242,\n",
       "          0.3240, 0.2719, 0.0358, 0.2569, 0.6358, 0.1262, 0.8805, 0.2290],\n",
       "         [0.0251, 0.3437, 0.6041, 0.5163, 0.1598, 0.3291, 0.4935, 0.3619,\n",
       "          0.6391, 0.0672, 0.6304, 0.9526, 0.2894, 0.1020, 0.2934, 0.5021]],\n",
       "\n",
       "        [[0.3750, 0.8641, 0.6428, 0.2147, 0.1505, 0.6797, 0.3281, 0.4346,\n",
       "          0.6701, 0.9275, 0.2201, 0.4869, 0.5746, 0.1994, 0.2517, 0.7517],\n",
       "         [0.9454, 0.0899, 0.2590, 0.6337, 0.8217, 0.8949, 0.2239, 0.9714,\n",
       "          0.4770, 0.6997, 0.3871, 0.4779, 0.3871, 0.0228, 0.4397, 0.0249],\n",
       "         [0.6364, 0.9836, 0.5061, 0.6871, 0.3444, 0.5406, 0.9787, 0.4838,\n",
       "          0.5937, 0.3147, 0.1446, 0.0582, 0.7527, 0.7586, 0.6514, 0.4133],\n",
       "         [0.9637, 0.7261, 0.2054, 0.7593, 0.2465, 0.0498, 0.4853, 0.2684,\n",
       "          0.9600, 0.2758, 0.9711, 0.6017, 0.3929, 0.7001, 0.7712, 0.9029],\n",
       "         [0.9053, 0.5524, 0.7032, 0.5509, 0.4718, 0.3339, 0.1973, 0.4447,\n",
       "          0.5774, 0.5186, 0.8995, 0.8910, 0.4798, 0.5249, 0.8323, 0.7018],\n",
       "         [0.7091, 0.0949, 0.1316, 0.0693, 0.9053, 0.0316, 0.4177, 0.7253,\n",
       "          0.7812, 0.3154, 0.5314, 0.0994, 0.6908, 0.6316, 0.3792, 0.2027],\n",
       "         [0.6025, 0.1197, 0.6458, 0.1750, 0.1040, 0.3801, 0.4435, 0.9952,\n",
       "          0.5194, 0.6785, 0.2009, 0.0441, 0.5488, 0.4397, 0.9188, 0.0353],\n",
       "         [0.0064, 0.2179, 0.9660, 0.2259, 0.2853, 0.5485, 0.2750, 0.6907,\n",
       "          0.2082, 0.6696, 0.2666, 0.4140, 0.8665, 0.5394, 0.6306, 0.4097],\n",
       "         [0.3429, 0.0756, 0.2864, 0.4117, 0.3041, 0.7143, 0.6640, 0.1473,\n",
       "          0.5960, 0.2288, 0.7439, 0.7563, 0.4283, 0.1927, 0.1999, 0.5186]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 9\n",
    "w = 16\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9, 16])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем попробовать поменять размер тензора, например, [вытянуть его в вектор](https://pytorch.org/docs/stable/generated/torch.ravel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5100, 0.2477, 0.0406, 0.9548, 0.6132, 0.4432, 0.5544, 0.7823, 0.9526,\n",
       "        0.3649, 0.4308, 0.3078, 0.5028, 0.1789, 0.6044, 0.0305, 0.7153, 0.1671,\n",
       "        0.0436, 0.7312, 0.7102, 0.6099, 0.4297, 0.8649, 0.5430, 0.4407, 0.1450,\n",
       "        0.2762, 0.5721, 0.6189, 0.4304, 0.6830, 0.5985, 0.3754, 0.2829, 0.3718,\n",
       "        0.0563, 0.5774, 0.7259, 0.1074, 0.2567, 0.4655, 0.5426, 0.4860, 0.6088,\n",
       "        0.5652, 0.0466, 0.1031, 0.9086, 0.5898, 0.5975, 0.1507, 0.0381, 0.8413,\n",
       "        0.4975, 0.5298, 0.5358, 0.8261, 0.1266, 0.2165, 0.4506, 0.5299, 0.4746,\n",
       "        0.9859, 0.8806, 0.9596, 0.9185, 0.0546, 0.0504, 0.2414, 0.7865, 0.1819,\n",
       "        0.6420, 0.5789, 0.7096, 0.4473, 0.3449, 0.1673, 0.9018, 0.9330, 0.7165,\n",
       "        0.3362, 0.7926, 0.0353, 0.5123, 0.2261, 0.4139, 0.1031, 0.0070, 0.2250,\n",
       "        0.0660, 0.6726, 0.4404, 0.4773, 0.2850, 0.4955, 0.4342, 0.2390, 0.2934,\n",
       "        0.0827, 0.4543, 0.0459, 0.4550, 0.3946, 0.3679, 0.6149, 0.6366, 0.4439,\n",
       "        0.1183, 0.4071, 0.8366, 0.7255, 0.4250, 0.4521, 0.9120, 0.8928, 0.5991,\n",
       "        0.5098, 0.2141, 0.1905, 0.2099, 0.7261, 0.1607, 0.2712, 0.1660, 0.2644,\n",
       "        0.1629, 0.7032, 0.8785, 0.0399, 0.3405, 0.2314, 0.5367, 0.9538, 0.1439,\n",
       "        0.7502, 0.0681, 0.2648, 0.8375, 0.5400, 0.7900, 0.8889, 0.9721, 0.9726,\n",
       "        0.7483, 0.5898, 0.7789, 0.1219, 0.1126, 0.2357, 0.2340, 0.9047, 0.1921,\n",
       "        0.2386, 0.6158, 0.1767, 0.6880, 0.2048, 0.3879, 0.1561, 0.2166, 0.0373,\n",
       "        0.8911, 0.9663, 0.2565, 0.3026, 0.4328, 0.5419, 0.2764, 0.3468, 0.0377,\n",
       "        0.2724, 0.0139, 0.6565, 0.2963, 0.3711, 0.0579, 0.1890, 0.0994, 0.4920,\n",
       "        0.0283, 0.4694, 0.3632, 0.2843, 0.8035, 0.2117, 0.9128, 0.5349, 0.5473,\n",
       "        0.6513, 0.8846, 0.5762, 0.1668, 0.3712, 0.3396, 0.9759, 0.1093, 0.9428,\n",
       "        0.9123, 0.9524, 0.9093, 0.8473, 0.1435, 0.6356, 0.4882, 0.5472, 0.0846,\n",
       "        0.3585, 0.7293, 0.9854, 0.3041, 0.8691, 0.4499, 0.0339, 0.6111, 0.6623,\n",
       "        0.9317, 0.2726, 0.9612, 0.0476, 0.8030, 0.5151, 0.2306, 0.7556, 0.3420,\n",
       "        0.9077, 0.3250, 0.5603, 0.8836, 0.9816, 0.6437, 0.9288, 0.9523, 0.1491,\n",
       "        0.7632, 0.5532, 0.9450, 0.1149, 0.7404, 0.9535, 0.5110, 0.4248, 0.5215,\n",
       "        0.3713, 0.0351, 0.0982, 0.5961, 0.3889, 0.0734, 0.5612, 0.2977, 0.6462,\n",
       "        0.2024, 0.0256, 0.5740, 0.4662, 0.8076, 0.1828, 0.0709, 0.2763, 0.6600,\n",
       "        0.3755, 0.9726, 0.3242, 0.3240, 0.2719, 0.0358, 0.2569, 0.6358, 0.1262,\n",
       "        0.8805, 0.2290, 0.0251, 0.3437, 0.6041, 0.5163, 0.1598, 0.3291, 0.4935,\n",
       "        0.3619, 0.6391, 0.0672, 0.6304, 0.9526, 0.2894, 0.1020, 0.2934, 0.5021,\n",
       "        0.3750, 0.8641, 0.6428, 0.2147, 0.1505, 0.6797, 0.3281, 0.4346, 0.6701,\n",
       "        0.9275, 0.2201, 0.4869, 0.5746, 0.1994, 0.2517, 0.7517, 0.9454, 0.0899,\n",
       "        0.2590, 0.6337, 0.8217, 0.8949, 0.2239, 0.9714, 0.4770, 0.6997, 0.3871,\n",
       "        0.4779, 0.3871, 0.0228, 0.4397, 0.0249, 0.6364, 0.9836, 0.5061, 0.6871,\n",
       "        0.3444, 0.5406, 0.9787, 0.4838, 0.5937, 0.3147, 0.1446, 0.0582, 0.7527,\n",
       "        0.7586, 0.6514, 0.4133, 0.9637, 0.7261, 0.2054, 0.7593, 0.2465, 0.0498,\n",
       "        0.4853, 0.2684, 0.9600, 0.2758, 0.9711, 0.6017, 0.3929, 0.7001, 0.7712,\n",
       "        0.9029, 0.9053, 0.5524, 0.7032, 0.5509, 0.4718, 0.3339, 0.1973, 0.4447,\n",
       "        0.5774, 0.5186, 0.8995, 0.8910, 0.4798, 0.5249, 0.8323, 0.7018, 0.7091,\n",
       "        0.0949, 0.1316, 0.0693, 0.9053, 0.0316, 0.4177, 0.7253, 0.7812, 0.3154,\n",
       "        0.5314, 0.0994, 0.6908, 0.6316, 0.3792, 0.2027, 0.6025, 0.1197, 0.6458,\n",
       "        0.1750, 0.1040, 0.3801, 0.4435, 0.9952, 0.5194, 0.6785, 0.2009, 0.0441,\n",
       "        0.5488, 0.4397, 0.9188, 0.0353, 0.0064, 0.2179, 0.9660, 0.2259, 0.2853,\n",
       "        0.5485, 0.2750, 0.6907, 0.2082, 0.6696, 0.2666, 0.4140, 0.8665, 0.5394,\n",
       "        0.6306, 0.4097, 0.3429, 0.0756, 0.2864, 0.4117, 0.3041, 0.7143, 0.6640,\n",
       "        0.1473, 0.5960, 0.2288, 0.7439, 0.7563, 0.4283, 0.1927, 0.1999, 0.5186])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([432])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h * w * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество элементов в тензоре с помощью [специальной функции](https://pytorch.org/docs/stable/generated/torch.numel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0070, 0.6046, 0.4565],\n",
       "         [0.1605, 0.3145, 0.6660]],\n",
       "\n",
       "        [[0.6404, 0.1047, 0.7150],\n",
       "         [0.8990, 0.1279, 0.7996]],\n",
       "\n",
       "        [[0.9388, 0.8845, 0.6763],\n",
       "         [0.6908, 0.7298, 0.6172]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 2\n",
    "w = 3\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем поменять размер с помощью функции [reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0070, 0.6046, 0.4565, 0.1605, 0.3145, 0.6660],\n",
       "        [0.6404, 0.1047, 0.7150, 0.8990, 0.1279, 0.7996],\n",
       "        [0.9388, 0.8845, 0.6763, 0.6908, 0.7298, 0.6172]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.reshape(c, h * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем собрать из нескольких тензоров один большой:\n",
    "\n",
    "[torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3401, -0.2445,  0.6631],\n",
       "        [-1.0394,  1.6872, -1.0868]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3401, -0.2445,  0.6631],\n",
       "        [-1.0394,  1.6872, -1.0868],\n",
       "        [-1.3401, -0.2445,  0.6631],\n",
       "        [-1.0394,  1.6872, -1.0868],\n",
       "        [-1.3401, -0.2445,  0.6631],\n",
       "        [-1.0394,  1.6872, -1.0868]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3401, -0.2445,  0.6631, -1.3401, -0.2445,  0.6631, -1.3401, -0.2445,\n",
       "          0.6631],\n",
       "        [-1.0394,  1.6872, -1.0868, -1.0394,  1.6872, -1.0868, -1.0394,  1.6872,\n",
       "         -1.0868]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3044,  0.8267,  0.8755],\n",
      "        [-2.0112, -1.6120, -0.2867],\n",
      "        [-0.9586, -0.1464,  2.5886]])\n",
      "tensor([[ 0.1100,  0.6240,  1.5551],\n",
      "        [-0.2397, -0.4386, -0.7614],\n",
      "        [-0.5056,  0.7966, -0.3969],\n",
      "        [ 0.8284,  1.0298,  0.9057],\n",
      "        [-0.5706, -1.2841, -1.1202]])\n",
      "tensor([[ 0.3892, -2.6462,  0.7541]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3044,  0.8267,  0.8755],\n",
       "        [-2.0112, -1.6120, -0.2867],\n",
       "        [-0.9586, -0.1464,  2.5886],\n",
       "        [ 0.1100,  0.6240,  1.5551],\n",
       "        [-0.2397, -0.4386, -0.7614],\n",
       "        [-0.5056,  0.7966, -0.3969],\n",
       "        [ 0.8284,  1.0298,  0.9057],\n",
       "        [-0.5706, -1.2841, -1.1202],\n",
       "        [ 0.3892, -2.6462,  0.7541]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(5, 3)\n",
    "z = torch.randn(1, 3)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0174, -0.8509,  0.9030],\n",
      "        [ 1.3923,  1.5079,  0.9380]])\n",
      "tensor([[ 0.1474,  0.3277, -0.2032, -0.8069, -1.9375],\n",
      "        [ 0.0154, -0.7514, -1.0922,  0.8017, -0.4774]])\n",
      "tensor([[-0.7476],\n",
      "        [ 0.8873]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0174, -0.8509,  0.9030,  0.1474,  0.3277, -0.2032, -0.8069, -1.9375,\n",
       "         -0.7476],\n",
       "        [ 1.3923,  1.5079,  0.9380,  0.0154, -0.7514, -1.0922,  0.8017, -0.4774,\n",
       "          0.8873]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 5)\n",
    "z = torch.randn(2, 1)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим дополнительную ось:\n",
    "\n",
    "[torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2776, 0.5042, 0.2253],\n",
      "        [0.8309, 0.2603, 0.9985]])\n",
      "\n",
      "tensor([[[0.2776, 0.5042, 0.2253],\n",
      "         [0.8309, 0.2603, 0.9985]]]) torch.Size([1, 2, 3])\n",
      "\n",
      "tensor([[[0.2776, 0.5042, 0.2253]],\n",
      "\n",
      "        [[0.8309, 0.2603, 0.9985]]]) torch.Size([2, 1, 3])\n",
      "\n",
      "tensor([[[0.2776],\n",
      "         [0.5042],\n",
      "         [0.2253]],\n",
      "\n",
      "        [[0.8309],\n",
      "         [0.2603],\n",
      "         [0.9985]]]) torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.unsqueeze(0), x.unsqueeze(0).shape)\n",
    "print()\n",
    "print(x.unsqueeze(1), x.unsqueeze(1).shape)\n",
    "print()\n",
    "print(x.unsqueeze(2), x.unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберем лишние оси (где размер единичка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7272, 0.9223, 0.6136]],\n",
      "\n",
      "         [[0.5088, 0.2604, 0.9767]]]])\n",
      "\n",
      "tensor([[0.7272, 0.9223, 0.6136],\n",
      "        [0.5088, 0.2604, 0.9767]]) torch.Size([2, 3])\n",
      "\n",
      "tensor([[[0.7272, 0.9223, 0.6136]],\n",
      "\n",
      "        [[0.5088, 0.2604, 0.9767]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 2, 1, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.squeeze(), x.squeeze().shape)\n",
    "print()\n",
    "print(x.squeeze(0), x.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поговорим про типы данных в тензорах. По умолчанию в тензорах лежат числа в torch.float32 для вещественных и torch.int64 для целочисленных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.1992, 3.6992, 4.8984], dtype=torch.float16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float64)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int32)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размещение тензора на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 24 23:11:03 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.79                 Driver Version: 531.79       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080       WDDM | 00000000:01:00.0  On |                  N/A |\n",
      "| 57%   49C    P0              110W / 370W|   1870MiB / 10240MiB |     10%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      6124    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      6464    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      8632    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     10828    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13080    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13480    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14828    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     15064    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     15708    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     16280    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     16416    C+G   ...al\\Discord\\app-1.0.9013\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     16832    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     17204    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     18140    C+G   ...\\Local\\slack\\app-4.32.122\\slack.exe    N/A      |\n",
      "|    0   N/A  N/A     18252    C+G   ...AppData\\Roaming\\Spotify\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     19084    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     19652    C+G   ...e Stream\\75.0.2.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A     22204    C+G   ...61.0_x64__8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     22568    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     22660    C+G   ...03.0_x64__8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     23228    C+G   ...2.0_x64__cv1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     24008    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A     24244    C+G   ..._8wekyb3d8bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     24656    C+G   ...aming\\Telegram Desktop\\Telegram.exe    N/A      |\n",
      "|    0   N/A  N/A     29672    C+G   ...am Files\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     33416    C+G   G:\\Games\\Battle.net\\Battle.net.exe        N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], device=device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 22, 37, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "tensor = tensor.to(device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor.cpu()\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5474, 1.2432, 1.6899],\n",
       "        [0.6411, 1.1031, 1.5623]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m a \u001b[39m+\u001b[39;49m b\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "a = a.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7184, 0.5498, 0.5604],\n",
       "        [0.4750, 1.2123, 0.9842]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Операции с тензорами\n",
    "\n",
    "Большая часть операций с тензорами хорошо описана в их [документации](https://pytorch.org/docs/stable/torch.html), разберем основные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0542, 0.3823, 0.5019],\n",
       "         [0.7535, 0.0122, 0.9364]]),\n",
       " tensor([[0.9840, 0.0882, 0.7651],\n",
       "         [0.7870, 0.4985, 0.0727]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0383, 0.4705, 1.2670],\n",
      "        [1.5404, 0.5107, 1.0090]])\n",
      "\n",
      "tensor([[1.0383, 0.4705, 1.2670],\n",
      "        [1.5404, 0.5107, 1.0090]])\n",
      "\n",
      "tensor([[1.0383, 0.4705, 1.2670],\n",
      "        [1.5404, 0.5107, 1.0090]])\n"
     ]
    }
   ],
   "source": [
    "# поэлементные\n",
    "\n",
    "print(a + b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.add(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.add(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n",
      "\n",
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n",
      "\n",
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n"
     ]
    }
   ],
   "source": [
    "print(a - b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.sub(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.sub(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n",
      "\n",
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n",
      "\n",
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n"
     ]
    }
   ],
   "source": [
    "print(a * b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.mul(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.mul(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n",
      "\n",
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n",
      "\n",
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n"
     ]
    }
   ],
   "source": [
    "print(a / b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.div(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.div(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9952, 0.3948, 0.6624],\n",
       "         [0.8935, 0.0395, 0.9213]]),\n",
       " tensor([[0.2513, 0.2235, 0.2446, 0.0461],\n",
       "         [0.7737, 0.9193, 0.6260, 0.1214],\n",
       "         [0.0117, 0.7833, 0.6692, 0.1084]]),\n",
       " tensor([[0.0264, 0.9775, 0.4449, 0.3109, 0.7528],\n",
       "         [0.3279, 0.7904, 0.5625, 0.1013, 0.3399],\n",
       "         [0.4000, 0.6632, 0.5702, 0.9594, 0.9428],\n",
       "         [0.8072, 0.0614, 0.3238, 0.0036, 0.6157],\n",
       "         [0.4995, 0.8749, 0.2871, 0.5104, 0.5305]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 4)\n",
    "c = torch.rand(5, 5)\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5633, 1.1042, 0.9339, 0.1656],\n",
      "        [0.2658, 0.9576, 0.8598, 0.1459]]) torch.Size([2, 4])\n",
      "\n",
      "tensor([[0.5633, 1.1042, 0.9339, 0.1656],\n",
      "        [0.2658, 0.9576, 0.8598, 0.1459]]) torch.Size([2, 4])\n",
      "\n",
      "tensor(1.9212)\n",
      "\n",
      "tensor([[1.0268, 2.6579, 1.5603, 1.3647, 2.1229],\n",
      "        [1.3880, 2.2043, 1.7551, 1.1066, 1.4048],\n",
      "        [1.4919, 1.9409, 1.7685, 2.6100, 2.5671],\n",
      "        [2.2417, 1.0633, 1.3823, 1.0036, 1.8509],\n",
      "        [1.6479, 2.3987, 1.3325, 1.6659, 1.6998]])\n"
     ]
    }
   ],
   "source": [
    "# матричные операции\n",
    "\n",
    "print(a @ b, (a @ b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.matmul(a, b), torch.matmul(a, b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.trace())\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### [Автоматическое дифференцирование](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7076, 0.3914, 0.9870, 0.3090, 0.6262])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3531, 0.4118, 0.7373, 0.5783, 0.4483],\n",
       "        [0.3005, 0.9868, 0.6453, 0.5094, 0.3786],\n",
       "        [0.6972, 0.2405, 0.4653, 0.5081, 0.5772]], requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7320e-12,  3.0774e-41, -1.7366e-12])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_z = torch.empty(3)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5983, 1.6304, 1.5652], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    first_z[i] = torch.sum(w[i] * x)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5983, 1.6304, 1.5652], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w.t())\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1148, 0.7261, 0.6116], requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.rand(3, requires_grad=True)\n",
    "\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3244, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.sum(z * v)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3244199752807617"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean((y - 2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1052, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=None\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuke/.local/lib/python3.10/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=tensor([[0.0527, 0.0291, 0.0735, 0.0230, 0.0466],\n",
      "        [0.3334, 0.1844, 0.4650, 0.1456, 0.2950],\n",
      "        [0.2808, 0.1553, 0.3917, 0.1226, 0.2485]])\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=tensor([1.0370, 1.0579, 1.0155])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5492], requires_grad=True), tensor([0.4124], requires_grad=True))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1368], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([2.])\n",
      "\n",
      "b.grad=tensor([-2.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 1\n",
    "print(f'{b.grad=}\\n')  # -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0187], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b) ** 2\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.])\n",
      "\n",
      "b.grad=tensor([0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.2736])\n",
      "\n",
      "b.grad=tensor([-0.2736])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 2 * (a - b)\n",
    "print(f'{b.grad=}\\n')  # -2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2736], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3279, 0.5942, 0.6495, 0.8076, 0.6961],\n",
       "         [0.7243, 0.3865, 0.4297, 0.0069, 0.3211],\n",
       "         [0.9084, 0.0009, 0.5393, 0.4543, 0.1057]], requires_grad=True),\n",
       " tensor([[0.3158, 0.7038, 0.3901, 0.7456, 0.8604],\n",
       "         [0.4813, 0.8476, 0.9554, 0.9591, 0.9958],\n",
       "         [0.5856, 0.4295, 0.9476, 0.6895, 0.3585]], requires_grad=True))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3189, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean(a * b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
      "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
      "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]])\n",
      "\n",
      "b.grad=tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
      "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
      "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # b / (3 * 5)\n",
    "print(f'{b.grad=}\\n')  # a / (3 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
       "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
       "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
       "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
       "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[0.9249, 0.5178, 0.4018, 0.5241, 0.2307],\n",
      "        [0.4165, 0.5878, 0.6177, 0.4664, 0.3297],\n",
      "        [0.2164, 0.3102, 0.1307, 0.8902, 0.9583]], requires_grad=True)\n",
      "\n",
      "a.grad=None\n",
      "\n",
      "a.grad=tensor([[1.8499, 1.0356, 0.8036, 1.0483, 0.4613],\n",
      "        [0.8329, 1.1757, 1.2353, 0.9329, 0.6594],\n",
      "        [0.4329, 0.6204, 0.2613, 1.7805, 1.9166]])\n",
      "\n",
      "a.grad=tensor([[2.8499, 2.0356, 1.8036, 2.0483, 1.4613],\n",
      "        [1.8329, 2.1757, 2.2353, 1.9329, 1.6594],\n",
      "        [1.4329, 1.6204, 1.2613, 2.7805, 2.9166]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "print(f'{a=}\\n')\n",
    "\n",
    "loss1 = torch.sum(a ** 2) # 2a\n",
    "loss2 = torch.sum(a) # 1\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss1.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss2.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*a=tensor([[1.8499, 1.0356, 0.8036, 1.0483, 0.4613],\n",
      "        [0.8329, 1.1757, 1.2353, 0.9329, 0.6594],\n",
      "        [0.4329, 0.6204, 0.2613, 1.7805, 1.9166]], grad_fn=<MulBackward0>)\n",
      "\n",
      "2*a+1=tensor([[2.8499, 2.0356, 1.8036, 2.0483, 1.4613],\n",
      "        [1.8329, 2.1757, 2.2353, 1.9329, 1.6594],\n",
      "        [1.4329, 1.6204, 1.2613, 2.7805, 2.9166]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'{2*a=}\\n')\n",
    "print(f'{2*a+1=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9585, 0.8137, 0.7837, 0.3117, 0.5308],\n",
       "         [0.5991, 0.7310, 0.9645, 0.7374, 0.7492],\n",
       "         [0.5522, 0.8426, 0.0137, 0.8135, 0.0080]], requires_grad=True),\n",
       " tensor([[0.0779, 0.3624, 0.0938, 0.1770, 0.2233],\n",
       "         [0.3437, 0.3984, 0.9332, 0.7502, 0.0515],\n",
       "         [0.3722, 0.6910, 0.0804, 0.5425, 0.1549]]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=False)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1571, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # all ones\n",
    "print(f'{b.grad=}\\n')  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4873, 0.0094, 0.3410, 0.6388, 0.3698],\n",
       "         [0.3318, 0.3989, 0.4540, 0.8396, 0.7872],\n",
       "         [0.2356, 0.5495, 0.3216, 0.0462, 0.1471]], requires_grad=True),\n",
       " tensor([[0.9219, 0.8603, 0.0196, 0.7148, 0.3156],\n",
       "         [0.4160, 0.4039, 0.9161, 0.9858, 0.4589],\n",
       "         [0.9566, 0.1626, 0.5449, 0.9074, 0.7661]], requires_grad=True))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.3924)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4194, 0.7270, 0.6554, 0.8120, 0.2465],\n",
       "         [0.3338, 0.5301, 0.4804, 0.7732, 0.9213],\n",
       "         [0.9066, 0.9495, 0.0488, 0.6136, 0.7182]], requires_grad=True),\n",
       " tensor([[0.2233, 0.5705, 0.0119, 0.8199, 0.7503],\n",
       "         [0.4163, 0.5326, 0.9683, 0.9022, 0.6971],\n",
       "         [0.3597, 0.9322, 0.4627, 0.8851, 0.0571]], requires_grad=True))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5469)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(15.7352)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [108]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.7352, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(19.4477)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [113]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.4477)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(0.9730)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0091, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.0910)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0245)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Полносвязные слои и функции активации в `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Полносвязный слой\n",
    "\n",
    ">$y_j = \\sum\\limits_{i=1}^{n}x_iw_{ji} + b_j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=5, out_features=3, bias=True)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1036, -0.0857,  0.1891,  0.1899,  0.0061],\n",
       "        [-0.2206, -0.1663, -0.0230, -0.0566, -0.0549],\n",
       "        [-0.1880, -0.2969, -0.1313,  0.2695,  0.4457]], requires_grad=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1110, -0.0980, -0.0736], requires_grad=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3352,  0.3743,  0.8121], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Сигмоида $f(x) = \\dfrac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7887, -2.3923,  0.1006, -1.6202,  1.0863])\n",
      "tensor([0.1432, 0.0838, 0.5251, 0.1652, 0.7477])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ReLU $f(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6977,  0.5788, -1.0462, -0.7537,  0.8037])\n",
      "tensor([0.0000, 0.5788, 0.0000, 0.0000, 0.8037])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Leaky ReLU $f(x) = \\max(0, x) + \\alpha \\min(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.LeakyReLU(negative_slope=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4079, -0.8732,  0.4297,  0.5221,  0.3086])\n",
      "tensor([ 1.4079e+00, -8.7321e-04,  4.2974e-01,  5.2209e-01,  3.0863e-01])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "b_true = torch.randn(1)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects) + b_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "step_size = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 23.65750\n",
      "MSE на шаге 2 15.53605\n",
      "MSE на шаге 3 12.11575\n",
      "MSE на шаге 4 10.36995\n",
      "MSE на шаге 5 9.30175\n",
      "MSE на шаге 6 8.55659\n",
      "MSE на шаге 7 7.99169\n",
      "MSE на шаге 8 7.53972\n",
      "MSE на шаге 9 7.16377\n",
      "MSE на шаге 10 6.84131\n",
      "MSE на шаге 11 6.55765\n",
      "MSE на шаге 12 6.30292\n",
      "MSE на шаге 13 6.07033\n",
      "MSE на шаге 14 5.85515\n",
      "MSE на шаге 15 5.65409\n",
      "MSE на шаге 16 5.46479\n",
      "MSE на шаге 17 5.28555\n",
      "MSE на шаге 18 5.11514\n",
      "MSE на шаге 19 4.95265\n",
      "MSE на шаге 20 4.79735\n",
      "MSE на шаге 21 4.64871\n",
      "MSE на шаге 31 3.45555\n",
      "MSE на шаге 41 2.65631\n",
      "MSE на шаге 51 2.11964\n",
      "MSE на шаге 61 1.75925\n",
      "MSE на шаге 71 1.51724\n",
      "MSE на шаге 81 1.35471\n",
      "MSE на шаге 91 1.24558\n",
      "MSE на шаге 101 1.17229\n",
      "MSE на шаге 111 1.12307\n",
      "MSE на шаге 121 1.09002\n",
      "MSE на шаге 131 1.06782\n",
      "MSE на шаге 141 1.05292\n",
      "MSE на шаге 151 1.04291\n",
      "MSE на шаге 161 1.03619\n",
      "MSE на шаге 171 1.03168\n",
      "MSE на шаге 181 1.02864\n",
      "MSE на шаге 191 1.02661\n"
     ]
    }
   ],
   "source": [
    "w = torch.rand(n_features, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = torch.matmul(x, w) + b\n",
    "    \n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * step_size\n",
    "        b -= b.grad * step_size\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 67.76115\n",
      "MSE на шаге 2 42.90440\n",
      "MSE на шаге 3 34.78294\n",
      "MSE на шаге 4 31.90576\n",
      "MSE на шаге 5 30.72448\n",
      "MSE на шаге 6 30.12742\n",
      "MSE на шаге 7 29.75548\n",
      "MSE на шаге 8 29.48539\n",
      "MSE на шаге 9 29.27014\n",
      "MSE на шаге 10 29.08890\n",
      "MSE на шаге 11 28.93083\n",
      "MSE на шаге 12 28.78951\n",
      "MSE на шаге 13 28.66082\n",
      "MSE на шаге 14 28.54199\n",
      "MSE на шаге 15 28.43109\n",
      "MSE на шаге 16 28.32677\n",
      "MSE на шаге 17 28.22807\n",
      "MSE на шаге 18 28.13428\n",
      "MSE на шаге 19 28.04487\n",
      "MSE на шаге 20 27.95945\n",
      "MSE на шаге 21 27.87770\n",
      "MSE на шаге 31 27.22173\n",
      "MSE на шаге 41 26.78238\n",
      "MSE на шаге 51 26.48738\n",
      "MSE на шаге 61 26.28927\n",
      "MSE на шаге 71 26.15623\n",
      "MSE на шаге 81 26.06689\n",
      "MSE на шаге 91 26.00690\n",
      "MSE на шаге 101 25.96661\n",
      "MSE на шаге 111 25.93955\n",
      "MSE на шаге 121 25.92138\n",
      "MSE на шаге 131 25.90918\n",
      "MSE на шаге 141 25.90099\n",
      "MSE на шаге 151 25.89549\n",
      "MSE на шаге 161 25.89179\n",
      "MSE на шаге 171 25.88931\n",
      "MSE на шаге 181 25.88764\n",
      "MSE на шаге 191 25.88653\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x)\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "#     layer.weight.grad.zero_()\n",
    "#     layer.bias.grad.zero_()\n",
    "    \n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7298],\n",
       "        [-2.7292],\n",
       "        [-2.7318],\n",
       "        [-2.7311],\n",
       "        [-2.7278],\n",
       "        [-2.7308],\n",
       "        [-2.7292],\n",
       "        [-2.7330],\n",
       "        [-2.7296],\n",
       "        [-2.7168],\n",
       "        [-2.7313],\n",
       "        [-2.7349],\n",
       "        [-2.7305],\n",
       "        [-2.7286],\n",
       "        [-2.7363],\n",
       "        [-2.7269],\n",
       "        [-2.7251],\n",
       "        [-2.7222],\n",
       "        [-2.7308],\n",
       "        [-2.7260],\n",
       "        [-2.7310],\n",
       "        [-2.7279],\n",
       "        [-2.7256],\n",
       "        [-2.7202],\n",
       "        [-2.7229],\n",
       "        [-2.7306],\n",
       "        [-2.7265],\n",
       "        [-2.7337],\n",
       "        [-2.7233],\n",
       "        [-2.7287],\n",
       "        [-2.7279],\n",
       "        [-2.7318],\n",
       "        [-2.7289],\n",
       "        [-2.7257],\n",
       "        [-2.7223],\n",
       "        [-2.7243],\n",
       "        [-2.7390],\n",
       "        [-2.7176],\n",
       "        [-2.7257],\n",
       "        [-2.7269],\n",
       "        [-2.7223],\n",
       "        [-2.7276],\n",
       "        [-2.7244],\n",
       "        [-2.7321],\n",
       "        [-2.7252],\n",
       "        [-2.7309],\n",
       "        [-2.7242],\n",
       "        [-2.7299],\n",
       "        [-2.7348],\n",
       "        [-2.7298],\n",
       "        [-2.7290],\n",
       "        [-2.7337],\n",
       "        [-2.7367],\n",
       "        [-2.7189],\n",
       "        [-2.7300],\n",
       "        [-2.7280],\n",
       "        [-2.7253],\n",
       "        [-2.7337],\n",
       "        [-2.7322],\n",
       "        [-2.7274],\n",
       "        [-2.7266],\n",
       "        [-2.7305],\n",
       "        [-2.7325],\n",
       "        [-2.7221],\n",
       "        [-2.7273],\n",
       "        [-2.7294],\n",
       "        [-2.7239],\n",
       "        [-2.7206],\n",
       "        [-2.7323],\n",
       "        [-2.7229],\n",
       "        [-2.7279],\n",
       "        [-2.7308],\n",
       "        [-2.7320],\n",
       "        [-2.7193],\n",
       "        [-2.7196],\n",
       "        [-2.7275],\n",
       "        [-2.7284],\n",
       "        [-2.7304],\n",
       "        [-2.7291],\n",
       "        [-2.7330],\n",
       "        [-2.7342],\n",
       "        [-2.7195],\n",
       "        [-2.7230],\n",
       "        [-2.7376],\n",
       "        [-2.7266],\n",
       "        [-2.7182],\n",
       "        [-2.7261],\n",
       "        [-2.7240],\n",
       "        [-2.7260],\n",
       "        [-2.7287],\n",
       "        [-2.7365],\n",
       "        [-2.7253],\n",
       "        [-2.7333],\n",
       "        [-2.7305],\n",
       "        [-2.7338],\n",
       "        [-2.7322],\n",
       "        [-2.7306],\n",
       "        [-2.7302],\n",
       "        [-2.7355],\n",
       "        [-2.7345],\n",
       "        [-2.7258],\n",
       "        [-2.7270],\n",
       "        [-2.7259],\n",
       "        [-2.7286],\n",
       "        [-2.7312],\n",
       "        [-2.7181],\n",
       "        [-2.7331],\n",
       "        [-2.7254],\n",
       "        [-2.7366],\n",
       "        [-2.7318],\n",
       "        [-2.7297],\n",
       "        [-2.7317],\n",
       "        [-2.7312],\n",
       "        [-2.7204],\n",
       "        [-2.7212],\n",
       "        [-2.7324],\n",
       "        [-2.7290],\n",
       "        [-2.7303],\n",
       "        [-2.7195],\n",
       "        [-2.7187],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7204],\n",
       "        [-2.7305],\n",
       "        [-2.7373],\n",
       "        [-2.7283],\n",
       "        [-2.7349],\n",
       "        [-2.7276],\n",
       "        [-2.7236],\n",
       "        [-2.7314],\n",
       "        [-2.7231],\n",
       "        [-2.7292],\n",
       "        [-2.7359],\n",
       "        [-2.7278],\n",
       "        [-2.7184],\n",
       "        [-2.7301],\n",
       "        [-2.7245],\n",
       "        [-2.7376],\n",
       "        [-2.7306],\n",
       "        [-2.7318],\n",
       "        [-2.7294],\n",
       "        [-2.7340],\n",
       "        [-2.7255],\n",
       "        [-2.7266],\n",
       "        [-2.7193],\n",
       "        [-2.7233],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7229],\n",
       "        [-2.7287],\n",
       "        [-2.7244],\n",
       "        [-2.7332],\n",
       "        [-2.7236],\n",
       "        [-2.7283],\n",
       "        [-2.7318],\n",
       "        [-2.7252],\n",
       "        [-2.7279],\n",
       "        [-2.7341],\n",
       "        [-2.7271],\n",
       "        [-2.7242],\n",
       "        [-2.7222],\n",
       "        [-2.7299],\n",
       "        [-2.7236],\n",
       "        [-2.7349],\n",
       "        [-2.7271],\n",
       "        [-2.7251],\n",
       "        [-2.7279],\n",
       "        [-2.7245],\n",
       "        [-2.7366],\n",
       "        [-2.7269],\n",
       "        [-2.7339],\n",
       "        [-2.7342],\n",
       "        [-2.7311],\n",
       "        [-2.7286],\n",
       "        [-2.7327],\n",
       "        [-2.7341],\n",
       "        [-2.7282],\n",
       "        [-2.7376],\n",
       "        [-2.7284],\n",
       "        [-2.7178],\n",
       "        [-2.7322],\n",
       "        [-2.7360],\n",
       "        [-2.7312],\n",
       "        [-2.7323],\n",
       "        [-2.7257],\n",
       "        [-2.7263],\n",
       "        [-2.7201],\n",
       "        [-2.7289],\n",
       "        [-2.7302],\n",
       "        [-2.7270],\n",
       "        [-2.7328],\n",
       "        [-2.7259],\n",
       "        [-2.7294],\n",
       "        [-2.7368],\n",
       "        [-2.7221],\n",
       "        [-2.7358],\n",
       "        [-2.7235],\n",
       "        [-2.7357],\n",
       "        [-2.7213],\n",
       "        [-2.7244],\n",
       "        [-2.7333],\n",
       "        [-2.7313],\n",
       "        [-2.7218],\n",
       "        [-2.7315],\n",
       "        [-2.7290],\n",
       "        [-2.7316],\n",
       "        [-2.7246],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7257],\n",
       "        [-2.7328],\n",
       "        [-2.7349],\n",
       "        [-2.7220],\n",
       "        [-2.7311],\n",
       "        [-2.7227],\n",
       "        [-2.7199],\n",
       "        [-2.7304],\n",
       "        [-2.7326],\n",
       "        [-2.7388],\n",
       "        [-2.7322],\n",
       "        [-2.7233],\n",
       "        [-2.7243],\n",
       "        [-2.7230],\n",
       "        [-2.7367],\n",
       "        [-2.7298],\n",
       "        [-2.7324],\n",
       "        [-2.7333],\n",
       "        [-2.7226],\n",
       "        [-2.7281],\n",
       "        [-2.7237],\n",
       "        [-2.7322],\n",
       "        [-2.7293],\n",
       "        [-2.7231],\n",
       "        [-2.7207],\n",
       "        [-2.7364],\n",
       "        [-2.7344],\n",
       "        [-2.7308],\n",
       "        [-2.7364],\n",
       "        [-2.7323],\n",
       "        [-2.7260],\n",
       "        [-2.7262],\n",
       "        [-2.7301],\n",
       "        [-2.7209],\n",
       "        [-2.7212],\n",
       "        [-2.7315],\n",
       "        [-2.7257],\n",
       "        [-2.7381],\n",
       "        [-2.7270],\n",
       "        [-2.7191],\n",
       "        [-2.7275],\n",
       "        [-2.7317],\n",
       "        [-2.7330],\n",
       "        [-2.7282],\n",
       "        [-2.7277],\n",
       "        [-2.7319],\n",
       "        [-2.7232],\n",
       "        [-2.7245],\n",
       "        [-2.7219],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7256],\n",
       "        [-2.7252],\n",
       "        [-2.7296],\n",
       "        [-2.7335],\n",
       "        [-2.7267],\n",
       "        [-2.7314],\n",
       "        [-2.7328],\n",
       "        [-2.7273],\n",
       "        [-2.7308],\n",
       "        [-2.7331],\n",
       "        [-2.7353],\n",
       "        [-2.7304],\n",
       "        [-2.7176],\n",
       "        [-2.7261],\n",
       "        [-2.7284],\n",
       "        [-2.7234],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7371],\n",
       "        [-2.7287],\n",
       "        [-2.7245],\n",
       "        [-2.7244],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7260],\n",
       "        [-2.7295],\n",
       "        [-2.7261],\n",
       "        [-2.7358],\n",
       "        [-2.7261],\n",
       "        [-2.7282],\n",
       "        [-2.7257],\n",
       "        [-2.7248],\n",
       "        [-2.7390],\n",
       "        [-2.7267],\n",
       "        [-2.7387],\n",
       "        [-2.7252],\n",
       "        [-2.7323],\n",
       "        [-2.7375],\n",
       "        [-2.7289],\n",
       "        [-2.7218]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 300])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x) - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x).ravel() - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 82.40031\n",
      "MSE на шаге 2 34.83059\n",
      "MSE на шаге 3 18.67639\n",
      "MSE на шаге 4 12.57786\n",
      "MSE на шаге 5 9.87140\n",
      "MSE на шаге 6 8.42349\n",
      "MSE на шаге 7 7.51441\n",
      "MSE на шаге 8 6.87739\n",
      "MSE на шаге 9 6.39865\n",
      "MSE на шаге 10 6.02125\n",
      "MSE на шаге 11 5.71254\n",
      "MSE на шаге 12 5.45201\n",
      "MSE на шаге 13 5.22616\n",
      "MSE на шаге 14 5.02584\n",
      "MSE на шаге 15 4.84476\n",
      "MSE на шаге 16 4.67857\n",
      "MSE на шаге 17 4.52421\n",
      "MSE на шаге 18 4.37955\n",
      "MSE на шаге 19 4.24304\n",
      "MSE на шаге 20 4.11358\n",
      "MSE на шаге 21 3.99035\n",
      "MSE на шаге 31 3.01138\n",
      "MSE на шаге 41 2.35799\n",
      "MSE на шаге 51 1.91931\n",
      "MSE на шаге 61 1.62472\n",
      "MSE на шаге 71 1.42689\n",
      "MSE на шаге 81 1.29405\n",
      "MSE на шаге 91 1.20484\n",
      "MSE на шаге 101 1.14493\n",
      "MSE на шаге 111 1.10470\n",
      "MSE на шаге 121 1.07768\n",
      "MSE на шаге 131 1.05954\n",
      "MSE на шаге 141 1.04736\n",
      "MSE на шаге 151 1.03917\n",
      "MSE на шаге 161 1.03368\n",
      "MSE на шаге 171 1.02999\n",
      "MSE на шаге 181 1.02751\n",
      "MSE на шаге 191 1.02585\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 2052.12988\n",
      "MSE на шаге 2 606.21967\n",
      "MSE на шаге 3 207.55234\n",
      "MSE на шаге 4 82.47279\n",
      "MSE на шаге 5 40.03379\n",
      "MSE на шаге 6 24.82249\n",
      "MSE на шаге 7 19.02928\n",
      "MSE на шаге 8 16.58483\n",
      "MSE на шаге 9 15.35648\n",
      "MSE на шаге 10 14.58259\n",
      "MSE на шаге 11 13.98817\n",
      "MSE на шаге 12 13.47272\n",
      "MSE на шаге 13 12.99872\n",
      "MSE на шаге 14 12.55160\n",
      "MSE на шаге 15 12.12528\n",
      "MSE на шаге 16 11.71682\n",
      "MSE на шаге 17 11.32460\n",
      "MSE на шаге 18 10.94749\n",
      "MSE на шаге 19 10.58467\n",
      "MSE на шаге 20 10.23542\n",
      "MSE на шаге 51 3.92278\n",
      "MSE на шаге 101 1.43192\n",
      "MSE на шаге 151 1.02777\n",
      "MSE на шаге 201 0.95445\n",
      "MSE на шаге 251 0.93498\n",
      "MSE на шаге 301 0.92541\n",
      "MSE на шаге 351 0.91856\n",
      "MSE на шаге 401 0.91310\n",
      "MSE на шаге 451 0.90864\n",
      "MSE на шаге 501 0.90498\n",
      "MSE на шаге 551 0.90198\n",
      "MSE на шаге 601 0.89951\n",
      "MSE на шаге 651 0.89748\n",
      "MSE на шаге 701 0.89582\n",
      "MSE на шаге 751 0.89445\n",
      "MSE на шаге 801 0.89333\n",
      "MSE на шаге 851 0.89241\n",
      "MSE на шаге 901 0.89165\n",
      "MSE на шаге 951 0.89103\n",
      "MSE на шаге 1001 0.89052\n",
      "MSE на шаге 1051 0.89010\n",
      "MSE на шаге 1101 0.88976\n",
      "MSE на шаге 1151 0.88948\n",
      "MSE на шаге 1201 0.88925\n",
      "MSE на шаге 1251 0.88906\n",
      "MSE на шаге 1301 0.88890\n",
      "MSE на шаге 1351 0.88877\n",
      "MSE на шаге 1401 0.88867\n",
      "MSE на шаге 1451 0.88858\n",
      "MSE на шаге 1501 0.88851\n",
      "MSE на шаге 1551 0.88845\n",
      "MSE на шаге 1601 0.88840\n",
      "MSE на шаге 1651 0.88836\n",
      "MSE на шаге 1701 0.88833\n",
      "MSE на шаге 1751 0.88830\n",
      "MSE на шаге 1801 0.88828\n",
      "MSE на шаге 1851 0.88826\n",
      "MSE на шаге 1901 0.88825\n",
      "MSE на шаге 1951 0.88824\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 1942.53650\n",
      "MSE на шаге 2 1676.33215\n",
      "MSE на шаге 3 1462.73621\n",
      "MSE на шаге 4 1252.29272\n",
      "MSE на шаге 5 1084.68542\n",
      "MSE на шаге 6 969.74554\n",
      "MSE на шаге 7 892.42969\n",
      "MSE на шаге 8 837.99945\n",
      "MSE на шаге 9 791.54950\n",
      "MSE на шаге 10 736.85345\n",
      "MSE на шаге 11 660.66931\n",
      "MSE на шаге 12 552.94324\n",
      "MSE на шаге 13 420.34772\n",
      "MSE на шаге 14 295.80695\n",
      "MSE на шаге 15 213.59764\n",
      "MSE на шаге 16 170.35097\n",
      "MSE на шаге 17 140.10820\n",
      "MSE на шаге 18 116.49634\n",
      "MSE на шаге 19 95.18255\n",
      "MSE на шаге 20 74.59748\n",
      "MSE на шаге 51 9.93147\n",
      "MSE на шаге 101 3.76550\n",
      "MSE на шаге 151 1.81388\n",
      "MSE на шаге 201 1.21714\n",
      "MSE на шаге 251 1.03567\n",
      "MSE на шаге 301 0.97787\n",
      "MSE на шаге 351 0.95785\n",
      "MSE на шаге 401 0.94890\n",
      "MSE на шаге 451 0.94261\n",
      "MSE на шаге 501 0.93729\n",
      "MSE на шаге 551 0.93249\n",
      "MSE на шаге 601 0.92810\n",
      "MSE на шаге 651 0.92405\n",
      "MSE на шаге 701 0.92031\n",
      "MSE на шаге 751 0.91686\n",
      "MSE на шаге 801 0.91367\n",
      "MSE на шаге 851 0.91071\n",
      "MSE на шаге 901 0.90798\n",
      "MSE на шаге 951 0.90545\n",
      "MSE на шаге 1001 0.90310\n",
      "MSE на шаге 1051 0.90093\n",
      "MSE на шаге 1101 0.89892\n",
      "MSE на шаге 1151 0.89705\n",
      "MSE на шаге 1201 0.89531\n",
      "MSE на шаге 1251 0.89371\n",
      "MSE на шаге 1301 0.89224\n",
      "MSE на шаге 1351 0.89088\n",
      "MSE на шаге 1401 0.88962\n",
      "MSE на шаге 1451 0.88845\n",
      "MSE на шаге 1501 0.88737\n",
      "MSE на шаге 1551 0.88636\n",
      "MSE на шаге 1601 0.88543\n",
      "MSE на шаге 1651 0.88456\n",
      "MSE на шаге 1701 0.88375\n",
      "MSE на шаге 1751 0.88300\n",
      "MSE на шаге 1801 0.88231\n",
      "MSE на шаге 1851 0.88166\n",
      "MSE на шаге 1901 0.88105\n",
      "MSE на шаге 1951 0.88049\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=n_features, out_features=3)\n",
    "layer2 = nn.Linear(in_features=3, out_features=1)\n",
    "activation = nn.ReLU()\n",
    "\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer2(activation(layer1(x))).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer1.weight -= layer1.weight.grad * step_size\n",
    "        layer1.bias -= layer1.bias.grad * step_size\n",
    "        layer2.weight -= layer2.weight.grad * step_size\n",
    "        layer2.bias -= layer2.bias.grad * step_size\n",
    "\n",
    "    layer1.zero_grad()\n",
    "    layer2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Seminar 1. Intro to DL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
