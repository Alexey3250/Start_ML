{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "f7rx07cd3zx4uteseq3t8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA GeForce RTX 3090 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "fvt8vkcdxisc3b2kj761r5",
    "execution_id": "531714d6-5615-4c4c-9ecb-5dc427507f03",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Модель\n",
    "\n",
    "https://huggingface.co/Grossmend/rudialogpt3_medium_based_on_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "3shv86od7ntbmd0690u0t",
    "id": "fn9KxEnfaxwo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "\n",
    "checkpoint = \"Grossmend/rudialogpt3_medium_based_on_gpt2\"   \n",
    "tokenizer =  GPT2TokenizerFast.from_pretrained(checkpoint)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
    "\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "zygm20ty51yug0m9heps",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_length_param(text: str, tokenizer) -> str:\n",
    "    tokens_count = len(tokenizer.encode(text))\n",
    "\n",
    "    if tokens_count <= 15:\n",
    "        len_param = '1'\n",
    "    elif tokens_count <= 50:\n",
    "        len_param = '2'\n",
    "    elif tokens_count <= 256:\n",
    "        len_param = '3'\n",
    "    else:\n",
    "        len_param = '-'\n",
    "    return len_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7f3qrfl8kyy291nam2lje2",
    "execution_id": "c2bbfd33-6c44-4199-95e4-3336a371c02e",
    "id": "psXZnJk0Eo3J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Общение с моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "h4lwwjqlh0bdwin38qaam",
    "id": "MGdCxVnOhK_K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def chat():\n",
    "    chat_history_ids = torch.zeros((1, 0), dtype=torch.int).to(device)\n",
    "\n",
    "    human = True\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if human:\n",
    "                input_user = input(\"===> Человек: \")\n",
    "                new_user_input_ids = tokenizer.encode(\n",
    "                    f\"|0|{get_length_param(input_user, tokenizer)}|\" + input_user + tokenizer.eos_token,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "                chat_history_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "            else:\n",
    "                new_user_input_ids = tokenizer.encode(f\"|1|{np.random.choice(['2', '3'])}|\", return_tensors=\"pt\").to(device)\n",
    "                chat_history_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "\n",
    "                input_len = chat_history_ids.shape[-1]\n",
    "\n",
    "                chat_history_ids = model.generate(\n",
    "                    chat_history_ids,\n",
    "                    num_return_sequences=1,\n",
    "                    max_length=1024,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.9,\n",
    "                    temperature = 0.5,\n",
    "                    mask_token_id=tokenizer.mask_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    unk_token_id=tokenizer.unk_token_id,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "                print(f\"===> GPT-3:  {tokenizer.decode(chat_history_ids[:, input_len:][0], skip_special_tokens=True)}\")\n",
    "\n",
    "                if chat_history_ids.shape[-1] > 600:\n",
    "                    return\n",
    "\n",
    "            human = not human\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "hl3co3znt8ba0x6x4c1g3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Человек: Расскажи мне анекдот про жабу, пожалуйста.\n",
      "===> GPT-3:  Жаба на берегу реки, а рядом мужик с удочкой. Жаба: -Мужик, дай удочку, я тебе червячка поймаю. Мужик: Жабы, это не для вас, мы рыбаки. И тут из-за кустов выходит мужик и говорит: \"Я тебе на червячке поймаюсь\".\n",
      "===> Человек: Смешной, спасибо!\n",
      "===> GPT-3:  Да не за что. Мне тоже нравится. А еще я знаю, что это анекдот. Но не могу вспомнить, где я его слышал. :)\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "vvvz3g6p9qj2liz0wq0nzu",
    "id": "mjEQiv5TMjZW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Человек: Расскажи анекдот про часы и мужика.\n",
      "===> GPT-3:  Мужик в магазине часов, а продавец в это время его спрашивает: - А что вы хотите за часы? - Часы, часы, Карл!\n",
      "===> Человек: Не смешной, давай другой анекдот\n",
      "===> GPT-3:  В магазине одежды мужик спрашивает у продавца: \"А что у вас есть?\" - часы. - Ну часы есть, но есть и часы! - Но часы-то есть! А есть ли у Вас часы?\"\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "b0unxe32kfwfqselwdvrf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Человек: Расскажи о себе, откуда ты?\n",
      "===> GPT-3:  Вроде бы из Беларуси, но не уверен. Может из России. Но это не точно.\n",
      "===> Человек: А я из Москвы.\n",
      "===> GPT-3:  Ну, я тоже из Мск, а в чем суть твоего поста? Я вот тоже хочу в Москву, что бы в метро не стоять и на работу не ездить. Вот у меня есть работа, дом, машина, квартира, и я не хочу никуда переезжать. Я не понимаю в чём проблема? Почему ты не можешь просто переехать в другую страну? Или может быть ты из Казахстана? Может быть из Узбекистана? Не знаю, просто хочу переехать. А если ты про Москву и Россию, то это ты зря, тут не все так однозначно. У меня вот, например, есть знакомый, который живет в Москве, у него есть квартира в МСК и машина в Питере, он работает в России, живет один и все у них хорошо. Просто он не хочет никуда уезжать, вот и всё. Что я делаю не так? И почему я должен переплачивать за это? А может я просто не могу? Это же не моя проблема. Ну и конечно же, мне не нравится то, как живут в\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "icx2jrk6tct7atd2hbtbqs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Человек: Как ты думаешь, зачем мы на планете Земля?\n",
      "===> GPT-3:  Ну, я думаю, что это не совсем то, о чём я думал. Как бы, мы - это всё человечество. А все люди - братья. И мы все - люди. Но мы не все, а лишь малая часть. Мы не люди, но мы люди и мы братья, и нам не надо быть братьями, чтобы быть людьми.\n",
      "===> Человек: Очень глубоко, я ничего не понял.\n",
      "===> GPT-3:  А что тут понимать? Всё просто: мы часть всего человечества. Это не так просто, как кажется. Люди - не часть человечества, это просто люди\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "vh4yp4v5kwp5td9aopgpxi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Человек: Кто такой Александр Сергеевич Пушкин?\n",
      "===> GPT-3:  Это тот, кто написал \"Сказку о попе и работнике его Балде\"\n",
      "===> Человек: А что ты еще о нем знаешь?\n",
      "===> GPT-3:  Ну, он же написал сказку \"О попе, работящемся на галерах\". А еще он был членом кружка \"Арифметика\" и читал курс \"Буржуазная этика\". И еще, у него были часы, которые показывали время по солнцу.\n",
      "===> Человек: А когда он умер?\n",
      "===> GPT-3:  Не знаю, как сейчас, но в 1835 году. Так как он написал много стихов, то его смерть была не очень приятной. Но, если говорить о его жизни, его жизнь была очень интересной, и он очень много путешествовал. Я думаю, что он жил бы очень долго, не будь его так много в России. Он был очень популярен. И, думаю что его любили бы. В общем, я думаю он прожил бы не мало лет. А, может быть, даже и больше. Не знаю. По крайней мере, мне так кажется. Это все, о чем я могу сказать. Хотя, это не точно. )\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "8av84m3q62qmnrjsvv0qoe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Человек: Как ты думаешь, пойдет ли завтра дождь?\n",
      "===> GPT-3:  Я думаю, что нет. Дождь идет, но не так часто, как хотелось бы. В основном идет дождь из-за того,что в городе очень много людей,которые не могут выйти на улицу. Но в целом, да, погода сегодня хорошая.\n",
      "===> Человек: Какое сегодня число?\n",
      "===> GPT-3:  Сегодня в Москве было +3, а в Питере +2. Сейчас дождь, и, если верить прогнозу, будет идти до конца недели. Надеюсь, завтра будет получше. :-)\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "77w3nai7crq1ec1z9f3f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Человек: Расскажи мне анекдот про Штирлица, пожалуйста.\n",
      "===> GPT-3:  Штирлиц приходит к Мюллеру и спрашивает: - Мюллера, а что такое \"Штрилиц\"? - Штрилица - это когда ты приходишь в бар и заказываешь два пива, но не заказывает два стула. - А что это такое - \"штрилице\"?\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uPZXtklAd0Cd",
    "ESogNuUOEmj_",
    "psXZnJk0Eo3J"
   ],
   "include_colab_link": true,
   "name": "Fine tune RuDialoGPT3 on telegram chat",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "startml",
   "language": "python",
   "name": "startml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "notebookId": "7f6f0fec-eacf-4685-9d41-7919d5e4ac34",
  "notebookPath": "Lecture_10_Intro_to_DL_gpt.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
