{"cells":[{"cell_type":"markdown","metadata":{"cellId":"9zxlngtrja7lv0wvs9xxij","execution_id":"e5781223-e8b3-4631-b6dd-f5fe7074afbc","pycharm":{"name":"#%% md\n"}},"source":["# Трансформер 2 - Улица Сезам.\n","\n","> Обсудим популярные архитектуры нейронных сетей, основанные на архитектуре Трансформер."]},{"cell_type":"markdown","metadata":{"cellId":"gd6wgzblwy68j35s98yy2x","execution_id":"f68dedd5-0c41-4e09-b2f5-cc55abbd3bcc"},"source":["## Hugging Face\n","\n","https://huggingface.co/"]},{"cell_type":"markdown","metadata":{"cellId":"h9oe5wpbsrtnhb2xrjywfl","execution_id":"08a81dc6-de2d-4407-9d5a-9ba9f735ae96"},"source":["## Модели\n","\n","https://huggingface.co/models"]},{"cell_type":"code","execution_count":2,"metadata":{"cellId":"sy01230za7aduvs9p1bsw7","execution":{"iopub.execute_input":"2023-07-13T16:07:09.299228Z","iopub.status.busy":"2023-07-13T16:07:09.298912Z","iopub.status.idle":"2023-07-13T16:07:13.525187Z","shell.execute_reply":"2023-07-13T16:07:13.524147Z","shell.execute_reply.started":"2023-07-13T16:07:09.299202Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","from warnings import filterwarnings\n","\n","filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"cellId":"r4i12xla7cbda1xcsp126f","execution":{"iopub.execute_input":"2023-07-13T16:07:13.529003Z","iopub.status.busy":"2023-07-13T16:07:13.528018Z","iopub.status.idle":"2023-07-13T16:07:25.153999Z","shell.execute_reply":"2023-07-13T16:07:25.152997Z","shell.execute_reply.started":"2023-07-13T16:07:13.528967Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","from transformers import BertModel  # https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n","from transformers import RobertaModel  # https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel\n","from transformers import DistilBertModel  # https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel\n","\n","\n","def get_model(model_name):\n","    assert model_name in ['bert', 'roberta', 'distilbert']\n","    \n","    checkpoint_names = {\n","        'bert': 'bert-base-cased',  # https://huggingface.co/bert-base-cased\n","        'roberta': 'roberta-base',  # https://huggingface.co/roberta-base\n","        'distilbert': 'distilbert-base-cased'  # https://huggingface.co/distilbert-base-cased\n","    }\n","    \n","    model_classes = {\n","        'bert': BertModel,\n","        'roberta': RobertaModel,\n","        'distilbert': DistilBertModel\n","    }\n","    \n","    return AutoTokenizer.from_pretrained(checkpoint_names[model_name]), model_classes[model_name].from_pretrained(checkpoint_names[model_name])"]},{"cell_type":"code","execution_count":4,"metadata":{"cellId":"q1awrdd62fjhcbt1x6gpme","execution":{"iopub.execute_input":"2023-07-13T16:07:25.156154Z","iopub.status.busy":"2023-07-13T16:07:25.155733Z","iopub.status.idle":"2023-07-13T16:07:30.544858Z","shell.execute_reply":"2023-07-13T16:07:30.543513Z","shell.execute_reply.started":"2023-07-13T16:07:25.156115Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer, model = get_model('bert')"]},{"cell_type":"code","execution_count":5,"metadata":{"cellId":"pd3t6jx4lkohuvopvez2","execution":{"iopub.execute_input":"2023-07-13T16:07:30.549373Z","iopub.status.busy":"2023-07-13T16:07:30.549038Z","iopub.status.idle":"2023-07-13T16:07:30.556712Z","shell.execute_reply":"2023-07-13T16:07:30.555714Z","shell.execute_reply.started":"2023-07-13T16:07:30.549344Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"cellId":"3fqqa081dq245vw8566o5g","execution":{"iopub.execute_input":"2023-07-13T16:07:30.558900Z","iopub.status.busy":"2023-07-13T16:07:30.558237Z","iopub.status.idle":"2023-07-13T16:07:30.629198Z","shell.execute_reply":"2023-07-13T16:07:30.628146Z","shell.execute_reply.started":"2023-07-13T16:07:30.558863Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[ 101, 8667,  106,  102]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["encoding = tokenizer.encode_plus('Hello!', add_special_tokens=True, return_token_type_ids=False, return_tensors='pt')\n","\n","encoding"]},{"cell_type":"code","execution_count":7,"metadata":{"cellId":"rcje2ws0ygidljov9narg","execution":{"iopub.execute_input":"2023-07-13T16:07:30.631391Z","iopub.status.busy":"2023-07-13T16:07:30.630601Z","iopub.status.idle":"2023-07-13T16:07:30.638587Z","shell.execute_reply":"2023-07-13T16:07:30.637539Z","shell.execute_reply.started":"2023-07-13T16:07:30.631358Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'[CLS] Hello! [SEP]'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(encoding['input_ids'][0])"]},{"cell_type":"code","execution_count":8,"metadata":{"cellId":"i1dygle24yqph9wulta5gt","execution":{"iopub.execute_input":"2023-07-13T16:07:30.640937Z","iopub.status.busy":"2023-07-13T16:07:30.640042Z","iopub.status.idle":"2023-07-13T16:07:30.899761Z","shell.execute_reply":"2023-07-13T16:07:30.898416Z","shell.execute_reply.started":"2023-07-13T16:07:30.640902Z"},"scrolled":true,"trusted":true},"outputs":[{"data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6283,  0.2166,  0.5605,  ...,  0.0136,  0.6158, -0.1712],\n","         [ 0.6108, -0.2253,  0.9263,  ..., -0.3028,  0.4500, -0.0714],\n","         [ 0.8040,  0.1809,  0.7076,  ..., -0.0685,  0.4837, -0.0774],\n","         [ 1.3290,  0.2360,  0.4567,  ...,  0.1509,  0.9621, -0.4841]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7105,  0.4876,  0.9999, -0.9947,  0.9599,  0.9521,  0.9767, -0.9946,\n","         -0.9815, -0.6238,  0.9776,  0.9984, -0.9989, -0.9998,  0.8559, -0.9755,\n","          0.9895, -0.5281, -1.0000, -0.7414, -0.7056, -0.9999,  0.2901,  0.9786,\n","          0.9729,  0.0734,  0.9828,  1.0000,  0.8981, -0.1109,  0.2780, -0.9920,\n","          0.8693, -0.9985,  0.1461,  0.2067,  0.8092, -0.2430,  0.8580, -0.9585,\n","         -0.8130, -0.6138,  0.7961, -0.5727,  0.9737,  0.2362, -0.1194, -0.0789,\n","          0.0031,  0.9997, -0.9519,  0.9899, -0.9962,  0.9931,  0.9950,  0.5050,\n","          0.9952,  0.1090, -0.9994,  0.3416,  0.9792,  0.2506,  0.8923, -0.2238,\n","          0.3518, -0.5293, -0.9570,  0.1357, -0.3313,  0.1627, -0.0078,  0.3608,\n","          0.9833, -0.9160,  0.0196, -0.9141,  0.2075, -0.9999,  0.9449,  1.0000,\n","          0.7796, -0.9997,  0.9935, -0.2309, -0.7830,  0.8880, -0.9994, -0.9994,\n","          0.0160, -0.6875,  0.9554, -0.9846,  0.7813, -0.9322,  1.0000, -0.9511,\n","         -0.1583,  0.3866,  0.9699, -0.8283, -0.7689,  0.9346,  0.9994, -0.9965,\n","          0.9992,  0.8548, -0.9459, -0.9451,  0.8292,  0.0406,  0.9891, -0.9857,\n","         -0.9555,  0.0784,  0.9803, -0.9302,  0.9901,  0.8020, -0.2408,  1.0000,\n","         -0.2367,  0.9689,  0.9982,  0.8761, -0.8891, -0.2167, -0.7227,  0.9385,\n","         -0.7771, -0.5691,  0.8276, -0.9881, -0.9987,  0.9994, -0.2452,  1.0000,\n","         -0.9992,  0.9935, -0.9999, -0.8808, -0.7632, -0.1224, -0.9886,  0.0386,\n","          0.9893,  0.0950, -0.9691, -0.8293,  0.7363, -0.9115,  0.4936,  0.6483,\n","         -0.9581,  0.9715,  0.9982,  0.9653,  0.9890,  0.1850, -0.9722,  0.8544,\n","          0.9774, -0.9995,  0.8701, -0.9960,  0.9993,  0.9746,  0.8500, -0.9971,\n","          0.9999, -0.8101,  0.0206,  0.0232,  0.0982, -0.9993,  0.4584,  0.4759,\n","          0.8759,  0.9993, -0.9945,  0.9995,  0.8257, -0.0958,  0.8097,  0.9992,\n","         -0.9966, -0.9701, -0.9847,  0.2290,  0.8039,  0.8126,  0.4859,  0.9628,\n","          0.9992,  0.7803, -0.9974, -0.3289,  0.9692, -0.1398,  1.0000, -0.4062,\n","         -0.9998, -0.7585,  0.9499,  0.9883, -0.2351,  0.9828, -0.7602, -0.1856,\n","          0.9882, -0.9727,  0.9989,  0.4863,  0.9102,  0.8847,  0.9915, -0.9258,\n","         -0.0788,  0.1543, -0.7543,  0.9999, -0.9996, -0.2332,  0.5874, -0.9942,\n","         -0.9979,  0.9794, -0.0194, -0.8693, -0.2168,  0.7979,  0.2004,  0.9521,\n","          0.9900, -0.6276, -0.7824, -0.9998, -0.9979, -0.8998, -0.9672,  0.0430,\n","          0.6920, -0.3973, -0.9400, -0.9990,  0.9653,  0.4508, -0.8941,  0.1173,\n","         -0.7708, -0.9993,  0.6723, -0.9301, -0.9982,  0.9996, -0.7676,  0.9977,\n","          0.9586, -0.9940,  0.8469, -0.9993, -0.1126, -0.9871,  0.6339,  0.6951,\n","         -0.6402, -0.0494,  0.9932, -0.9665, -0.8052,  0.8924, -0.9999,  0.9337,\n","         -0.2121,  0.9991,  0.8133,  0.2323,  0.9850,  0.9504, -0.9842, -0.9998,\n","          0.9709,  0.8598, -0.9926, -0.1464,  0.9999, -0.9989, -0.8643, -0.9607,\n","         -0.9940, -0.9996,  0.2709, -0.8859,  0.2709,  0.9882,  0.6940,  0.1279,\n","          0.9929,  0.9910,  0.2286, -0.3951,  0.1043, -0.9768, -0.9314,  0.9270,\n","          0.1190, -1.0000,  0.9999, -0.9937,  0.9782,  0.9644, -0.9963,  0.8284,\n","          0.1232, -0.9757,  0.0153,  0.9999,  0.9855, -0.1870,  0.2675,  0.9221,\n","         -0.3209,  0.6954, -0.8953, -0.7505,  0.2031, -0.9330,  0.9959,  0.7163,\n","         -0.9901,  0.9979,  0.0020,  0.8427, -0.8604,  0.9138,  0.9909, -0.1418,\n","         -0.6551,  0.0506, -0.1576, -0.9825,  0.2171, -0.9974, -0.5776,  0.9791,\n","          0.9842, -0.9881,  0.9842, -0.0714,  0.9470, -0.9988,  1.0000, -0.9961,\n","          0.0919,  0.8178, -0.8827, -0.6587,  0.9920,  0.9926,  0.9770, -0.9782,\n","         -0.8553,  0.8854,  0.9670, -0.9807, -0.0805, -0.9996, -0.8613,  0.9954,\n","          0.9971,  0.0548, -0.1328, -0.9985,  0.9674, -0.8499, -0.9574, -0.0873,\n","         -0.8494,  0.8687,  0.9986, -0.7471,  0.7234,  0.1623, -0.9843,  0.9408,\n","          0.9139,  0.9998, -0.9575,  0.6504,  0.9878, -0.2068, -0.8754,  0.6124,\n","          0.9995, -0.9634, -0.2490, -0.9995, -0.0428, -0.6623, -0.3127, -0.6662,\n","          0.0496, -0.8922,  0.9753,  0.0668,  0.8677, -0.4532,  0.9877, -0.1256,\n","         -0.0084, -0.3633, -0.4310,  0.5263,  0.2795,  0.9855, -0.9623,  0.9997,\n","         -0.2454, -1.0000, -0.9985, -0.8149, -0.9996,  0.8664, -0.9936,  0.9833,\n","          0.9645, -0.9990, -0.9995, -0.9971, -0.9827,  0.8944,  0.7312, -0.0281,\n","          0.3282, -0.0801, -0.0505, -0.5034,  0.0436, -0.9379, -0.6098, -0.9991,\n","          0.9075, -1.0000, -0.8943,  0.9983, -0.9974, -0.9702, -0.9073, -0.5143,\n","         -0.8728,  0.5948,  0.9871, -0.4346, -0.8071, -0.9995,  0.9870, -0.8455,\n","          0.0911, -0.9254, -0.9747,  0.9997,  0.8546, -0.1713, -0.0070, -0.9989,\n","          0.9919, -0.9512, -0.9604, -0.9775,  0.2517, -0.9669, -0.9998,  0.0274,\n","          0.9981,  0.9918,  0.9872,  0.3586, -0.4554, -0.9641,  0.1662, -0.9999,\n","          0.8734,  0.9199, -0.9861, -0.7962,  0.9937,  0.9731, -0.9745, -0.9908,\n","          0.9605,  0.3886,  0.9752, -0.6621, -0.5456,  0.3875,  0.0179, -0.9900,\n","         -0.9324,  0.9966, -0.9996,  0.9857,  0.9975,  0.9992, -0.2936,  0.1672,\n","         -0.9914, -0.9807, -0.5163,  0.3407, -0.9999,  0.9999, -1.0000,  0.4725,\n","         -0.8529,  0.9192,  0.9880, -0.4080, -0.9999, -0.9998,  0.3443,  0.1171,\n","          0.9887,  0.4144,  0.1948, -0.6747, -0.3843,  0.9973, -0.8765, -0.8126,\n","         -0.9988,  0.9997,  0.4865, -0.9982,  0.9962, -0.9995,  0.8606,  0.9813,\n","          0.8977,  0.9750, -0.9995,  1.0000, -0.9998,  0.9968, -1.0000, -0.9994,\n","          0.9998, -0.9914, -0.8091, -0.9997, -0.9992,  0.7226,  0.1572, -0.5659,\n","          0.9883, -0.9998, -0.9987, -0.4541, -0.9309, -0.8762,  0.9972, -0.8189,\n","          0.9894, -0.0908,  0.9623,  0.3485,  0.9983,  0.9907, -0.7728, -0.8770,\n","         -0.9934,  0.9906, -0.6931,  0.4029,  0.9709, -0.0175, -0.8430,  0.3529,\n","         -0.9967,  0.5958,  0.1322,  0.9368,  0.9241,  0.8697, -0.1084, -0.5757,\n","         -0.3093, -0.9923,  0.5895, -0.9995,  0.9794, -0.9611,  0.0368, -0.4532,\n","          0.2907, -0.9616,  0.9996,  0.9988, -0.9890,  0.0842,  0.9875, -0.8991,\n","          0.9725, -0.9923,  0.0591,  0.9806, -0.7373,  0.9830, -0.0013,  0.0637,\n","          0.9887, -0.9946, -0.9119, -0.6857,  0.3650,  0.0621, -0.9661,  0.0115,\n","          0.9902, -0.4771, -0.9997,  0.9412, -0.9993, -0.1202,  0.9763, -0.0377,\n","          0.9999, -0.8283,  0.1905,  0.1534, -0.9998, -0.9995,  0.0540, -0.1655,\n","         -0.9095,  0.9996, -0.3539,  0.8774, -0.9999,  0.3085,  0.9981,  0.2593,\n","          0.8318, -0.9150, -0.9679, -0.9740, -0.7623,  0.0018,  0.8898, -0.9830,\n","         -0.6928, -0.9300,  1.0000, -0.9982, -0.9065, -0.9884,  0.7844,  0.9037,\n","          0.4408,  0.1464, -0.9218,  0.9302, -0.9442,  0.9973, -0.9939, -0.9963,\n","          0.9998,  0.5109, -0.9965,  0.2434, -0.4521,  0.3922,  0.1305,  0.8537,\n","         -0.8603, -0.2880, -0.9964,  0.8409, -0.9161, -0.9866, -0.6739, -0.3227,\n","         -0.9776,  0.9935,  0.9731,  0.9999, -0.9998,  0.9386, -0.0222,  0.9991,\n","          0.0548, -0.7093,  0.9169,  0.9997, -0.7628,  0.8665, -0.1445,  0.0659,\n","          0.4782, -0.4443,  0.9984, -0.9318,  0.0735, -0.9737, -0.9999,  0.9999,\n","         -0.0253,  0.9916,  0.3062,  0.8545, -0.9106,  0.9857, -0.9849, -0.9302,\n","         -1.0000,  0.1720, -0.9895, -0.9877, -0.1270,  0.9847, -0.9996, -0.9917,\n","         -0.4165, -1.0000,  0.9550, -0.9948, -0.8871, -0.9896,  0.9988, -0.2867,\n","         -0.8413,  0.9756, -0.9648,  0.9592,  0.9145, -0.4687,  0.2182,  0.1638,\n","         -0.8462, -0.9960, -0.9356, -0.9699,  0.9437, -0.9875, -0.8543,  0.9966,\n","          0.9835, -0.9992, -0.9944,  0.9973,  0.2491,  0.9927, -0.5164, -0.9998,\n","         -0.9999,  0.0790,  0.2047,  0.9947, -0.3889,  0.9576,  0.8522, -0.5662,\n","          0.6047, -0.8014, -0.2958, -0.6084, -0.2891,  1.0000, -0.9179,  0.9894]],\n","       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["output = model(**encoding)\n","\n","output"]},{"cell_type":"code","execution_count":9,"metadata":{"cellId":"ryxpqezgmlko9pjufqe1ib","execution":{"iopub.execute_input":"2023-07-13T16:07:30.901768Z","iopub.status.busy":"2023-07-13T16:07:30.901395Z","iopub.status.idle":"2023-07-13T16:07:30.909289Z","shell.execute_reply":"2023-07-13T16:07:30.907972Z","shell.execute_reply.started":"2023-07-13T16:07:30.901735Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 4, 768])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["output.last_hidden_state.shape"]},{"cell_type":"code","execution_count":10,"metadata":{"cellId":"ierg4tdbhyrqh60hbzyhom","execution":{"iopub.execute_input":"2023-07-13T16:07:30.912012Z","iopub.status.busy":"2023-07-13T16:07:30.911227Z","iopub.status.idle":"2023-07-13T16:07:30.920210Z","shell.execute_reply":"2023-07-13T16:07:30.918993Z","shell.execute_reply.started":"2023-07-13T16:07:30.911977Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 768])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["output.pooler_output.shape"]},{"cell_type":"code","execution_count":11,"metadata":{"cellId":"m7l5vn91elh8xjnt3ctcqw","execution":{"iopub.execute_input":"2023-07-13T16:07:30.926321Z","iopub.status.busy":"2023-07-13T16:07:30.926046Z","iopub.status.idle":"2023-07-13T16:07:36.546695Z","shell.execute_reply":"2023-07-13T16:07:36.545682Z","shell.execute_reply.started":"2023-07-13T16:07:30.926297Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer, model = get_model('roberta')"]},{"cell_type":"code","execution_count":12,"metadata":{"cellId":"js9rugxy80hh6cf71fr86s","execution":{"iopub.execute_input":"2023-07-13T16:07:36.548589Z","iopub.status.busy":"2023-07-13T16:07:36.548212Z","iopub.status.idle":"2023-07-13T16:07:36.560409Z","shell.execute_reply":"2023-07-13T16:07:36.559332Z","shell.execute_reply.started":"2023-07-13T16:07:36.548554Z"},"trusted":true},"outputs":[{"data":{"text/plain":["RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":13,"metadata":{"cellId":"9ug35z8ns5f5vxd45cp5t4","execution":{"iopub.execute_input":"2023-07-13T16:07:36.561936Z","iopub.status.busy":"2023-07-13T16:07:36.561565Z","iopub.status.idle":"2023-07-13T16:07:37.619569Z","shell.execute_reply":"2023-07-13T16:07:37.618484Z","shell.execute_reply.started":"2023-07-13T16:07:36.561903Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[    0, 31414,   328,     2]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["encoding = tokenizer.encode_plus('Hello!', add_special_tokens=True, return_token_type_ids=False, return_tensors='pt')\n","\n","encoding"]},{"cell_type":"code","execution_count":14,"metadata":{"cellId":"yqkia6yrqqoce505sq9e4t","execution":{"iopub.execute_input":"2023-07-13T16:07:37.622370Z","iopub.status.busy":"2023-07-13T16:07:37.621272Z","iopub.status.idle":"2023-07-13T16:07:37.629986Z","shell.execute_reply":"2023-07-13T16:07:37.628813Z","shell.execute_reply.started":"2023-07-13T16:07:37.622334Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'<s>Hello!</s>'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(encoding['input_ids'][0])"]},{"cell_type":"code","execution_count":15,"metadata":{"cellId":"ufgnbnu4o0s8tjmhpez7bs","execution":{"iopub.execute_input":"2023-07-13T16:07:37.632353Z","iopub.status.busy":"2023-07-13T16:07:37.631922Z","iopub.status.idle":"2023-07-13T16:07:37.731330Z","shell.execute_reply":"2023-07-13T16:07:37.730257Z","shell.execute_reply.started":"2023-07-13T16:07:37.632319Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0632,  0.0918, -0.0026,  ..., -0.0642, -0.0555,  0.0080],\n","         [-0.2026, -0.0124,  0.0174,  ..., -0.0176,  0.0094,  0.0870],\n","         [-0.1557, -0.1340,  0.2350,  ..., -0.0798, -0.1061,  0.5419],\n","         [-0.0534,  0.0986, -0.0293,  ..., -0.1175, -0.0659, -0.0090]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 1.0118e-02, -2.2553e-01, -2.2348e-01, -8.2207e-02,  1.1413e-01,\n","          2.0735e-01,  2.7447e-01, -8.3260e-02, -6.2361e-02, -1.7518e-01,\n","          2.1636e-01, -2.5967e-02, -9.4576e-02,  9.4475e-02, -1.3591e-01,\n","          4.9016e-01,  2.1106e-01, -4.6220e-01,  5.0346e-02, -2.3451e-02,\n","         -2.5763e-01,  8.7108e-02,  4.7654e-01,  3.5332e-01,  1.1738e-01,\n","          4.8578e-02, -1.3495e-01,  5.9979e-04,  1.8758e-01,  2.3595e-01,\n","          2.9100e-01,  6.5356e-02,  7.7346e-02,  2.3497e-01, -2.4440e-01,\n","          5.0584e-02, -3.1700e-01,  2.3022e-02,  2.6311e-01, -1.9050e-01,\n","         -8.6479e-02,  1.6912e-01,  2.1422e-01, -1.3219e-01, -1.1539e-01,\n","          4.0322e-01,  2.6244e-01,  1.1725e-02, -1.4109e-01, -8.3927e-02,\n","         -3.6414e-01,  3.4345e-01,  2.8735e-01,  2.1016e-01, -1.5213e-02,\n","          4.9236e-02, -1.4490e-01,  2.6061e-01, -7.1260e-02, -9.3889e-02,\n","         -1.0403e-01, -2.1510e-01, -3.3935e-02, -5.9922e-02,  3.7989e-02,\n","         -1.4566e-01,  8.2178e-02, -1.5788e-01, -1.2733e-01,  5.0785e-02,\n","         -8.8803e-02,  1.6738e-01,  1.6446e-01, -2.9873e-01, -2.8953e-01,\n","          5.0433e-02, -5.8537e-01, -8.9558e-02,  3.1289e-01,  4.3414e-01,\n","         -1.1615e-01,  1.8867e-01,  4.6700e-02,  2.2332e-01, -1.7091e-02,\n","         -5.8655e-02, -2.5692e-02, -1.1094e-01,  1.9310e-01,  2.5799e-01,\n","         -1.9191e-01, -3.8370e-01,  5.7671e-02,  1.0469e-02, -1.0562e-01,\n","          1.1278e-02, -1.8523e-02, -7.8209e-02, -1.6009e-01, -1.8985e-01,\n","          7.3578e-02, -2.6986e-01, -1.5874e-01,  2.8572e-01, -2.7447e-02,\n","         -1.9765e-01, -1.4153e-02,  3.1042e-01,  7.0917e-02, -1.2198e-01,\n","         -1.8918e-01,  4.4590e-01,  3.1875e-01,  8.7024e-03,  5.6866e-03,\n","          1.6598e-01,  1.3123e-01, -2.8546e-01,  4.3665e-01, -3.2801e-01,\n","         -1.8452e-02, -9.8888e-02,  1.0351e-01,  1.4738e-01, -2.2820e-01,\n","          2.8747e-01,  1.5277e-01,  2.7726e-01,  1.8845e-01,  1.1907e-01,\n","         -2.5823e-02,  1.5180e-01, -9.6640e-02,  1.4872e-01,  2.4250e-01,\n","          1.1419e-01, -8.7491e-03, -3.3386e-01, -2.0569e-01,  2.8772e-01,\n","          3.5000e-01,  1.5743e-01, -4.6989e-02,  2.0326e-01,  1.0744e-01,\n","          2.1788e-01,  1.5106e-01, -4.1663e-01,  3.3106e-02,  3.4290e-01,\n","          9.1567e-02,  1.7440e-01, -9.1458e-02, -2.7184e-01, -2.6769e-01,\n","         -1.0485e-01,  5.3897e-02, -3.2522e-01, -1.2076e-01,  3.6939e-01,\n","          3.5122e-02, -1.1313e-02, -1.5769e-01, -2.2375e-01, -2.4918e-02,\n","         -1.0496e-01,  1.4617e-02,  1.0023e-01, -9.3627e-02, -4.2223e-01,\n","         -9.1500e-02, -5.5379e-01, -1.1524e-01,  1.8061e-01, -3.3040e-01,\n","          2.6260e-01, -2.8920e-01,  1.0215e-01,  4.0565e-01,  5.7942e-02,\n","         -1.3332e-02, -1.9797e-01, -1.8287e-02,  1.1391e-01,  3.2504e-01,\n","          2.5887e-01, -4.0658e-01,  1.0513e-01,  1.4425e-01,  2.4611e-01,\n","          1.3859e-01, -7.9068e-02, -1.3120e-01,  1.4771e-01, -2.1248e-01,\n","          1.7976e-01, -2.2309e-01,  1.8665e-01, -2.5408e-01, -2.1255e-01,\n","          2.8222e-01, -4.1031e-01, -2.8460e-02,  8.5511e-02,  2.7669e-01,\n","         -7.6721e-03, -3.4935e-02, -8.5721e-02,  1.0734e-01,  1.7564e-01,\n","          1.4225e-01, -4.0331e-01,  2.6446e-01, -2.9884e-02, -1.6981e-02,\n","         -2.3724e-02,  1.7601e-01,  2.5539e-01,  8.3780e-02, -3.8247e-01,\n","         -1.4288e-01,  1.2093e-01,  2.7932e-01, -2.4104e-01,  1.6109e-01,\n","         -2.9300e-01, -4.0703e-01, -1.2663e-01,  2.2077e-01,  2.3947e-01,\n","          1.8266e-01, -2.7795e-01,  1.7054e-01, -1.0591e-01, -4.1658e-01,\n","         -3.7567e-01, -1.2646e-01,  2.3536e-01,  1.6887e-01,  1.7926e-01,\n","          2.5752e-01,  3.7846e-02,  1.1938e-01,  1.5864e-01,  1.6021e-01,\n","         -1.5266e-01,  1.7528e-01, -3.6356e-01, -4.8026e-02, -2.6735e-01,\n","         -1.9393e-01, -2.1720e-01,  4.0867e-01, -2.2771e-01,  2.3330e-01,\n","          3.9985e-01, -3.0882e-01, -1.2435e-01,  1.5003e-01,  8.8648e-02,\n","          7.9778e-02, -1.1415e-01,  1.9528e-01,  1.4956e-01, -1.1915e-01,\n","          2.5725e-01,  9.2090e-03,  2.6384e-01,  1.6437e-01,  9.9818e-02,\n","          1.3314e-01,  1.3148e-01, -1.4667e-01,  6.2790e-02,  1.6111e-02,\n","         -2.0466e-02, -2.3305e-01, -1.4842e-01,  2.2611e-01, -5.8155e-02,\n","          3.4740e-02, -1.8005e-01, -1.1308e-01,  2.1972e-02,  4.1089e-01,\n","         -3.6362e-01,  2.5254e-01,  8.4500e-02,  1.4597e-01, -2.3708e-01,\n","         -2.3668e-01,  8.8839e-02,  1.8577e-01, -4.0227e-01,  1.5481e-02,\n","          1.7293e-01,  9.6360e-02,  2.2496e-01,  2.7419e-01,  1.7919e-03,\n","         -1.2764e-01,  5.0044e-01, -1.6126e-01, -1.2322e-01,  2.5468e-01,\n","         -2.6847e-01, -2.9783e-01,  2.4442e-01, -4.2464e-02,  2.9567e-01,\n","          1.3648e-01,  5.1872e-02,  7.3852e-02, -6.0790e-01,  6.5884e-02,\n","         -4.5680e-01, -1.3807e-02,  2.6241e-02, -8.6523e-02, -2.0121e-01,\n","          1.5596e-01,  2.9137e-01, -2.5556e-01, -2.3166e-02,  2.0865e-01,\n","          8.5941e-02, -1.1919e-01,  4.7576e-01, -1.4159e-02,  2.2871e-01,\n","         -4.9841e-02,  2.5398e-01, -2.1359e-01,  2.6585e-01, -2.7601e-01,\n","         -1.0925e-01,  2.4161e-02,  9.1276e-02,  6.0348e-02, -6.0059e-02,\n","         -3.3812e-01,  2.4309e-01, -3.3747e-02, -4.9382e-02, -4.3979e-02,\n","          9.4939e-02, -5.1362e-03,  5.6559e-02,  5.4164e-02,  3.3093e-01,\n","          2.2633e-01, -2.0338e-02, -3.7753e-01, -4.6650e-02, -8.6735e-02,\n","          3.9355e-02,  4.5744e-02, -1.9750e-02,  4.4724e-01, -9.5248e-02,\n","          5.6638e-03, -1.4348e-01,  2.6583e-01,  2.0027e-01,  1.2248e-01,\n","          1.1799e-01,  5.2730e-02,  1.4474e-01, -5.7766e-02, -2.1255e-02,\n","         -1.5518e-01, -2.4543e-01, -2.7947e-01,  2.0526e-01, -2.5367e-01,\n","         -1.8341e-01,  1.5058e-01,  1.9739e-01, -1.5367e-01,  1.3827e-01,\n","          3.0879e-01,  1.0774e-01, -1.4018e-01,  2.8086e-01, -1.2816e-01,\n","          1.1241e-01,  3.0695e-01, -1.6665e-02,  1.7593e-01,  5.0237e-01,\n","          2.0615e-01, -3.7308e-01, -4.5742e-02, -2.1244e-01,  6.5450e-03,\n","          2.6009e-01, -1.5179e-01,  2.0992e-01,  3.8641e-01,  3.2387e-01,\n","          4.5787e-01,  2.7464e-03, -1.3871e-01,  8.4105e-02,  2.2514e-01,\n","          1.7218e-02, -1.6348e-01, -1.7553e-01,  2.5409e-01,  5.2481e-02,\n","         -1.4782e-01, -3.4883e-02, -1.3299e-01,  3.9151e-02, -1.2918e-01,\n","         -4.0502e-01,  4.1638e-02,  1.9915e-01, -4.7533e-01,  8.5064e-02,\n","         -2.7669e-01,  4.3024e-02, -2.4551e-01,  2.1614e-01, -2.1704e-01,\n","         -1.0856e-01,  4.0376e-01, -7.0558e-02,  5.3691e-02, -1.8987e-01,\n","         -1.4700e-01,  1.7793e-02,  1.9047e-02, -2.5505e-02, -2.8986e-02,\n","          3.4475e-01, -1.2815e-01,  4.3385e-02,  1.7214e-02,  2.1012e-01,\n","         -5.0543e-02,  1.9904e-01,  1.2607e-02, -1.3551e-01, -3.8602e-01,\n","          1.3687e-01, -1.9452e-01, -4.2718e-01, -3.8813e-01,  3.7123e-01,\n","         -1.3491e-01, -2.5552e-01, -2.1628e-01, -2.5203e-01,  8.6603e-02,\n","          1.8874e-01,  4.6717e-01, -3.9924e-01, -9.3266e-02,  4.6801e-01,\n","         -5.9054e-02, -1.8160e-01,  2.8641e-01,  1.8650e-01, -3.3684e-01,\n","          3.3408e-01,  2.7453e-01, -5.1202e-02,  1.9442e-02,  5.1057e-01,\n","          1.2458e-01,  1.9900e-01, -2.2594e-01,  4.4677e-01, -2.0924e-01,\n","          3.2095e-01, -1.4214e-01, -2.2136e-01, -2.1145e-01, -1.1643e-02,\n","          3.3575e-01,  1.9193e-01, -4.2778e-01, -1.2267e-01,  4.0184e-02,\n","          3.6021e-01, -3.8999e-01, -9.3912e-02,  3.0063e-02, -3.4897e-01,\n","          1.1534e-01,  1.0927e-01,  2.2407e-01, -3.8088e-01, -6.9022e-03,\n","          3.9142e-01, -3.0255e-01,  1.3211e-01,  3.0998e-01,  7.0233e-02,\n","          3.3604e-01, -3.9129e-02,  7.1658e-03,  4.5464e-02, -2.3357e-01,\n","         -3.5424e-02,  1.4983e-01,  5.6157e-01,  1.5481e-01, -3.7794e-01,\n","          9.6562e-02,  2.5686e-01, -1.8405e-01,  3.1480e-01, -9.8664e-02,\n","         -4.9152e-02,  2.6249e-01, -4.7398e-02,  1.4743e-01, -9.5249e-02,\n","         -2.3134e-01, -3.1139e-01,  3.8853e-01, -2.1314e-01, -1.1326e-01,\n","         -1.5672e-01, -1.1077e-01, -1.4227e-01,  4.2703e-02, -3.8290e-01,\n","          3.4183e-01,  1.2081e-01, -2.0239e-01, -1.0166e-01, -6.4598e-02,\n","         -1.5710e-01, -2.2432e-01, -2.6498e-01,  4.2805e-01, -1.7004e-01,\n","         -4.5224e-01,  2.5748e-01,  3.4049e-02,  3.4972e-01,  2.3862e-02,\n","          1.1412e-01, -4.0268e-02,  1.3973e-01,  9.7067e-02, -1.3805e-01,\n","          2.9496e-01,  5.1764e-02, -5.5853e-01, -1.3225e-01, -2.2105e-01,\n","          7.7306e-02,  1.9263e-01, -3.4430e-01,  2.3568e-02,  2.5253e-02,\n","          1.3726e-01,  1.8869e-02, -1.2904e-01, -6.3532e-02,  4.0615e-01,\n","          2.3894e-01,  2.8557e-01,  1.0163e-01,  2.3908e-01, -2.7205e-02,\n","         -3.3512e-01,  5.0001e-02,  9.2257e-02, -2.1549e-01,  4.3521e-01,\n","         -9.5459e-02, -4.0110e-01, -6.6202e-02,  4.0455e-01,  8.8541e-02,\n","         -3.0166e-02, -2.4921e-02,  2.0519e-01,  1.5607e-01, -1.3660e-01,\n","          1.7513e-01, -2.1779e-02, -1.3987e-01, -1.1539e-01,  7.7553e-02,\n","         -2.2114e-01,  4.9953e-02, -1.4691e-01, -6.5428e-03, -2.0092e-01,\n","         -1.2912e-03, -2.1149e-01,  2.6101e-01, -3.4130e-01,  1.0355e-01,\n","          6.9206e-02,  2.9983e-01, -3.5010e-01, -1.5777e-01, -5.8081e-02,\n","          1.5107e-01,  2.6498e-01,  3.5853e-01,  3.1821e-02,  2.0004e-02,\n","         -1.6611e-01, -2.6602e-01,  6.3947e-02, -2.1414e-01,  1.3864e-01,\n","          6.6462e-02,  2.6377e-01, -3.1288e-01, -1.9596e-01,  2.2185e-01,\n","         -9.2401e-02, -1.4946e-01,  4.2032e-01,  2.5380e-01,  2.1337e-01,\n","          1.4461e-02,  2.5495e-01,  3.6831e-02, -1.7628e-01, -1.2115e-01,\n","         -2.4977e-01,  8.9324e-02, -9.1066e-02, -5.5009e-02, -6.3157e-02,\n","         -1.0775e-01, -2.0748e-01, -1.7954e-01,  1.4105e-01,  1.5018e-01,\n","          2.8174e-02, -6.2964e-02, -2.7871e-02, -2.9133e-01,  3.0717e-01,\n","          1.2619e-02,  7.7062e-02, -6.0462e-02,  3.2284e-02, -1.4622e-01,\n","          2.2979e-01,  2.1566e-01,  1.0340e-01, -2.1087e-01, -4.8798e-02,\n","         -2.9432e-01, -3.6423e-01,  5.7379e-02,  1.4822e-01,  1.2086e-01,\n","         -9.1928e-02, -3.0596e-01, -2.4023e-02, -1.3632e-01,  1.8085e-01,\n","          9.3025e-03, -1.5814e-01, -9.5690e-02, -5.0533e-02, -5.0963e-02,\n","          7.7562e-02, -2.1167e-01, -1.9824e-01, -1.2067e-01, -7.1133e-02,\n","         -8.1443e-02,  3.6474e-01, -4.7666e-02,  2.9349e-01, -1.6252e-01,\n","          8.0746e-03, -1.7717e-01,  1.1926e-01, -6.5944e-02,  6.8345e-02,\n","          2.7466e-01, -4.3901e-01, -1.6312e-01, -1.4789e-02, -2.0616e-01,\n","         -1.7526e-01, -6.4864e-02, -3.9855e-02,  2.3424e-01, -3.4360e-01,\n","          2.3490e-01, -1.1169e-01,  1.9504e-01, -7.1054e-02, -2.5389e-01,\n","         -1.5752e-01,  1.6864e-02,  2.5012e-01, -3.4816e-01, -2.4864e-01,\n","         -2.7978e-01, -1.0969e-01, -8.8405e-02, -2.6577e-01,  4.2844e-01,\n","         -1.1936e-01, -7.8804e-02,  1.3716e-02,  4.4209e-01,  1.9712e-01,\n","          1.5742e-01,  2.0831e-01, -1.3510e-02,  3.7341e-02,  1.1352e-01,\n","         -4.6823e-01,  2.4004e-01, -2.3180e-01, -1.2410e-01,  3.5086e-03,\n","          9.5102e-02, -3.7601e-02,  4.0568e-03, -1.4060e-01, -1.1048e-01,\n","          2.1256e-01, -3.6218e-01, -4.2808e-02,  2.8913e-01,  1.5989e-01,\n","         -2.5916e-01,  3.4223e-02,  1.1789e-01,  3.8187e-01,  9.9987e-02,\n","         -2.1562e-01,  1.3011e-01, -3.5898e-01, -4.6917e-02, -2.0830e-01,\n","         -2.9843e-01,  1.6889e-01, -7.5929e-02,  6.6976e-02, -9.7291e-02,\n","         -2.8745e-01,  2.0640e-01, -5.0420e-02, -7.3856e-02,  4.2221e-01,\n","          2.8958e-02, -1.0939e-01,  1.4974e-01,  1.2731e-02,  2.0679e-02,\n","         -1.0458e-01,  2.5952e-01,  2.0616e-01, -2.7987e-01,  1.3307e-01,\n","         -1.3394e-01, -4.5800e-02, -1.0073e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["output = model(**encoding)\n","\n","output"]},{"cell_type":"code","execution_count":16,"metadata":{"cellId":"d9fgs5bfx5z9kez5rcrag","execution":{"iopub.execute_input":"2023-07-13T16:07:37.733339Z","iopub.status.busy":"2023-07-13T16:07:37.732930Z","iopub.status.idle":"2023-07-13T16:07:37.741653Z","shell.execute_reply":"2023-07-13T16:07:37.740539Z","shell.execute_reply.started":"2023-07-13T16:07:37.733295Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 4, 768])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["output.last_hidden_state.shape"]},{"cell_type":"code","execution_count":17,"metadata":{"cellId":"1js6rxki3g432uiphsys2x","execution":{"iopub.execute_input":"2023-07-13T16:07:37.744058Z","iopub.status.busy":"2023-07-13T16:07:37.743191Z","iopub.status.idle":"2023-07-13T16:07:37.753487Z","shell.execute_reply":"2023-07-13T16:07:37.752271Z","shell.execute_reply.started":"2023-07-13T16:07:37.744014Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 768])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["output.pooler_output.shape"]},{"cell_type":"code","execution_count":18,"metadata":{"cellId":"231wk6rflkjafpdzg8kir","execution":{"iopub.execute_input":"2023-07-13T16:07:37.755854Z","iopub.status.busy":"2023-07-13T16:07:37.755394Z","iopub.status.idle":"2023-07-13T16:07:41.579901Z","shell.execute_reply":"2023-07-13T16:07:41.578845Z","shell.execute_reply.started":"2023-07-13T16:07:37.755821Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer, model = get_model('distilbert')"]},{"cell_type":"code","execution_count":19,"metadata":{"cellId":"ovm11d6s6grt25h3kfnkik","execution":{"iopub.execute_input":"2023-07-13T16:07:41.582173Z","iopub.status.busy":"2023-07-13T16:07:41.581357Z","iopub.status.idle":"2023-07-13T16:07:41.589918Z","shell.execute_reply":"2023-07-13T16:07:41.588956Z","shell.execute_reply.started":"2023-07-13T16:07:41.582130Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":20,"metadata":{"cellId":"28xoztlx805qrw1iifwuub","execution":{"iopub.execute_input":"2023-07-13T16:07:41.592319Z","iopub.status.busy":"2023-07-13T16:07:41.591601Z","iopub.status.idle":"2023-07-13T16:07:41.927706Z","shell.execute_reply":"2023-07-13T16:07:41.926557Z","shell.execute_reply.started":"2023-07-13T16:07:41.592284Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[ 101, 8667,  106,  102]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["encoding = tokenizer.encode_plus('Hello!', add_special_tokens=True, return_token_type_ids=False, return_tensors='pt')\n","\n","encoding"]},{"cell_type":"code","execution_count":21,"metadata":{"cellId":"i5tzbn2rlgtzohehdcve","execution":{"iopub.execute_input":"2023-07-13T16:07:41.929859Z","iopub.status.busy":"2023-07-13T16:07:41.929438Z","iopub.status.idle":"2023-07-13T16:07:41.940440Z","shell.execute_reply":"2023-07-13T16:07:41.939365Z","shell.execute_reply.started":"2023-07-13T16:07:41.929826Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'[CLS] Hello! [SEP]'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(encoding['input_ids'][0])"]},{"cell_type":"code","execution_count":22,"metadata":{"cellId":"osmxaat8heg23t48fy0we","execution":{"iopub.execute_input":"2023-07-13T16:07:41.943177Z","iopub.status.busy":"2023-07-13T16:07:41.942023Z","iopub.status.idle":"2023-07-13T16:07:41.994662Z","shell.execute_reply":"2023-07-13T16:07:41.993488Z","shell.execute_reply.started":"2023-07-13T16:07:41.943149Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BaseModelOutput(last_hidden_state=tensor([[[ 0.3344,  0.2363,  0.1612,  ...,  0.1000,  0.2444, -0.0667],\n","         [ 0.1081,  0.3777,  0.4785,  ...,  0.3207,  0.6012,  0.1848],\n","         [ 0.6096,  0.5275,  0.6734,  ...,  0.3253,  0.1565,  0.1588],\n","         [ 0.5685,  0.6846,  0.5774,  ...,  0.8333,  0.9534, -0.0596]]],\n","       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["output = model(**encoding)\n","\n","output"]},{"cell_type":"code","execution_count":23,"metadata":{"cellId":"bjuvrzjm4iazi770yo4ts","execution":{"iopub.execute_input":"2023-07-13T16:07:41.998336Z","iopub.status.busy":"2023-07-13T16:07:41.998046Z","iopub.status.idle":"2023-07-13T16:07:42.006106Z","shell.execute_reply":"2023-07-13T16:07:42.005145Z","shell.execute_reply.started":"2023-07-13T16:07:41.998311Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 4, 768])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["output.last_hidden_state.shape"]},{"cell_type":"markdown","metadata":{"cellId":"c5mt4chwj4rlt9uxm6kkn","execution_id":"dc266d1e-6904-4305-974f-3f5bc8943352"},"source":["## Датасеты\n","\n","https://huggingface.co/datasets"]},{"cell_type":"code","execution_count":24,"metadata":{"cellId":"3198ksgkmth5ho0umw0y9m","execution":{"iopub.execute_input":"2023-07-13T16:07:42.008245Z","iopub.status.busy":"2023-07-13T16:07:42.007440Z","iopub.status.idle":"2023-07-13T16:08:22.303899Z","shell.execute_reply":"2023-07-13T16:08:22.302950Z","shell.execute_reply.started":"2023-07-13T16:07:42.008199Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset imdb (C:/Users/Alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n","100%|██████████| 3/3 [00:00<00:00, 57.51it/s]\n"]}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"imdb\")"]},{"cell_type":"code","execution_count":25,"metadata":{"cellId":"ctweialvomsjn69tgx8x","execution":{"iopub.execute_input":"2023-07-13T16:08:22.306987Z","iopub.status.busy":"2023-07-13T16:08:22.305201Z","iopub.status.idle":"2023-07-13T16:08:22.314088Z","shell.execute_reply":"2023-07-13T16:08:22.312880Z","shell.execute_reply.started":"2023-07-13T16:08:22.306949Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    test: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 25000\n","    })\n","    unsupervised: Dataset({\n","        features: ['text', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":26,"metadata":{"cellId":"hdin2z380y3fg6eacms8r","execution":{"iopub.execute_input":"2023-07-13T16:08:22.316322Z","iopub.status.busy":"2023-07-13T16:08:22.315888Z","iopub.status.idle":"2023-07-13T16:08:22.326845Z","shell.execute_reply":"2023-07-13T16:08:22.325682Z","shell.execute_reply.started":"2023-07-13T16:08:22.316267Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text', 'label'],\n","    num_rows: 25000\n","})"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train']"]},{"cell_type":"code","execution_count":27,"metadata":{"cellId":"uinv0vtq5lbwr77j502m5o","execution":{"iopub.execute_input":"2023-07-13T16:08:22.329675Z","iopub.status.busy":"2023-07-13T16:08:22.328855Z","iopub.status.idle":"2023-07-13T16:08:22.338625Z","shell.execute_reply":"2023-07-13T16:08:22.337513Z","shell.execute_reply.started":"2023-07-13T16:08:22.329600Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n"," 'label': 0}"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["dataset['train'][0]"]},{"cell_type":"code","execution_count":28,"metadata":{"cellId":"8pq48qh8685zehh7s8dq7e","execution":{"iopub.execute_input":"2023-07-13T16:08:22.341266Z","iopub.status.busy":"2023-07-13T16:08:22.340430Z","iopub.status.idle":"2023-07-13T16:08:22.347835Z","shell.execute_reply":"2023-07-13T16:08:22.346858Z","shell.execute_reply.started":"2023-07-13T16:08:22.341231Z"},"trusted":true},"outputs":[],"source":["def tokenization(example):\n","    return tokenizer.batch_encode_plus(example['text'], add_special_tokens=True, return_token_type_ids=False, truncation=True)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"cellId":"wstqr51kxytkezqrizmo","execution":{"iopub.execute_input":"2023-07-13T16:08:22.355349Z","iopub.status.busy":"2023-07-13T16:08:22.354997Z","iopub.status.idle":"2023-07-13T16:08:23.156680Z","shell.execute_reply":"2023-07-13T16:08:23.155654Z","shell.execute_reply.started":"2023-07-13T16:08:22.355323Z"},"trusted":true},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":30,"metadata":{"cellId":"ys9mi6x8n8bl6uwhe3uke","execution":{"iopub.execute_input":"2023-07-13T16:08:23.159252Z","iopub.status.busy":"2023-07-13T16:08:23.158520Z","iopub.status.idle":"2023-07-13T16:08:23.164290Z","shell.execute_reply":"2023-07-13T16:08:23.163117Z","shell.execute_reply.started":"2023-07-13T16:08:23.159215Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":31,"metadata":{"cellId":"tls1wuu9sgqtx140sa1cfe","execution":{"iopub.execute_input":"2023-07-13T16:08:23.166694Z","iopub.status.busy":"2023-07-13T16:08:23.166114Z","iopub.status.idle":"2023-07-13T16:08:23.176539Z","shell.execute_reply":"2023-07-13T16:08:23.175497Z","shell.execute_reply.started":"2023-07-13T16:08:23.166642Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","from transformers import BertForSequenceClassification  # https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification\n","from transformers import RobertaForSequenceClassification  # https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaForSequenceClassification\n","from transformers import DistilBertForSequenceClassification  # https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertForSequenceClassification\n","\n","\n","def get_model_sc(model_name):\n","    assert model_name in ['bert', 'roberta', 'distilbert']\n","    \n","    checkpoint_names = {\n","        'bert': 'bert-base-cased',  # https://huggingface.co/bert-base-cased\n","        'roberta': 'roberta-base',  # https://huggingface.co/roberta-base\n","        'distilbert': 'distilbert-base-cased'  # https://huggingface.co/distilbert-base-cased\n","    }\n","\n","    model_classes = {\n","        'bert': BertForSequenceClassification,\n","        'roberta': RobertaForSequenceClassification,\n","        'distilbert': DistilBertForSequenceClassification\n","    }\n","\n","    return AutoTokenizer.from_pretrained(checkpoint_names[model_name]), model_classes[model_name].from_pretrained(checkpoint_names[model_name], num_labels=2)"]},{"cell_type":"markdown","metadata":{"cellId":"8v6ujlj0z1bnwdgnexwo8r","execution_id":"06e74472-434f-4190-956a-df20d390a13d"},"source":["## Обучение (не будем запускать)"]},{"cell_type":"markdown","metadata":{"cellId":"4nqavfd9dhmrcrp76unm1k","execution_id":"e0014cb0-2b6a-468f-88f4-0811af342c6c"},"source":["## Альтернативный вариант"]},{"cell_type":"code","execution_count":32,"metadata":{"cellId":"wnujsbeqy8g7ssr5014l","execution":{"iopub.execute_input":"2023-07-13T16:08:23.178805Z","iopub.status.busy":"2023-07-13T16:08:23.177853Z","iopub.status.idle":"2023-07-13T16:08:24.113310Z","shell.execute_reply":"2023-07-13T16:08:24.112334Z","shell.execute_reply.started":"2023-07-13T16:08:23.178770Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["tokenizer, model = get_model('distilbert')"]},{"cell_type":"code","execution_count":33,"metadata":{"cellId":"wfdbm64x0422zplv22tgeh","execution":{"iopub.execute_input":"2023-07-13T16:08:24.115339Z","iopub.status.busy":"2023-07-13T16:08:24.114969Z","iopub.status.idle":"2023-07-13T16:08:24.187703Z","shell.execute_reply":"2023-07-13T16:08:24.186565Z","shell.execute_reply.started":"2023-07-13T16:08:24.115304Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","print(device)\n","# print(torch.cuda.get_device_name())"]},{"cell_type":"code","execution_count":34,"metadata":{"cellId":"rfsesl3a9884b4znoz1wb1","execution":{"iopub.execute_input":"2023-07-13T16:08:24.189933Z","iopub.status.busy":"2023-07-13T16:08:24.189451Z","iopub.status.idle":"2023-07-13T16:08:29.841805Z","shell.execute_reply":"2023-07-13T16:08:29.840537Z","shell.execute_reply.started":"2023-07-13T16:08:24.189897Z"},"trusted":true},"outputs":[],"source":["model = model.to(device)"]},{"cell_type":"code","execution_count":35,"metadata":{"cellId":"kjp22pduy3ipw4w8lkbhl","execution":{"iopub.execute_input":"2023-07-13T16:08:29.843651Z","iopub.status.busy":"2023-07-13T16:08:29.843222Z","iopub.status.idle":"2023-07-13T16:08:29.851663Z","shell.execute_reply":"2023-07-13T16:08:29.850539Z","shell.execute_reply.started":"2023-07-13T16:08:29.843608Z"},"trusted":true},"outputs":[],"source":["def tokenization(example):\n","    return tokenizer.batch_encode_plus(example['text'], add_special_tokens=True, return_token_type_ids=False, truncation=True)"]},{"cell_type":"code","execution_count":36,"metadata":{"cellId":"mwg7jc04m1jjh0defuivcs","execution":{"iopub.execute_input":"2023-07-13T16:08:29.854665Z","iopub.status.busy":"2023-07-13T16:08:29.853796Z","iopub.status.idle":"2023-07-13T16:08:29.863521Z","shell.execute_reply":"2023-07-13T16:08:29.862425Z","shell.execute_reply.started":"2023-07-13T16:08:29.854629Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","\n","@torch.inference_mode()\n","def get_embeddings_labels(model, loader):\n","    model.eval()\n","    \n","    total_embeddings = []\n","    labels = []\n","    \n","    for batch in tqdm(loader):\n","        labels.append(batch['labels'].unsqueeze(1))\n","\n","        batch = {key: batch[key].to(device) for key in ['attention_mask', 'input_ids']}\n","\n","        embeddings = model(**batch)['last_hidden_state'][:, 0, :]\n","\n","        total_embeddings.append(embeddings.cpu())\n","\n","    return torch.cat(total_embeddings, dim=0), torch.cat(labels, dim=0).to(torch.float32)"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:08:29.865639Z","iopub.status.busy":"2023-07-13T16:08:29.865066Z","iopub.status.idle":"2023-07-13T16:08:50.676803Z","shell.execute_reply":"2023-07-13T16:08:50.675664Z","shell.execute_reply.started":"2023-07-13T16:08:29.865600Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Found cached dataset imdb (C:/Users/Alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n","Loading cached processed dataset at C:\\Users\\Alex\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-a2bd8ebf4e0e8394.arrow\n"]}],"source":["from datasets import load_dataset\n","from torch.utils.data import Subset\n","\n","dataset = load_dataset(\"imdb\", split=\"train\")\n","dataset = dataset.map(tokenization, batched=True)\n","dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","\n","np.random.seed(100)\n","idx = np.random.randint(len(dataset), size=200).tolist()\n","\n","loader = DataLoader(Subset(dataset, idx), batch_size=50, collate_fn=data_collator, pin_memory=True, shuffle=False)"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:08:50.678328Z","iopub.status.busy":"2023-07-13T16:08:50.677986Z","iopub.status.idle":"2023-07-13T16:09:21.294796Z","shell.execute_reply":"2023-07-13T16:09:21.293854Z","shell.execute_reply.started":"2023-07-13T16:08:50.678295Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Found cached dataset imdb (C:/Users/Alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n","Loading cached processed dataset at C:\\Users\\Alex\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-f15e83b45d0fe7c6.arrow\n","  0%|          | 0/4 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"," 50%|█████     | 2/4 [00:24<00:24, 12.47s/it]\n"]},{"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[49], line 26\u001b[0m\n\u001b[0;32m     19\u001b[0m idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39mlen\u001b[39m(dataset), size\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     21\u001b[0m loader \u001b[39m=\u001b[39m DataLoader(Subset(dataset, idx), batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, collate_fn\u001b[39m=\u001b[39mdata_collator, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 26\u001b[0m embeddings, labels \u001b[39m=\u001b[39m get_embeddings_labels(model, loader)\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(embeddings\u001b[39m.\u001b[39mshape, labels\u001b[39m.\u001b[39mshape)\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","Cell \u001b[1;32mIn[47], line 19\u001b[0m, in \u001b[0;36mget_embeddings_labels\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     16\u001b[0m total_embeddings \u001b[39m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m labels \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(loader):\n\u001b[0;32m     20\u001b[0m     labels\u001b[39m.\u001b[39mappend(batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[0;32m     22\u001b[0m     batch \u001b[39m=\u001b[39m {key: batch[key]\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]}\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:679\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    677\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m--> 679\u001b[0m     data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39;49mpin_memory\u001b[39m.\u001b[39;49mpin_memory(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pin_memory_device)\n\u001b[0;32m    680\u001b[0m \u001b[39mreturn\u001b[39;00m data\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:60\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m     59\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)({k: pin_memory(sample, device) \u001b[39mfor\u001b[39;00m k, sample \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()})  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[39mreturn\u001b[39;00m {k: pin_memory(sample, device) \u001b[39mfor\u001b[39;00m k, sample \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()}\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:60\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m     59\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)({k: pin_memory(sample, device) \u001b[39mfor\u001b[39;00m k, sample \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()})  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m         \u001b[39mreturn\u001b[39;00m {k: pin_memory(sample, device) \u001b[39mfor\u001b[39;00m k, sample \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems()}\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:55\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpin_memory\u001b[39m(data, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m---> 55\u001b[0m         \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39;49mpin_memory(device)\n\u001b[0;32m     56\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n\u001b[0;32m     57\u001b[0m         \u001b[39mreturn\u001b[39;00m data\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}],"source":["# ['bert', 'roberta', 'distilbert']\n","\n","from datasets import load_dataset\n","from torch.utils.data import Subset\n","\n","tokenizer, model = get_model('bert')\n","model = model.to(device)\n","\n","from transformers import DataCollatorWithPadding\n","\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","dataset = load_dataset(\"imdb\", split=\"train\")\n","dataset = dataset.map(tokenization, batched=True)\n","dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","\n","np.random.seed(100)\n","idx = np.random.randint(len(dataset), size=200).tolist()\n","\n","loader = DataLoader(Subset(dataset, idx), batch_size=50, collate_fn=data_collator, pin_memory=True, shuffle=False)\n","\n","\n","\n","\n","embeddings, labels = get_embeddings_labels(model, loader)\n","\n","print(embeddings.shape, labels.shape)"]},{"cell_type":"code","execution_count":39,"metadata":{"cellId":"bvk5z0i3d5rwa4s2xlol2","execution":{"iopub.execute_input":"2023-07-13T16:09:21.296996Z","iopub.status.busy":"2023-07-13T16:09:21.296356Z","iopub.status.idle":"2023-07-13T16:09:21.304643Z","shell.execute_reply":"2023-07-13T16:09:21.303547Z","shell.execute_reply.started":"2023-07-13T16:09:21.296959Z"},"trusted":true},"outputs":[],"source":["torch.save(embeddings, f'bert_embeddings.pt')"]},{"cell_type":"code","execution_count":40,"metadata":{"cellId":"z09rhihuhtj1xdizf7x98","execution":{"iopub.execute_input":"2023-07-13T16:09:21.308014Z","iopub.status.busy":"2023-07-13T16:09:21.307699Z","iopub.status.idle":"2023-07-13T16:09:52.043988Z","shell.execute_reply":"2023-07-13T16:09:52.042934Z","shell.execute_reply.started":"2023-07-13T16:09:21.307988Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Found cached dataset imdb (C:/Users/Alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n","Loading cached processed dataset at C:\\Users\\Alex\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-c5cd06045a3cbf68.arrow\n","  0%|          | 0/4 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","100%|██████████| 4/4 [00:02<00:00,  1.82it/s]"]},{"name":"stdout","output_type":"stream","text":["torch.Size([200, 768]) torch.Size([200, 1])\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# ['bert', 'roberta', 'distilbert']\n","\n","tokenizer, model = get_model('roberta')\n","model = model.to(device)\n","from transformers import DataCollatorWithPadding\n","\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","dataset = load_dataset(\"imdb\", split=\"train\")\n","dataset = dataset.map(tokenization, batched=True)\n","dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","\n","np.random.seed(100)\n","idx = np.random.randint(len(dataset), size=200).tolist()\n","\n","loader = DataLoader(Subset(dataset, idx), batch_size=50, collate_fn=data_collator, pin_memory=True, shuffle=False)\n","\n","\n","embeddings, labels = get_embeddings_labels(model, loader)\n","\n","print(embeddings.shape, labels.shape)\n","\n","torch.save(embeddings, f'roberta_embeddings.pt')"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 600kB/s]\n","Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 28.0kB/s]\n","Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 483kB/s]\n","Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","  0%|          | 0/4 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","  0%|          | 0/4 [00:00<?, ?it/s]\n"]},{"ename":"IndexError","evalue":"index out of range in self","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[47], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m     37\u001b[0m loader \u001b[39m=\u001b[39m DataLoader(Subset(dataset, idx), batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, collate_fn\u001b[39m=\u001b[39mdata_collator, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 38\u001b[0m embeddings, labels \u001b[39m=\u001b[39m get_embeddings_labels(model, loader)\n\u001b[0;32m     40\u001b[0m \u001b[39mprint\u001b[39m(embeddings\u001b[39m.\u001b[39mshape, labels\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     42\u001b[0m torch\u001b[39m.\u001b[39msave(embeddings, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdistilbert_embeddings.pt\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","Cell \u001b[1;32mIn[47], line 24\u001b[0m, in \u001b[0;36mget_embeddings_labels\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     20\u001b[0m     labels\u001b[39m.\u001b[39mappend(batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m))\n\u001b[0;32m     22\u001b[0m     batch \u001b[39m=\u001b[39m {key: batch[key]\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[1;32m---> 24\u001b[0m     embeddings \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)[\u001b[39m'\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m'\u001b[39m][:, \u001b[39m0\u001b[39m, :]\n\u001b[0;32m     26\u001b[0m     total_embeddings\u001b[39m.\u001b[39mappend(embeddings\u001b[39m.\u001b[39mcpu())\n\u001b[0;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(total_embeddings, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mcat(labels, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:581\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[0;32m    584\u001b[0m     x\u001b[39m=\u001b[39membeddings,\n\u001b[0;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    590\u001b[0m )\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:120\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[1;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39m    input_ids (torch.Tensor):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39membeddings)\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     input_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    122\u001b[0m seq_length \u001b[39m=\u001b[39m input_embeds\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[39m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39m# when tracing the model without passing position-ids, solves\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[39m# isues similar to issue #5664\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n","File \u001b[1;32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n","\u001b[1;31mIndexError\u001b[0m: index out of range in self"]}],"source":["import torch\n","from torch.utils.data import DataLoader, Subset\n","from tqdm import tqdm\n","from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n","\n","from transformers import DistilBertTokenizer\n","\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","\n","\n","\n","@torch.inference_mode()\n","def get_embeddings_labels(model, loader):\n","    model.eval()\n","    \n","    total_embeddings = []\n","    labels = []\n","    \n","    for batch in tqdm(loader):\n","        labels.append(batch['labels'].unsqueeze(1))\n","\n","        batch = {key: batch[key].to(device) for key in ['attention_mask', 'input_ids']}\n","\n","        embeddings = model(**batch)['last_hidden_state'][:, 0, :]\n","\n","        total_embeddings.append(embeddings.cpu())\n","\n","    return torch.cat(total_embeddings, dim=0), torch.cat(labels, dim=0).to(torch.float32)\n","\n","# ['bert', 'roberta', 'distilbert']\n","device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n","tokenizer, model = get_model('distilbert')\n","model = model.to(device)\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","loader = DataLoader(Subset(dataset, idx), batch_size=50, collate_fn=data_collator, pin_memory=True, shuffle=False)\n","embeddings, labels = get_embeddings_labels(model, loader)\n","\n","print(embeddings.shape, labels.shape)\n","\n","torch.save(embeddings, f'distilbert_embeddings.pt')\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Max input id: 49069\n","Vocab size: 28996\n"]}],"source":["max_input_id = max([batch['input_ids'].max().item() for batch in loader])\n","vocab_size = len(tokenizer)\n","\n","print(f\"Max input id: {max_input_id}\")\n","print(f\"Vocab size: {vocab_size}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:09:52.046386Z","iopub.status.busy":"2023-07-13T16:09:52.045549Z","iopub.status.idle":"2023-07-13T16:09:54.978405Z","shell.execute_reply":"2023-07-13T16:09:54.975878Z","shell.execute_reply.started":"2023-07-13T16:09:52.046348Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"notebookId":"c44e6467-27e8-4a10-8e04-098c2a59e4c5","notebookPath":"8/Lecture_10_Intro_to_DL_Bert.ipynb"},"nbformat":4,"nbformat_minor":4}
