{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Data loaded and merged successfully.\n",
      "Timestamps processed\n",
      "Categorical features encoded\n",
      "Additional features created\n",
      "Text features processed\n",
      "TF-IDF processed\n",
      "Data processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from catboost import Pool, CatBoostClassifier, CatBoost\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "import os\n",
    "\n",
    "'''\n",
    "ФУНКЦИИ ПО ЗАГРУЗКЕ МОДЕЛЕЙ\n",
    "'''\n",
    "# Проверка если код выполняется в лмс, или локально\n",
    "def get_model_path(path: str) -> str:\n",
    "    \"\"\"Просьба не менять этот код\"\"\"\n",
    "    if os.environ.get(\"IS_LMS\") == \"1\":  # проверяем где выполняется код в лмс, или локально. Немного магии\n",
    "        MODEL_PATH = '/workdir/user_input/model'\n",
    "    else:\n",
    "        MODEL_PATH = path\n",
    "    return MODEL_PATH\n",
    "\n",
    "# Загрузка модели\n",
    "def load_models(model_path):\n",
    "    model = CatBoost()\n",
    "    model.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "ФУНКЦИИ ПО ПОДГОТОВКЕ ДАННЫХ\n",
    "'''\n",
    "# Загрузка данных из базы данных\n",
    "def load_and_merge_data(engine, feed_data_size):\n",
    "    # Чтение данных таблицы user_data\n",
    "    query = \"SELECT * FROM user_data\"\n",
    "    user_data = pd.read_sql(query, engine)\n",
    "\n",
    "    # Чтение данных таблицы post_text_df\n",
    "    query = \"SELECT * FROM post_text_df\"\n",
    "    post_text_df = pd.read_sql(query, engine)\n",
    "\n",
    "    # Чтение ограниченного количества данных таблицы feed_data\n",
    "    query = f\"SELECT * FROM feed_data LIMIT {feed_data_size}\"\n",
    "    feed_data = pd.read_sql(query, engine)\n",
    "\n",
    "    # Переименование столбцов идентификаторов\n",
    "    user_data = user_data.rename(columns={'id': 'user_id'})\n",
    "    post_text_df = post_text_df.rename(columns={'id': 'post_id'})\n",
    "\n",
    "    # Объединение таблиц\n",
    "    data = feed_data.merge(user_data, on='user_id', how='left')\n",
    "    data = data.merge(post_text_df, on='post_id', how='left')\n",
    "\n",
    "    return data\n",
    "\n",
    "# Обработка временных меток\n",
    "def process_timestamps(data):\n",
    "    # Преобразование формата временных меток в объект datetime\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "    # Извлечение признаков из временных меток\n",
    "    data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "    data['hour_of_day'] = data['timestamp'].dt.hour\n",
    "\n",
    "    # Расчет времени с момента последнего действия для каждого пользователя\n",
    "    data = data.sort_values(['user_id', 'timestamp'])\n",
    "    data['time_since_last_action'] = data.groupby('user_id')['timestamp'].diff().dt.total_seconds()\n",
    "    data['time_since_last_action'].fillna(0, inplace=True)\n",
    "\n",
    "    # Удаление столбца временных меток\n",
    "    data = data.drop('timestamp', axis=1)\n",
    "    \n",
    "    print('Timestamps processed')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Кодирование категориальных признаков\n",
    "def encode_categorical_features(data):\n",
    "    # One-hot encoding для 'country', 'city' и 'topic'\n",
    "    data = pd.get_dummies(data, columns=['country', 'city', 'topic'], prefix=['country', 'city', 'topic'])\n",
    "\n",
    "    le_gender = LabelEncoder()\n",
    "    le_os = LabelEncoder()\n",
    "    le_source = LabelEncoder()\n",
    "    le_action = LabelEncoder()\n",
    "\n",
    "    # Label encoding для 'gender', 'os' и 'source'\n",
    "    data['gender'] = le_gender.fit_transform(data['gender'])\n",
    "    data['os'] = le_os.fit_transform(data['os'])\n",
    "    data['source'] = le_source.fit_transform(data['source'])\n",
    "    data['action'] = le_action.fit_transform(data['action'])\n",
    "\n",
    "    print('Categorical features encoded')\n",
    "    return data\n",
    "\n",
    "# Создание дополнительных признаков\n",
    "def create_additional_features(data):\n",
    "    # Feature 1: Количество просмотров и лайков для каждого пользователя\n",
    "    user_views_likes = data.groupby('user_id')['action'].value_counts().unstack().fillna(0)\n",
    "    user_views_likes.columns = ['user_views', 'user_likes']\n",
    "    data = data.merge(user_views_likes, on='user_id', how='left')\n",
    "\n",
    "    # Feature 2: Количество просмотров и лайков для каждого поста\n",
    "    post_views_likes = data.groupby('post_id')['action'].value_counts().unstack().fillna(0)\n",
    "    post_views_likes.columns = ['post_views', 'post_likes']\n",
    "    data = data.merge(post_views_likes, on='post_id', how='left')\n",
    "\n",
    "    # Feature 3: Количество просмотров и лайков для каждой группы тематик\n",
    "    temp_df = data[['exp_group', 'topic_business', 'topic_covid', 'topic_entertainment', 'topic_movie', 'topic_politics', 'topic_sport', 'topic_tech', 'action']]\n",
    "    for col in ['topic_business', 'topic_covid', 'topic_entertainment', 'topic_movie', 'topic_politics', 'topic_sport', 'topic_tech']:\n",
    "        temp_df.loc[:, col] = temp_df[col] * temp_df['action']\n",
    "    grouped_data = temp_df.groupby('exp_group').sum().reset_index()\n",
    "    grouped_data.columns = ['exp_group'] + [f'{col}_exp_group_views' if i % 2 == 0 else f'{col}_exp_group_likes' for i, col in enumerate(grouped_data.columns[1:], 1)]\n",
    "    data = data.merge(grouped_data, on='exp_group', how='left')\n",
    "\n",
    "    print('Additional features created')\n",
    "    return data\n",
    "\n",
    "# Обработка текстовых признаков\n",
    "def process_text_features(data):\n",
    "    def word_count(X):\n",
    "        return np.array([len(re.findall(r'\\b\\w+\\b', text)) for text in X])\n",
    "    def sentence_count(X):\n",
    "        return np.array([len(re.findall(r'[.!?]+', text)) for text in X])\n",
    "    def avg_word_length(X):\n",
    "        return np.array([sum(len(word) for word in re.findall(r'\\b\\w+\\b', text)) / len(re.findall(r'\\b\\w+\\b', text)) if len(re.findall(r'\\b\\w+\\b', text)) > 0 else 0 for text in X])\n",
    "    def punctuation_count(X):\n",
    "        return np.array([sum(1 for char in text if char in punctuation) for text in X])\n",
    "\n",
    "    # Применение функций извлечения признаков к столбцу 'text'\n",
    "    word_counts = word_count(data['text'])\n",
    "    sentence_counts = sentence_count(data['text'])\n",
    "    avg_word_lengths = avg_word_length(data['text'])\n",
    "    punctuation_counts = punctuation_count(data['text'])\n",
    "\n",
    "    # Добавление новых признаков в виде столбцов в DataFrame\n",
    "    data['word_count'] = word_counts\n",
    "    data['sentence_count'] = sentence_counts\n",
    "    data['avg_word_length'] = avg_word_lengths\n",
    "    data['punctuation_count'] = punctuation_counts\n",
    "\n",
    "    print('Text features processed')\n",
    "    return data\n",
    "\n",
    "# Обработка TF-IDF\n",
    "def process_tfidf(data):\n",
    "    # Инициализация TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=1000) # Вы можете настроить max_features в зависимости от ваших потребностей\n",
    "    \n",
    "    # Обучение vectorizer на столбце 'text' и преобразование текстовых данных\n",
    "    tfidf_matrix = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "    # Преобразование матрицы TF-IDF в DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Конкатенация исходных данных с DataFrame TF-IDF\n",
    "    data_with_tfidf = pd.concat([data.drop(columns=['text']), tfidf_df], axis=1)\n",
    "\n",
    "    print('TF-IDF processed')\n",
    "    return data_with_tfidf\n",
    "\n",
    "\n",
    "'''\n",
    "ФУНККЦИИ ДЛЯ ОБУЧЕНИЯ МОДЕЛИ \n",
    "'''\n",
    "# Отбор признаков на основе Mutual Information\n",
    "''' \n",
    "# Отбор признаков на основе взаимной информации\n",
    "def select_features_mi(data, target_col, k=50):\n",
    "    X = data.drop([target_col], axis=1)\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Вычисление взаимной информации между каждым признаком и целевой переменной\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "    # Создание DataFrame с именами признаков и соответствующими им оценками MI\n",
    "    mi_scores_df = pd.DataFrame({'feature': X.columns, 'mi_score': mi_scores})\n",
    "\n",
    "    # Сортировка DataFrame по оценкам MI в порядке убывания\n",
    "    mi_scores_df = mi_scores_df.sort_values('mi_score', ascending=False)\n",
    "\n",
    "    # При необходимости выберите k лучших признаков с помощью SelectKBest\n",
    "    selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    selector.fit(X, y)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "    print(\"Top k features based on mutual information:\")\n",
    "    print(selected_features)\n",
    "\n",
    "    return selected_features\n",
    "'''\n",
    "\n",
    "# Функция тренировки модели CatBoost\n",
    "'''\n",
    "# Обучение модели CatBoost\n",
    "def train_catboost_model(X_train, X_test, y_train, y_test, group_id_col):\n",
    "    # Сортируем наборы данных для обучения и тестирования по 'group_id'\n",
    "    X_train = X_train.sort_values(by=group_id_col)\n",
    "    y_train = y_train.loc[X_train.index]\n",
    "    \n",
    "    X_test = X_test.sort_values(by=group_id_col)\n",
    "    y_test = y_test.loc[X_test.index]\n",
    "\n",
    "    # Создание объектов Pool для обучения и тестирования с указанием столбца 'group_id'\n",
    "    train_pool = Pool(X_train.drop(columns=[group_id_col]), y_train, group_id=X_train[group_id_col])\n",
    "    test_pool = Pool(X_test.drop(columns=[group_id_col]), y_test, group_id=X_test[group_id_col])\n",
    "\n",
    "    # Обучение модели CatBoost с использованием метрики оценки PrecisionAt:top=5\n",
    "    model = CatBoostClassifier(iterations=1000,\n",
    "                            learning_rate=0.1,\n",
    "                            depth=6,\n",
    "                            custom_metric='PrecisionAt:top=5',\n",
    "                            eval_metric='PrecisionAt:top=5',\n",
    "                            random_seed=42,\n",
    "                            verbose=100)\n",
    "\n",
    "    model.fit(train_pool, eval_set=test_pool)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Сохранение и загрузка модели CatBoost\n",
    "def save_and_load_catboost_model(model, model_path):\n",
    "    model.save_model(model_path)\n",
    "    loaded_model = CatBoostClassifier()\n",
    "    loaded_model.load_model(model_path)\n",
    "\n",
    "    return loaded_model\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "ОСНОВНАЯ ЧАСТЬ ПРОГРАММЫ\n",
    "'''\n",
    "\n",
    "# Создали функцию для обработки данных\n",
    "def process_inference_data(data, preselected_features):\n",
    "    '''\n",
    "    На выходе мы получим обработанные данные, которые можно будет передать в модель для получения прогнозов.\n",
    "    '''\n",
    "    \n",
    "    # Обработка данных\n",
    "    data = process_timestamps(data)\n",
    "    data = encode_categorical_features(data)\n",
    "    data = create_additional_features(data)\n",
    "    data = process_text_features(data)\n",
    "    data = process_tfidf(data)\n",
    "        \n",
    "    # Добавление отсутствующих колонок и заполнение их нулями\n",
    "    for col in preselected_features:\n",
    "        if col not in data.columns:\n",
    "            data[col] = 0\n",
    "\n",
    "    # Оставить только предварительно отобранные признаки\n",
    "    data = data[preselected_features]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "'''\n",
    "ВАЖНЫЕ ПЕРЕМЕННЫЕ\n",
    "'''\n",
    "# Для работы с БД\n",
    "engine = create_engine(\n",
    "        \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    "    )\n",
    "\n",
    "# Отобранные признаки для модели\n",
    "preselected_features = ['user_id', 'gender', 'exp_group', 'os', 'day_of_week', 'hour_of_day',\n",
    "    'time_since_last_action', 'country_Russia', 'city_Kansk', 'city_Kazan',\n",
    "    'city_Mangghystaū', 'city_Michurinsk', 'city_Moscow', 'city_Pavlovo',\n",
    "    'city_Samara', 'topic_covid', 'topic_entertainment', 'topic_movie',\n",
    "    'user_views', 'user_likes', 'post_views',\n",
    "    'topic_business_exp_group_likes', 'topic_covid_exp_group_views',\n",
    "    'topic_entertainment_exp_group_likes', 'topic_movie_exp_group_views',\n",
    "    'topic_politics_exp_group_likes', 'topic_sport_exp_group_views',\n",
    "    'topic_tech_exp_group_likes', 'action_exp_group_views',\n",
    "    'sentence_count', 'punctuation_count', 'australian', 'based', 'city',\n",
    "    'ending', 'failed', 'given', 'idea', 'life', 'likely', 'opportunity',\n",
    "    'order', 'paid', 'party', 'past', 'political', 'robert', 'scene',\n",
    "    'seem', 'wanted']\n",
    "\n",
    "# Сколько рядов данных загружать за один раз\n",
    "feed_data_size = 1000\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "ГЛАВНАЯ ФУНКЦИЯ\n",
    "'''\n",
    "# Загрузка и обработка данных для инференса -> Обработка и сохранение результатов предсказаний\n",
    "def main():\n",
    "    # Загрузка обученной модели\n",
    "    model_path = \"models/catboost_MAP_model.cbm\"\n",
    "    model = load_models(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    # Загружаем данные\n",
    "    data = load_and_merge_data(engine, feed_data_size)\n",
    "    print(\"Data loaded and merged successfully.\")\n",
    "    \n",
    "    # Обработка данных\n",
    "    data = process_inference_data(data, preselected_features)\n",
    "    print(\"Data processed successfully.\")\n",
    "    \n",
    "    # Выполнение инференса\n",
    "    # predictions = catboost_inference(model, inference_data, 'group_id')\n",
    "\n",
    "    # Обработка и сохранение результатов предсказаний\n",
    "    # ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
