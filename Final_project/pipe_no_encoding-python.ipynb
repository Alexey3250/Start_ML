{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import catboost\n",
    "from catboost import Pool, CatBoostClassifier, CatBoost\n",
    "import numpy as np\n",
    "import re\n",
    "from string import punctuation\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ВАЖНЫЕ ПЕРЕМЕННЫЕ\n",
    "'''\n",
    "# Для работы с БД\n",
    "engine = create_engine(\n",
    "        \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    "    )\n",
    "\n",
    "# Сколько рядов данных загружать за один раз\n",
    "feed_data_size = 1000\n",
    "\n",
    "data = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ФУНКЦИИ ПО ЗАГРУЗКЕ МОДЕЛЕЙ\n",
    "'''\n",
    "# Проверка если код выполняется в лмс, или локально\n",
    "def get_model_path(path: str) -> str:\n",
    "    \"\"\"Просьба не менять этот код\"\"\"\n",
    "    if os.environ.get(\"IS_LMS\") == \"1\":  # проверяем где выполняется код в лмс, или локально. Немного магии\n",
    "        MODEL_PATH = '/workdir/user_input/model'\n",
    "    else:\n",
    "        MODEL_PATH = path\n",
    "    return MODEL_PATH\n",
    "\n",
    "# Загрузка модели\n",
    "def load_models(model_path):\n",
    "    model = CatBoost()\n",
    "    model.load_model(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ФУНКЦИИ ПО ПОДГОТОВКЕ ДАННЫХ\n",
    "'''\n",
    "# Загрузка данных из базы данных\n",
    "def load_and_merge_data(engine, feed_data_size):\n",
    "    # Чтение данных таблицы user_data\n",
    "    query = \"SELECT * FROM user_data\"\n",
    "    user_data = pd.read_sql(query, engine)\n",
    "\n",
    "    # Чтение данных таблицы post_text_df\n",
    "    query = \"SELECT * FROM post_text_df\"\n",
    "    post_text_df = pd.read_sql(query, engine)\n",
    "\n",
    "    # Чтение ограниченного количества данных таблицы feed_data\n",
    "    query = f\"SELECT * FROM feed_data LIMIT {feed_data_size}\"\n",
    "    feed_data = pd.read_sql(query, engine)\n",
    "\n",
    "    # Переименование столбцов идентификаторов\n",
    "    user_data = user_data.rename(columns={'id': 'user_id'})\n",
    "    post_text_df = post_text_df.rename(columns={'id': 'post_id'})\n",
    "\n",
    "    # Объединение таблиц\n",
    "    data = feed_data.merge(user_data, on='user_id', how='left')\n",
    "    data = data.merge(post_text_df, on='post_id', how='left')\n",
    "    \n",
    "    print(f\"Data shape after load_and_merge_data: {data.shape}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Обработка временных меток\n",
    "def process_timestamps(data):\n",
    "    # Преобразование формата временных меток в объект datetime\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "    # Извлечение признаков из временных меток\n",
    "    data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "    data['hour_of_day'] = data['timestamp'].dt.hour\n",
    "\n",
    "    # Расчет времени с момента последнего действия для каждого пользователя\n",
    "    data = data.sort_values(['user_id', 'timestamp'])\n",
    "    data['time_since_last_action'] = data.groupby('user_id')['timestamp'].diff().dt.total_seconds()\n",
    "    data['time_since_last_action'].fillna(0, inplace=True)\n",
    "\n",
    "    # Удаление столбца временных меток\n",
    "    data = data.drop('timestamp', axis=1)\n",
    "    \n",
    "    print('Timestamps processed')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Кодирование категориальных признаков\n",
    "def encode_categorical_features(data):\n",
    "    # One-hot encoding для 'country', 'city' и 'topic'\n",
    "    data = pd.get_dummies(data, columns=['country', 'city', 'topic'], prefix=['country', 'city', 'topic'])\n",
    "\n",
    "    le_gender = LabelEncoder()\n",
    "    le_os = LabelEncoder()\n",
    "    le_source = LabelEncoder()\n",
    "    le_action = LabelEncoder()\n",
    "\n",
    "    # Label encoding для 'gender', 'os' и 'source'\n",
    "    data['gender'] = le_gender.fit_transform(data['gender'])\n",
    "    data['os'] = le_os.fit_transform(data['os'])\n",
    "    data['source'] = le_source.fit_transform(data['source'])\n",
    "    data['action'] = le_action.fit_transform(data['action'])\n",
    "\n",
    "    print('Categorical features encoded')\n",
    "    return data\n",
    "\n",
    "# Создание дополнительных признаков\n",
    "def create_additional_features(data):\n",
    "    # Feature 1: Количество просмотров и лайков для каждого пользователя\n",
    "    user_views_likes = data.groupby('user_id')['action'].value_counts().unstack().fillna(0)\n",
    "    user_views_likes.columns = ['user_views', 'user_likes']\n",
    "    data = data.merge(user_views_likes, on='user_id', how='left')\n",
    "\n",
    "    # Feature 2: Количество просмотров и лайков для каждого поста\n",
    "    post_views_likes = data.groupby('post_id')['action'].value_counts().unstack().fillna(0)\n",
    "    post_views_likes.columns = ['post_views', 'post_likes']\n",
    "    data = data.merge(post_views_likes, on='post_id', how='left')\n",
    "\n",
    "    # Feature 3: Количество просмотров и лайков для каждой группы тематик\n",
    "    temp_df = data[['exp_group', 'topic_business', 'topic_covid', 'topic_entertainment', 'topic_movie', 'topic_politics', 'topic_sport', 'topic_tech', 'action']]\n",
    "    for col in ['topic_business', 'topic_covid', 'topic_entertainment', 'topic_movie', 'topic_politics', 'topic_sport', 'topic_tech']:\n",
    "        temp_df.loc[:, col] = temp_df[col] * temp_df['action']\n",
    "    grouped_data = temp_df.groupby('exp_group').sum().reset_index()\n",
    "    grouped_data.columns = ['exp_group'] + [f'{col}_exp_group_views' if i % 2 == 0 else f'{col}_exp_group_likes' for i, col in enumerate(grouped_data.columns[1:], 1)]\n",
    "    data = data.merge(grouped_data, on='exp_group', how='left')\n",
    "\n",
    "    print('Additional features created')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для инференса\n",
    "def prepare_data_for_prediction(data):\n",
    "    # Убираем ненужные столбцы\n",
    "    X = data.drop(['target', 'action', 'text'], axis=1)\n",
    "\n",
    "    # Замените названия столбцов на соответствующие вашим данным\n",
    "    categorical_columns = ['country', 'topic', 'city', 'gender', 'os', 'source']\n",
    "\n",
    "    # Создание ID группы на основе столбца 'user_id'\n",
    "    unique_user_ids = X['user_id'].unique()\n",
    "    group_id_dict = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "    X['group_id'] = X['user_id'].map(group_id_dict)\n",
    "\n",
    "    # Сортировка набора данных для предсказаний по 'group_id'\n",
    "    X = X.sort_values(by='group_id')\n",
    "\n",
    "    # Убедитесь, что категориальные переменные представлены в виде строк\n",
    "    X[categorical_columns] = X[categorical_columns].astype(str)\n",
    "\n",
    "    # Получение индексов категориальных столбцов\n",
    "    cat_features = [X.drop(columns=['user_id']).columns.get_loc(col) for col in categorical_columns]\n",
    "\n",
    "    # Создание объекта Pool для набора данных предсказаний с колонкой 'group_id' и категориальными признаками\n",
    "    prediction_pool = Pool(X.drop(columns=['user_id']), cat_features=cat_features, group_id=X['group_id'])\n",
    "\n",
    "    return prediction_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Data shape after load_and_merge_data: (1000, 14)\n",
      "Data loaded and merged successfully.\n",
      "Timestamps processed\n",
      "Categorical features encoded\n",
      "Additional features created\n",
      "Data processed successfully.\n",
      "Data shape after process_inference_data: (1000, 40)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ГЛАВНАЯ ФУНКЦИЯ\n",
    "'''\n",
    "# Загрузка и обработка данных для инференса -> Обработка и сохранение результатов предсказаний\n",
    "def main():\n",
    "    # Загрузка обученной модели\n",
    "    model_path = \"models/catboost_precision_model.cbm\"\n",
    "    model = load_models(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    # Загружаем данные\n",
    "    data = load_and_merge_data(engine, feed_data_size)\n",
    "    print(\"Data loaded and merged successfully.\")\n",
    "    \n",
    "    # Обработка данных\n",
    "    data = process_timestamps(data)\n",
    "    data = encode_categorical_features(data)\n",
    "    data = create_additional_features(data)\n",
    "    print(\"Data processed successfully.\")\n",
    "    print(f\"Data shape after process_inference_data: {data.shape}\")\n",
    "    \n",
    "    # Shape data for prediction\n",
    "    #prediction_pool = prepare_data_for_prediction(data)\n",
    "    #print(\"Data shaped for prediction successfully.\")\n",
    "    \n",
    "    # Предсказание\n",
    "    #predictions = model.predict(prediction_pool)\n",
    "    \n",
    "    # Сохранение результатов предсказаний\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
