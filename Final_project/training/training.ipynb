{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed data shape: (15378483, 51)\n"
     ]
    }
   ],
   "source": [
    "# Чтение feed_data из csv файла\n",
    "feed_data = pd.read_csv(r'C:\\Users\\Alex\\Desktop\\data_for_training.csv')\n",
    "print(f\"Feed data shape: {feed_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename post_id_x to post_id \n",
    "feed_data = feed_data.rename(columns={'post_id_x': 'post_id'})\n",
    "\n",
    "# drop post_id_y\n",
    "feed_data = feed_data.drop('post_id_y', axis=1)\n",
    "\n",
    "# drop action\n",
    "feed_data = feed_data.drop('action', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['user_views', 'user_likes',\n",
    "                     'component_2', '4_exp_group_views', '0_exp_group_likes', '6_exp_group_likes', \n",
    "                     '1_exp_group_views', '2_exp_group_views', 'exp_group', '0_exp_group_views', \n",
    "                     'component_4', '5_exp_group_views', '6_exp_group_views', 'component_10', \n",
    "                     'component_5', 'avg_word_length', 'component_1', 'component_3', 'component_6', \n",
    "                     'component_7', 'component_8', 'word_count', 'component_9', 'day_of_week', \n",
    "                     'sentence_count', 'punctuation_count', 'day_of_month', 'os', \n",
    "                     'source', 'year','1_exp_group_likes', '2_exp_group_likes',\n",
    "                     '3_exp_group_likes', '3_exp_group_views', '4_exp_group_likes',\n",
    "                     '5_exp_group_likes' , 'time_since_last_action']\n",
    "\n",
    "feed_data = feed_data.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_10_percent(group):\n",
    "    frac = 0.2\n",
    "    return group.sample(frac=frac)\n",
    "\n",
    "feed_data = feed_data.groupby('user_id', group_keys=False).apply(sample_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'post_id', 'target', 'gender', 'age', 'country', 'city',\n",
       "       'topic', 'month', 'hour_of_day', 'post_views', 'post_likes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features  = ['gender', 'age', 'country', 'city', 'exp_group', 'os', 'source', 'topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Get a list of unique users\n",
    "unique_users = feed_data['user_id'].unique()\n",
    "\n",
    "# Split unique users into training and testing sets\n",
    "train_users, test_users = train_test_split(unique_users, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = feed_data[feed_data['user_id'].isin(train_users)]\n",
    "X_train = X_train.drop('target', axis=1)\n",
    "\n",
    "X_test = feed_data[feed_data['user_id'].isin(test_users)]\n",
    "X_test = X_test.drop('target', axis=1)\n",
    "\n",
    "y_train = feed_data[feed_data['user_id'].isin(train_users)]['target']\n",
    "y_test = feed_data[feed_data['user_id'].isin(test_users)]['target']\n",
    "\n",
    "# Create a group ID based on the 'user_id' column in the training set\n",
    "group_id_dict_train = {user_id: idx for idx, user_id in enumerate(train_users)}\n",
    "X_train['group_id'] = X_train['user_id'].map(group_id_dict_train)\n",
    "\n",
    "# Create a group ID based on the 'user_id' column in the test set\n",
    "group_id_dict_test = {user_id: idx for idx, user_id in enumerate(test_users)}\n",
    "X_test['group_id'] = X_test['user_id'].map(group_id_dict_test)\n",
    "\n",
    "# Sort the train and test sets by 'group_id'\n",
    "X_train = X_train.sort_values(by='group_id')\n",
    "y_train = y_train.loc[X_train.index]\n",
    "\n",
    "X_test = X_test.sort_values(by='group_id')\n",
    "y_test = y_test.loc[X_test.index]\n",
    "\n",
    "# Create train and test Pool objects with the 'group_id' column\n",
    "train_pool = Pool(X_train.drop(columns=['user_id']), y_train, group_id=X_train['group_id'])\n",
    "test_pool = Pool(X_test.drop(columns=['user_id']), y_test, group_id=X_test['group_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6842021\ttest: 0.6842544\tbest: 0.6842544 (0)\ttotal: 576ms\tremaining: 9m 35s\n",
      "100:\tlearn: 0.3627769\ttest: 0.3627011\tbest: 0.3627011 (100)\ttotal: 41.9s\tremaining: 6m 12s\n",
      "200:\tlearn: 0.3303082\ttest: 0.3302401\tbest: 0.3302401 (200)\ttotal: 1m 22s\tremaining: 5m 27s\n",
      "300:\tlearn: 0.3251785\ttest: 0.3251125\tbest: 0.3251125 (300)\ttotal: 2m 3s\tremaining: 4m 47s\n",
      "400:\tlearn: 0.3236567\ttest: 0.3235858\tbest: 0.3235858 (400)\ttotal: 2m 44s\tremaining: 4m 5s\n",
      "500:\tlearn: 0.3229098\ttest: 0.3228395\tbest: 0.3228395 (500)\ttotal: 3m 24s\tremaining: 3m 23s\n",
      "600:\tlearn: 0.3224383\ttest: 0.3223708\tbest: 0.3223708 (600)\ttotal: 4m 4s\tremaining: 2m 42s\n",
      "700:\tlearn: 0.3220757\ttest: 0.3220118\tbest: 0.3220118 (700)\ttotal: 4m 44s\tremaining: 2m 1s\n",
      "800:\tlearn: 0.3217645\ttest: 0.3217019\tbest: 0.3217019 (800)\ttotal: 5m 24s\tremaining: 1m 20s\n",
      "900:\tlearn: 0.3215188\ttest: 0.3214573\tbest: 0.3214573 (900)\ttotal: 6m 4s\tremaining: 40.1s\n",
      "999:\tlearn: 0.3212975\ttest: 0.3212390\tbest: 0.3212390 (999)\ttotal: 6m 44s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.3212389948\n",
      "bestIteration = 999\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x16d7f367c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the training parameters \n",
    "params = {'loss_function': 'Logloss', # suitable for binary classification\n",
    "          'custom_metric': ['AUC', 'NDCG'], # evaluation metrics\n",
    "          'thread_count': 16,\n",
    "          'verbose': 100,\n",
    "          'random_seed': 42,\n",
    "          'iterations': 1000,\n",
    "          'learning_rate': 0.01,\n",
    "          }\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(**params)\n",
    "\n",
    "# Fit model\n",
    "model.fit(train_pool, eval_set=test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 37.73608477896087\n",
      "post_views: 17.37096487032175\n",
      "post_likes: 14.827768820765382\n",
      "city: 10.76735901325584\n",
      "month: 6.931971983839355\n",
      "country: 6.195213257891395\n",
      "hour_of_day: 3.077188188443453\n",
      "gender: 1.3535355344578608\n",
      "post_id: 0.9229312087558498\n",
      "topic: 0.81206534004215\n",
      "group_id: 0.0049170032660802426\n"
     ]
    }
   ],
   "source": [
    "feature_importances = model.get_feature_importance(train_pool)\n",
    "feature_names = X_train.drop(columns=['user_id']).columns\n",
    "\n",
    "for score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n",
    "    print(f'{name}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(test_pool)[:, 1]\n",
    "# Add the prediction probabilities to the test dataset\n",
    "X_test['pred_proba'] = y_pred_proba\n",
    "\n",
    "# Group by 'user_id' and find the top 5 predicted 'post_id' for each user\n",
    "top_5_posts = X_test.groupby('user_id').apply(lambda x: x.nlargest(5, 'pred_proba')['post_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id          \n",
       "200      39          6697\n",
       "         37          6404\n",
       "         25          6934\n",
       "         30          6661\n",
       "         23          5989\n",
       "                     ... \n",
       "168552   15378472    3130\n",
       "         15378473    5400\n",
       "         15378474    6024\n",
       "         15378466    6323\n",
       "         15378471      49\n",
       "Name: post_id, Length: 408015, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_posts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HitRate@5 Evaluation Metric for Recommender System\n",
    "\n",
    "This Python function calculates the HitRate@5 for a recommender system. HitRate@5 is a commonly used metric for evaluating the performance of a recommender system. It specifically measures the proportion of instances where at least one of the top 5 recommended items was liked by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HitRate@5: 0.6300993836010931\n"
     ]
    }
   ],
   "source": [
    "def hitrate_at_k(X_test, y_test, top_k_posts, k=5):\n",
    "    hits = 0\n",
    "    total_checks = 0\n",
    "\n",
    "    # Get unique user_ids from the test set\n",
    "    user_ids = X_test['user_id'].unique()\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        # Get the list of top 5 predicted post_ids for this user\n",
    "        top_k_pred = list(top_k_posts.loc[user_id])\n",
    "\n",
    "        # Get the actual liked post_ids for this user\n",
    "        true_post_ids = list(X_test[(X_test['user_id'] == user_id) & (y_test == 1)]['post_id'])\n",
    "\n",
    "        # Increase the count of total checks\n",
    "        total_checks += 1\n",
    "\n",
    "        # If at least one of the top 5 recommended posts is liked by the user\n",
    "        if set(top_k_pred) & set(true_post_ids):\n",
    "            hits += 1\n",
    "\n",
    "    # Calculate the hit rate as the proportion of checks where at least one of the top 5 recommended posts was liked\n",
    "    hitrate_at_k = hits / total_checks if total_checks > 0 else 0\n",
    "\n",
    "    return hitrate_at_k\n",
    "\n",
    "# Use the function to calculate the HitRate@5\n",
    "hitrate_at_5 = hitrate_at_k(X_test, y_test, top_5_posts, k=5)\n",
    "print(\"HitRate@5:\", hitrate_at_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file\n",
    "model.save_model(\"catboost_model_1.cbm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = feed_data.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data.to_csv(r'C:\\Users\\Alex\\Desktop\\data_for_inference2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to a-efimik_features_lesson_22_4: 100%|██████████| 31/31 [26:31<00:00, 51.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "def upload_dataframe_in_chunks(data, table_name, engine, chunksize=10000):\n",
    "    total_chunks = math.ceil(len(data) / chunksize)\n",
    "    for i in tqdm(range(total_chunks), desc=f\"Uploading to {table_name}\"):\n",
    "        chunk = data[i * chunksize : (i + 1) * chunksize]\n",
    "        if_exists = \"replace\" if i == 0 else \"append\"\n",
    "        chunk.to_sql(table_name, con=engine, if_exists=if_exists, index=False, method=\"multi\")\n",
    "\n",
    "\n",
    "engine = create_engine(\n",
    "    \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "    \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "chunksize = 100000\n",
    "upload_dataframe_in_chunks(inference_data, \"a-efimik_features_lesson_22_4\", engine, chunksize=chunksize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
