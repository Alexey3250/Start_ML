{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут мы реализуем сервис записи фичей в новую таблицу для того чтобы не загружать основной алгоритм обработкой данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка фичей из базы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "def load_and_merge_data(engine, chunksize=200000):\n",
    "    # Чтение данных таблицы user_data\n",
    "    query = \"SELECT * FROM user_data\"\n",
    "    user_data = pd.read_sql(query, engine)\n",
    "    print(f\"User data shape: {user_data.shape}\")\n",
    "\n",
    "    # Чтение данных таблицы post_text_df\n",
    "    query = \"SELECT * FROM post_text_df\"\n",
    "    post_text_df = pd.read_sql(query, engine)\n",
    "    print(f\"Post text data shape: {post_text_df.shape}\")\n",
    "\n",
    "    # Чтение ограниченного количества данных таблицы feed_data\n",
    "    query = f\"SELECT * FROM feed_data\"\n",
    "    feed_data = batch_load_sql_timed(engine, query, chunksize)\n",
    "    print(f\"Feed data shape: {feed_data.shape}\")\n",
    "\n",
    "    # Переименование столбцов идентификаторов\n",
    "    user_data = user_data.rename(columns={'id': 'user_id'})\n",
    "    post_text_df = post_text_df.rename(columns={'id': 'post_id'})\n",
    "\n",
    "    # Объединение таблиц\n",
    "    data = feed_data.merge(user_data, on='user_id', how='left')\n",
    "    data = data.merge(post_text_df, on='post_id', how='left')\n",
    "\n",
    "    print(f\"Data shape after load_and_merge_data: {data.shape}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batch_load_sql(engine, query: str, chunksize: int) -> pd.DataFrame:\n",
    "    conn = engine.connect().execution_options(stream_results=True)\n",
    "    chunks = []\n",
    "    for chunk_dataframe in pd.read_sql(query, conn, chunksize=chunksize):\n",
    "        chunks.append(chunk_dataframe)\n",
    "    conn.close()\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "import time\n",
    "\n",
    "def batch_load_sql_timed(engine, query: str, chunksize: int) -> pd.DataFrame:\n",
    "    conn = engine.connect().execution_options(stream_results=True)\n",
    "    chunks = []\n",
    "    row_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for chunk_dataframe in pd.read_sql(query, conn, chunksize=chunksize):\n",
    "        chunks.append(chunk_dataframe)\n",
    "        row_count += len(chunk_dataframe)\n",
    "        print(f\"Loaded {row_count} rows, elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    conn.close()\n",
    "    return pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "        \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    "    )\n",
    "\n",
    "chunksize = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User data shape: (163205, 8)\n"
     ]
    }
   ],
   "source": [
    "# Чтение данных таблицы user_data\n",
    "query = \"SELECT * FROM user_data\"\n",
    "user_data = pd.read_sql(query, engine)\n",
    "print(f\"User data shape: {user_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post text data shape: (7023, 3)\n"
     ]
    }
   ],
   "source": [
    "# Чтение данных таблицы post_text_df\n",
    "query = \"SELECT * FROM post_text_df\"\n",
    "post_text_df = pd.read_sql(query, engine)\n",
    "print(f\"Post text data shape: {post_text_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def batch_load_sql_timed(engine, query: str, chunksize: int) -> pd.DataFrame:\n",
    "    conn = engine.connect().execution_options(stream_results=True)\n",
    "    chunks = []\n",
    "    row_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for chunk_dataframe in pd.read_sql(query, conn, chunksize=chunksize):\n",
    "        chunks.append(chunk_dataframe)\n",
    "        row_count += len(chunk_dataframe)\n",
    "        print(f\"Loaded {row_count} rows, elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    conn.close()\n",
    "    return pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000000 rows, elapsed time: 19.23 seconds\n",
      "Loaded 2000000 rows, elapsed time: 38.49 seconds\n",
      "Loaded 3000000 rows, elapsed time: 53.43 seconds\n",
      "Loaded 4000000 rows, elapsed time: 74.20 seconds\n",
      "Loaded 5000000 rows, elapsed time: 94.05 seconds\n",
      "Loaded 6000000 rows, elapsed time: 109.64 seconds\n",
      "Loaded 7000000 rows, elapsed time: 124.32 seconds\n",
      "Loaded 8000000 rows, elapsed time: 142.20 seconds\n",
      "Loaded 9000000 rows, elapsed time: 158.91 seconds\n",
      "Loaded 10000000 rows, elapsed time: 178.88 seconds\n",
      "Loaded 11000000 rows, elapsed time: 198.42 seconds\n",
      "Loaded 12000000 rows, elapsed time: 214.21 seconds\n",
      "Loaded 13000000 rows, elapsed time: 231.56 seconds\n",
      "Loaded 14000000 rows, elapsed time: 251.92 seconds\n",
      "Loaded 15000000 rows, elapsed time: 269.51 seconds\n",
      "Loaded 16000000 rows, elapsed time: 290.21 seconds\n",
      "Loaded 17000000 rows, elapsed time: 310.99 seconds\n",
      "Loaded 18000000 rows, elapsed time: 331.12 seconds\n",
      "Loaded 19000000 rows, elapsed time: 350.60 seconds\n",
      "Loaded 20000000 rows, elapsed time: 371.48 seconds\n",
      "Loaded 21000000 rows, elapsed time: 391.91 seconds\n",
      "Loaded 22000000 rows, elapsed time: 422.02 seconds\n",
      "Loaded 23000000 rows, elapsed time: 456.88 seconds\n",
      "Loaded 24000000 rows, elapsed time: 478.17 seconds\n",
      "Loaded 25000000 rows, elapsed time: 496.25 seconds\n",
      "Loaded 26000000 rows, elapsed time: 518.73 seconds\n",
      "Loaded 27000000 rows, elapsed time: 542.47 seconds\n",
      "Loaded 28000000 rows, elapsed time: 562.08 seconds\n",
      "Loaded 29000000 rows, elapsed time: 581.48 seconds\n",
      "Loaded 30000000 rows, elapsed time: 598.64 seconds\n",
      "Loaded 31000000 rows, elapsed time: 616.81 seconds\n",
      "Loaded 32000000 rows, elapsed time: 636.65 seconds\n",
      "Loaded 33000000 rows, elapsed time: 655.85 seconds\n",
      "Loaded 34000000 rows, elapsed time: 674.81 seconds\n",
      "Loaded 35000000 rows, elapsed time: 700.43 seconds\n",
      "Loaded 36000000 rows, elapsed time: 725.11 seconds\n",
      "Loaded 37000000 rows, elapsed time: 757.16 seconds\n",
      "Loaded 38000000 rows, elapsed time: 777.71 seconds\n",
      "Loaded 39000000 rows, elapsed time: 801.71 seconds\n",
      "Loaded 40000000 rows, elapsed time: 831.40 seconds\n",
      "Loaded 41000000 rows, elapsed time: 869.26 seconds\n",
      "Loaded 42000000 rows, elapsed time: 900.93 seconds\n",
      "Loaded 43000000 rows, elapsed time: 928.81 seconds\n",
      "Loaded 44000000 rows, elapsed time: 957.06 seconds\n",
      "Loaded 45000000 rows, elapsed time: 987.31 seconds\n",
      "Loaded 46000000 rows, elapsed time: 1013.65 seconds\n",
      "Loaded 47000000 rows, elapsed time: 1038.23 seconds\n",
      "Loaded 48000000 rows, elapsed time: 1063.03 seconds\n",
      "Loaded 49000000 rows, elapsed time: 1089.44 seconds\n",
      "Loaded 50000000 rows, elapsed time: 1115.54 seconds\n",
      "Loaded 51000000 rows, elapsed time: 1141.15 seconds\n",
      "Loaded 52000000 rows, elapsed time: 1165.91 seconds\n",
      "Loaded 53000000 rows, elapsed time: 1192.34 seconds\n",
      "Loaded 54000000 rows, elapsed time: 1222.17 seconds\n",
      "Loaded 55000000 rows, elapsed time: 1246.83 seconds\n",
      "Loaded 56000000 rows, elapsed time: 1271.97 seconds\n",
      "Loaded 57000000 rows, elapsed time: 1297.05 seconds\n",
      "Loaded 58000000 rows, elapsed time: 1324.39 seconds\n",
      "Loaded 59000000 rows, elapsed time: 1350.95 seconds\n",
      "Loaded 60000000 rows, elapsed time: 1376.16 seconds\n",
      "Loaded 61000000 rows, elapsed time: 1406.59 seconds\n",
      "Loaded 62000000 rows, elapsed time: 1431.53 seconds\n",
      "Loaded 63000000 rows, elapsed time: 1459.14 seconds\n",
      "Loaded 64000000 rows, elapsed time: 1491.13 seconds\n",
      "Loaded 65000000 rows, elapsed time: 1521.06 seconds\n",
      "Loaded 66000000 rows, elapsed time: 1548.93 seconds\n",
      "Loaded 67000000 rows, elapsed time: 1577.60 seconds\n",
      "Loaded 68000000 rows, elapsed time: 1604.92 seconds\n",
      "Loaded 69000000 rows, elapsed time: 1635.27 seconds\n",
      "Loaded 70000000 rows, elapsed time: 1666.48 seconds\n",
      "Loaded 71000000 rows, elapsed time: 1690.72 seconds\n",
      "Loaded 72000000 rows, elapsed time: 1721.80 seconds\n",
      "Loaded 73000000 rows, elapsed time: 1743.74 seconds\n",
      "Loaded 74000000 rows, elapsed time: 1767.11 seconds\n",
      "Loaded 75000000 rows, elapsed time: 1794.44 seconds\n",
      "Loaded 76000000 rows, elapsed time: 1818.19 seconds\n",
      "Loaded 76892800 rows, elapsed time: 1841.54 seconds\n",
      "Feed data shape: (76892800, 5)\n"
     ]
    }
   ],
   "source": [
    "# Чтение ограниченного количества данных таблицы feed_data\n",
    "query = \"SELECT * FROM feed_data\"\n",
    "feed_data = batch_load_sql_timed(engine, query, chunksize)\n",
    "print(f\"Feed data shape: {feed_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение feed_data из csv файла\n",
    "feed_data = pd.read_csv('feed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to csv\n",
    "import os\n",
    "\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "file_path = os.path.join(desktop_path, \"feed_data.csv\")\n",
    "\n",
    "feed_data.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after load_and_merge_data: (76892800, 14)\n"
     ]
    }
   ],
   "source": [
    "# Переименование столбцов идентификаторов\n",
    "user_data = user_data.rename(columns={'id': 'user_id'})\n",
    "post_text_df = post_text_df.rename(columns={'id': 'post_id'})\n",
    "\n",
    "# Объединение таблиц\n",
    "data = feed_data.merge(user_data, on='user_id', how='left')\n",
    "data = data.merge(post_text_df, on='post_id', how='left')\n",
    "\n",
    "print(f\"Data shape after load_and_merge_data: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'user_id', 'post_id', 'action', 'target', 'gender', 'age',\n",
      "       'country', 'city', 'exp_group', 'os', 'source', 'text', 'topic'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print all data columns \n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_10_percent(group):\n",
    "    frac = 0.2\n",
    "    return group.sample(frac=frac)\n",
    "\n",
    "sampled_df = data.groupby('user_id', group_keys=False).apply(sample_10_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id  original_count  sampled_count  percentage\n",
      "0           200             401             40    9.975062\n",
      "1           201             748             75   10.026738\n",
      "2           202             724             72    9.944751\n",
      "3           203             382             38    9.947644\n",
      "4           204             161             16    9.937888\n",
      "...         ...             ...            ...         ...\n",
      "163200   168548             382             38    9.947644\n",
      "163201   168549             274             27    9.854015\n",
      "163202   168550             407             41   10.073710\n",
      "163203   168551             525             52    9.904762\n",
      "163204   168552             263             26    9.885932\n",
      "\n",
      "[163205 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Подсчитайте количество строк для каждого пользователя в исходном dataframe\n",
    "original_counts = data.groupby('user_id').size().reset_index(name='original_count')\n",
    "\n",
    "# Подсчитайте количество строк для каждого пользователя в выборочном dataframe\n",
    "sampled_counts = sampled_df.groupby('user_id').size().reset_index(name='sampled_count')\n",
    "\n",
    "# Объедините два dataframe с подсчетом на столбце `user_id` для сравнения подсчета\n",
    "counts_comparison = pd.merge(original_counts, sampled_counts, on='user_id')\n",
    "\n",
    "# Рассчитайте процент строк в выборочном dataframe для каждого пользователя\n",
    "counts_comparison['percentage'] = (counts_comparison['sampled_count'] / counts_comparison['original_count']) * 100\n",
    "\n",
    "# Проверьте, равен ли процент около 10% для каждого пользователя\n",
    "print(counts_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка временных меток"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding year and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps processed\n",
      "Data shape after timestamps processing: (76892800, 18)\n"
     ]
    }
   ],
   "source": [
    "# Преобразование формата временных меток в объект datetime\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Извлечение признаков из временных меток\n",
    "data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "data['hour_of_day'] = data['timestamp'].dt.hour\n",
    "\n",
    "# Расчет времени с момента последнего действия для каждого пользователя\n",
    "data = data.sort_values(['user_id', 'timestamp'])\n",
    "data['time_since_last_action'] = data.groupby('user_id')['timestamp'].diff().dt.total_seconds()\n",
    "data['time_since_last_action'].fillna(0, inplace=True)\n",
    "\n",
    "# Extracting day of the month and year from the timestamp\n",
    "data['day_of_month'] = data['timestamp'].dt.day\n",
    "data['year'] = data['timestamp'].dt.year\n",
    "\n",
    "# Удаление столбца временных меток\n",
    "data = data.drop('timestamp', axis=1)\n",
    "\n",
    "print('Timestamps processed')\n",
    "print(f\"Data shape after timestamps processing: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание дополнительных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1: Количество просмотров и лайков для каждого пользователя\n",
    "user_views_likes = data.groupby('user_id')['action'].value_counts().unstack().fillna(0)\n",
    "user_views_likes.columns = ['user_views', 'user_likes']\n",
    "data = data.merge(user_views_likes, on='user_id', how='left')\n",
    "\n",
    "# Feature 2: Количество просмотров и лайков для каждого поста\n",
    "post_views_likes = data.groupby('post_id')['action'].value_counts().unstack().fillna(0)\n",
    "post_views_likes.columns = ['post_views', 'post_likes']\n",
    "data = data.merge(post_views_likes, on='post_id', how='left')\n",
    "\n",
    "# Feature 3: Количество просмотров и лайков для каждой группы тематик\n",
    "temp_df = data[['exp_group', 'topic', 'action']]\n",
    "\n",
    "# Создание колонок с количеством просмотров и лайков для каждой темы внутри группы\n",
    "topic_action_count = temp_df.pivot_table(index='exp_group', columns=['topic', 'action'], aggfunc=len, fill_value=0)\n",
    "topic_action_count.columns = [f'{col[0]}_exp_group_{col[1]}s' for col in topic_action_count.columns]\n",
    "grouped_data = topic_action_count.reset_index()\n",
    "\n",
    "data = data.merge(grouped_data, on='exp_group', how='left')\n",
    "\n",
    "# Преобразование категориальных признаков в строковый формат\n",
    "categorical_columns = ['country', 'city', 'topic', 'gender', 'os', 'source']\n",
    "data[categorical_columns] = data[categorical_columns].astype(str)\n",
    "\n",
    "print('Additional features created')\n",
    "print(f\"Data shape after additional features creation: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all columns of the dataframe\n",
    "print(data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Truncated Singular Value Decomposition (Truncated SVD), also known as Latent Semantic Analysis (LSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 'text' and 'target' columns\n",
    "df = data[['text', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVD\n",
    "n_components = 10\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "reduced_tfidf_matrix = svd.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df = pd.DataFrame(reduced_tfidf_matrix, columns=[f'component_{i}' for i in range(1, n_components+1)])\n",
    "reduced_df['target'] = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to csv\n",
    "import os\n",
    "\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "file_path = os.path.join(desktop_path, \"SVD.csv\")\n",
    "\n",
    "reduced_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPLOADING FEATURES TO DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data to csv\n",
    "import os\n",
    "\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "file_path = os.path.join(desktop_path, \"data.csv\")\n",
    "\n",
    "data.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to a-efimik_features_lesson_22:   0%|          | 3/769 [23:49<100:32:03, 472.48s/it]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def upload_dataframe_in_chunks(data, table_name, engine, chunksize=10000):\n",
    "    total_chunks = math.ceil(len(data) / chunksize)\n",
    "    for i in tqdm(range(total_chunks), desc=f\"Uploading to {table_name}\"):\n",
    "        chunk = data[i * chunksize : (i + 1) * chunksize]\n",
    "        if_exists = \"replace\" if i == 0 else \"append\"\n",
    "        chunk.to_sql(table_name, con=engine, if_exists=if_exists, index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "upload_dataframe_in_chunks(data, \"a-efimik_features_lesson_22\", engine, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM public.yancharskaya_features_lesson_22 LIMIT 1000', con=engine) # считываем таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  total_size table_size indexes_size\n",
      "0     359 MB     321 MB        38 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "size_query = '''\n",
    "SELECT \n",
    "    pg_size_pretty(pg_total_relation_size('public.yancharskaya_features_lesson_22')) AS total_size,\n",
    "    pg_size_pretty(pg_relation_size('public.yancharskaya_features_lesson_22')) AS table_size,\n",
    "    pg_size_pretty(pg_total_relation_size('public.yancharskaya_features_lesson_22') - pg_relation_size('public.yancharskaya_features_lesson_22')) AS indexes_size\n",
    "FROM\n",
    "    information_schema.tables\n",
    "WHERE\n",
    "    table_schema='public' AND table_name='yancharskaya_features_lesson_22';\n",
    "'''\n",
    "\n",
    "size_df = pd.read_sql(size_query, con=engine)\n",
    "print(size_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   row_count  column_count\n",
      "0    1768926            20\n"
     ]
    }
   ],
   "source": [
    "dimensions_query = '''\n",
    "SELECT\n",
    "    COUNT(*) AS row_count,\n",
    "    (SELECT COUNT(*)\n",
    "     FROM information_schema.columns\n",
    "     WHERE table_schema = 'public'\n",
    "     AND table_name = 'yancharskaya_features_lesson_22') AS column_count\n",
    "FROM\n",
    "    public.yancharskaya_features_lesson_22;\n",
    "'''\n",
    "\n",
    "dimensions_df = pd.read_sql(dimensions_query, con=engine)\n",
    "print(dimensions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>exp_group</th>\n",
       "      <th>source</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>text_size</th>\n",
       "      <th>iOS</th>\n",
       "      <th>covid</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>movie</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "      <th>top_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1232208</td>\n",
       "      <td>157937</td>\n",
       "      <td>5416</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0.132464</td>\n",
       "      <td>0.102506</td>\n",
       "      <td>2</td>\n",
       "      <td>0.139363</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>631</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1232209</td>\n",
       "      <td>3192</td>\n",
       "      <td>1156</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.195424</td>\n",
       "      <td>0.184388</td>\n",
       "      <td>3</td>\n",
       "      <td>0.139631</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1232210</td>\n",
       "      <td>16804</td>\n",
       "      <td>7087</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.132464</td>\n",
       "      <td>0.079235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.139631</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>993</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1232211</td>\n",
       "      <td>37226</td>\n",
       "      <td>7147</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0.132464</td>\n",
       "      <td>0.176157</td>\n",
       "      <td>2</td>\n",
       "      <td>0.139631</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1232212</td>\n",
       "      <td>137700</td>\n",
       "      <td>5049</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.132464</td>\n",
       "      <td>0.129808</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139363</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  user_id  post_id  gender  age   country      city  exp_group   \n",
       "0  1232208   157937     5416       1   19  0.132464  0.102506          2  \\\n",
       "1  1232209     3192     1156       0   19  0.195424  0.184388          3   \n",
       "2  1232210    16804     7087       0   19  0.132464  0.079235          1   \n",
       "3  1232211    37226     7147       1   38  0.132464  0.176157          2   \n",
       "4  1232212   137700     5049       1   28  0.132464  0.129808          0   \n",
       "\n",
       "     source  day_of_week  hour  text_size  iOS  covid  entertainment  movie   \n",
       "0  0.139363            3     8        631    0      0              0      1  \\\n",
       "1  0.139631            3     8       3080    1      0              0      0   \n",
       "2  0.139631            3     8        993    1      0              0      1   \n",
       "3  0.139631            3     8       5500    0      0              0      1   \n",
       "4  0.139363            3     8        428    0      0              0      1   \n",
       "\n",
       "   politics  sport  tech  top_words  \n",
       "0         0      0     0          0  \n",
       "1         1      0     0          0  \n",
       "2         0      0     0          0  \n",
       "3         0      0     0          1  \n",
       "4         0      0     0          1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6v/_xvyw44141s6_68z7m_2yyp80000gn/T/ipykernel_22973/1472378890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "df.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для инференса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем ненужные столбцы\n",
    "X = data.drop(['target', 'action', 'text'], axis=1)\n",
    "\n",
    "categorical_columns = ['country', 'topic', 'city', 'gender', 'os', 'source']\n",
    "\n",
    "# Создание ID группы на основе столбца 'user_id'\n",
    "unique_user_ids = X['user_id'].unique()\n",
    "group_id_dict = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "X['group_id'] = X['user_id'].map(group_id_dict)\n",
    "\n",
    "# Сортировка набора данных для предсказаний по 'group_id'\n",
    "X = X.sort_values(by='group_id')\n",
    "\n",
    "# Убедитесь, что категориальные переменные представлены в виде строк\n",
    "X[categorical_columns] = X[categorical_columns].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запись фичей в базу данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# То что будет в сервисе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "\n",
    "## TODO: надо просто передать лист с индексами категориальных признаков\n",
    "# Получение индексов категориальных столбцов\n",
    "cat_features = [X.drop(columns=['user_id']).columns.get_loc(col) for col in categorical_columns]\n",
    "\n",
    "# Создание объекта Pool для набора данных предсказаний с колонкой 'group_id' и категориальными признаками\n",
    "prediction_pool = Pool(X.drop(columns=['user_id']), cat_features=cat_features, group_id=X['group_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5525e501b23e2010ab883f77d6209697fb9a7ecd2dc8bb136f049441ff2c9ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
