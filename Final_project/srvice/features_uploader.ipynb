{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут мы реализуем сервис записи фичей в новую таблицу для того чтобы не загружать основной алгоритм обработкой данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка фичей из базы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "def load_and_merge_data(engine, chunksize=200000):\n",
    "    # Чтение данных таблицы user_data\n",
    "    query = \"SELECT * FROM user_data\"\n",
    "    user_data = pd.read_sql(query, engine)\n",
    "    print(f\"User data shape: {user_data.shape}\")\n",
    "\n",
    "    # Чтение данных таблицы post_text_df\n",
    "    query = \"SELECT * FROM post_text_df\"\n",
    "    post_text_df = pd.read_sql(query, engine)\n",
    "    print(f\"Post text data shape: {post_text_df.shape}\")\n",
    "\n",
    "    # Чтение ограниченного количества данных таблицы feed_data\n",
    "    query = f\"SELECT * FROM feed_data\"\n",
    "    feed_data = batch_load_sql_timed(engine, query, chunksize)\n",
    "    print(f\"Feed data shape: {feed_data.shape}\")\n",
    "\n",
    "    # Переименование столбцов идентификаторов\n",
    "    user_data = user_data.rename(columns={'id': 'user_id'})\n",
    "    post_text_df = post_text_df.rename(columns={'id': 'post_id'})\n",
    "\n",
    "    # Объединение таблиц\n",
    "    data = feed_data.merge(user_data, on='user_id', how='left')\n",
    "    data = data.merge(post_text_df, on='post_id', how='left')\n",
    "\n",
    "    print(f\"Data shape after load_and_merge_data: {data.shape}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batch_load_sql(engine, query: str, chunksize: int) -> pd.DataFrame:\n",
    "    conn = engine.connect().execution_options(stream_results=True)\n",
    "    chunks = []\n",
    "    for chunk_dataframe in pd.read_sql(query, conn, chunksize=chunksize):\n",
    "        chunks.append(chunk_dataframe)\n",
    "    conn.close()\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "import time\n",
    "\n",
    "def batch_load_sql_timed(engine, query: str, chunksize: int) -> pd.DataFrame:\n",
    "    conn = engine.connect().execution_options(stream_results=True)\n",
    "    chunks = []\n",
    "    row_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for chunk_dataframe in pd.read_sql(query, conn, chunksize=chunksize):\n",
    "        chunks.append(chunk_dataframe)\n",
    "        row_count += len(chunk_dataframe)\n",
    "        print(f\"Loaded {row_count} rows, elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    conn.close()\n",
    "    return pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\n",
    "        \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    "    )\n",
    "\n",
    "chunksize = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User data shape: (163205, 8)\n"
     ]
    }
   ],
   "source": [
    "# Чтение данных таблицы user_data\n",
    "query = \"SELECT * FROM user_data\"\n",
    "user_data = pd.read_sql(query, engine)\n",
    "print(f\"User data shape: {user_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post text data shape: (7023, 3)\n"
     ]
    }
   ],
   "source": [
    "# Чтение данных таблицы post_text_df\n",
    "query = \"SELECT * FROM post_text_df\"\n",
    "post_text_df = pd.read_sql(query, engine)\n",
    "print(f\"Post text data shape: {post_text_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def batch_load_sql_timed(engine, query: str, chunksize: int) -> pd.DataFrame:\n",
    "    conn = engine.connect().execution_options(stream_results=True)\n",
    "    chunks = []\n",
    "    row_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for chunk_dataframe in pd.read_sql(query, conn, chunksize=chunksize):\n",
    "        chunks.append(chunk_dataframe)\n",
    "        row_count += len(chunk_dataframe)\n",
    "        print(f\"Loaded {row_count} rows, elapsed time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    conn.close()\n",
    "    return pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000000 rows, elapsed time: 111.40 seconds\n",
      "Feed data shape: (1000000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Чтение ограниченного количества данных таблицы feed_data\n",
    "query = f\"SELECT * FROM feed_data LIMIT 1000000\"\n",
    "feed_data = batch_load_sql_timed(engine, query, chunksize)\n",
    "print(f\"Feed data shape: {feed_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after load_and_merge_data: (1000000, 14)\n"
     ]
    }
   ],
   "source": [
    "# Переименование столбцов идентификаторов\n",
    "user_data = user_data.rename(columns={'id': 'user_id'})\n",
    "post_text_df = post_text_df.rename(columns={'id': 'post_id'})\n",
    "\n",
    "# Объединение таблиц\n",
    "data = feed_data.merge(user_data, on='user_id', how='left')\n",
    "data = data.merge(post_text_df, on='post_id', how='left')\n",
    "\n",
    "print(f\"Data shape after load_and_merge_data: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка временных меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps processed\n",
      "Data shape after timestamps processing: (1000000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Преобразование формата временных меток в объект datetime\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# Извлечение признаков из временных меток\n",
    "data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "data['hour_of_day'] = data['timestamp'].dt.hour\n",
    "\n",
    "# Расчет времени с момента последнего действия для каждого пользователя\n",
    "data = data.sort_values(['user_id', 'timestamp'])\n",
    "data['time_since_last_action'] = data.groupby('user_id')['timestamp'].diff().dt.total_seconds()\n",
    "data['time_since_last_action'].fillna(0, inplace=True)\n",
    "\n",
    "# Удаление столбца временных меток\n",
    "data = data.drop('timestamp', axis=1)\n",
    "\n",
    "print('Timestamps processed')\n",
    "print(f\"Data shape after timestamps processing: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание дополнительных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional features created\n",
      "Data shape after additional features creation: (1000000, 34)\n"
     ]
    }
   ],
   "source": [
    "# Feature 1: Количество просмотров и лайков для каждого пользователя\n",
    "user_views_likes = data.groupby('user_id')['action'].value_counts().unstack().fillna(0)\n",
    "user_views_likes.columns = ['user_views', 'user_likes']\n",
    "data = data.merge(user_views_likes, on='user_id', how='left')\n",
    "\n",
    "# Feature 2: Количество просмотров и лайков для каждого поста\n",
    "post_views_likes = data.groupby('post_id')['action'].value_counts().unstack().fillna(0)\n",
    "post_views_likes.columns = ['post_views', 'post_likes']\n",
    "data = data.merge(post_views_likes, on='post_id', how='left')\n",
    "\n",
    "# Feature 3: Количество просмотров и лайков для каждой группы тематик\n",
    "temp_df = data[['exp_group', 'topic', 'action']]\n",
    "\n",
    "# Создание колонок с количеством просмотров и лайков для каждой темы внутри группы\n",
    "topic_action_count = temp_df.pivot_table(index='exp_group', columns=['topic', 'action'], aggfunc=len, fill_value=0)\n",
    "topic_action_count.columns = [f'{col[0]}_exp_group_{col[1]}s' for col in topic_action_count.columns]\n",
    "grouped_data = topic_action_count.reset_index()\n",
    "\n",
    "data = data.merge(grouped_data, on='exp_group', how='left')\n",
    "\n",
    "# Преобразование категориальных признаков в строковый формат\n",
    "categorical_columns = ['country', 'city', 'topic', 'gender', 'os', 'source']\n",
    "data[categorical_columns] = data[categorical_columns].astype(str)\n",
    "\n",
    "print('Additional features created')\n",
    "print(f\"Data shape after additional features creation: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Table 'my_favourite_table_69' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6v/_xvyw44141s6_68z7m_2yyp80000gn/T/ipykernel_22973/1606438746.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_favourite_table_69'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# записываем таблицу\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2949\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2951\u001b[0;31m         return sql.to_sql(\n\u001b[0m\u001b[1;32m   2952\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2953\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m     return pandas_sql.to_sql(\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0msql_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m         table = self.prep_table(\n\u001b[0m\u001b[1;32m   1733\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mprep_table\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, dtype)\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0;31m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fail\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Table '{self.name}' already exists.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd_sql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Table 'my_favourite_table_69' already exists."
     ]
    }
   ],
   "source": [
    "data.to_sql('my_favourite_table_69', con=engine) # записываем таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM my_favourite_table_69 LIMIT 1000', con=engine) # считываем таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>action</th>\n",
       "      <th>target</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>exp_group</th>\n",
       "      <th>...</th>\n",
       "      <th>entertainment_exp_group_likes</th>\n",
       "      <th>entertainment_exp_group_views</th>\n",
       "      <th>movie_exp_group_likes</th>\n",
       "      <th>movie_exp_group_views</th>\n",
       "      <th>politics_exp_group_likes</th>\n",
       "      <th>politics_exp_group_views</th>\n",
       "      <th>sport_exp_group_likes</th>\n",
       "      <th>sport_exp_group_views</th>\n",
       "      <th>tech_exp_group_likes</th>\n",
       "      <th>tech_exp_group_views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, user_id, post_id, action, target, gender, age, country, city, exp_group, os, source, text, topic, day_of_week, hour_of_day, time_since_last_action, user_views, user_likes, post_views, post_likes, business_exp_group_likes, business_exp_group_views, covid_exp_group_likes, covid_exp_group_views, entertainment_exp_group_likes, entertainment_exp_group_views, movie_exp_group_likes, movie_exp_group_views, politics_exp_group_likes, politics_exp_group_views, sport_exp_group_likes, sport_exp_group_views, tech_exp_group_likes, tech_exp_group_views]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6v/_xvyw44141s6_68z7m_2yyp80000gn/T/ipykernel_22973/1472378890.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "df.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных для инференса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем ненужные столбцы\n",
    "X = data.drop(['target', 'action', 'text'], axis=1)\n",
    "\n",
    "categorical_columns = ['country', 'topic', 'city', 'gender', 'os', 'source']\n",
    "\n",
    "# Создание ID группы на основе столбца 'user_id'\n",
    "unique_user_ids = X['user_id'].unique()\n",
    "group_id_dict = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "X['group_id'] = X['user_id'].map(group_id_dict)\n",
    "\n",
    "# Сортировка набора данных для предсказаний по 'group_id'\n",
    "X = X.sort_values(by='group_id')\n",
    "\n",
    "# Убедитесь, что категориальные переменные представлены в виде строк\n",
    "X[categorical_columns] = X[categorical_columns].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запись фичей в базу данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# То что будет в сервисе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "\n",
    "## TODO: надо просто передать лист с индексами категориальных признаков\n",
    "# Получение индексов категориальных столбцов\n",
    "cat_features = [X.drop(columns=['user_id']).columns.get_loc(col) for col in categorical_columns]\n",
    "\n",
    "# Создание объекта Pool для набора данных предсказаний с колонкой 'group_id' и категориальными признаками\n",
    "prediction_pool = Pool(X.drop(columns=['user_id']), cat_features=cat_features, group_id=X['group_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5525e501b23e2010ab883f77d6209697fb9a7ecd2dc8bb136f049441ff2c9ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
